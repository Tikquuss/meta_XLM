{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: eval_only=True\n",
      "env: exp_id=maml\n"
     ]
    }
   ],
   "source": [
    "%env eval_only=True\n",
    "%env exp_id=maml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(bookmark:HOME) -> /home/jupyter/meta_XLM/XLM\n",
      "/home/jupyter/meta_XLM/XLM\n"
     ]
    }
   ],
   "source": [
    "%bookmark HOME \"/home/jupyter/meta_XLM/XLM\" \n",
    "%cd -b HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: remove_long_sentences_train=True\n",
      "env: remove_long_sentences_valid=True\n",
      "env: remove_long_sentences_test=True\n",
      "env: train_n_samples=-1\n",
      "env: valid_n_samples=-1\n",
      "env: test_n_samples=-1\n"
     ]
    }
   ],
   "source": [
    "# If you don't have enough RAM or swap memory, leave these three parameters to True, otherwise you may get an error like this when evaluating \n",
    "# RuntimeError: copy_if failed to synchronize: cudaErrorAssert: device-side assert triggered\n",
    "%env remove_long_sentences_train=True\n",
    "%env remove_long_sentences_valid=True\n",
    "%env remove_long_sentences_test=True\n",
    "#--remove_long_sentences_train $remove_long_sentences_train --remove_long_sentences_valid $remove_long_sentences_valid --remove_long_sentences_test $remove_long_sentences_test\n",
    "\n",
    "# limit the number of examples (-1 by default for non limitation)\n",
    "%env train_n_samples=-1\n",
    "%env valid_n_samples=-1\n",
    "%env test_n_samples=-1\n",
    "#--train_n_samples $train_n_samples --valid_n_samples $valid_n_samples --test_n_samples $test_n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XLM Ghomala Limbum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OUTPATH=/home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed\n",
      "env: epoch_size=6333\n",
      "env: lgs=Ghomala-Limbum\n",
      "env: batch_size=32\n",
      "env: max_epoch=100\n",
      "env: dump_path=/home/jupyter/models/africa/cluster3\n"
     ]
    }
   ],
   "source": [
    "%env OUTPATH=/home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed\n",
    "%env epoch_size=6333\n",
    "%env lgs=Ghomala-Limbum\n",
    "%env batch_size=32\n",
    "%env max_epoch=100\n",
    "%env dump_path=/home/jupyter/models/africa/cluster3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLM + TLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: stopping_criterion=_valid_mlm_ppl,10\n",
      "env: eval_bleu=False\n",
      "env: mlm_steps=Ghomala,Limbum,Ghomala-Limbum\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaLimbum --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed' --lgs 'Ghomala-Limbum' --clm_steps '' --mlm_steps 'Ghomala,Limbum,Ghomala-Limbum' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 6333 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 6333\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_GhomalaLimbum\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Ghomala', 1: 'Limbum'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Ghomala': 0, 'Limbum': 1}\n",
      "                                     langs: ['Ghomala', 'Limbum']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Ghomala-Limbum']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Ghomala', None), ('Limbum', None), ('Ghomala', 'Limbum')]\n",
      "                                     mono_dataset: {'Ghomala': {'train': '/home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/train.Ghomala.pth', 'valid': '/home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/valid.Ghomala.pth', 'test': '/home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/test.Ghomala.pth'}, 'Limbum': {'train': '/home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/train.Limbum.pth', 'valid': '/home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/valid.Limbum.pth', 'test': '/home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/test.Limbum.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {('Ghomala', 'Limbum'): {'train': ('/home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/train.Ghomala-Limbum.Ghomala.pth', '/home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/train.Ghomala-Limbum.Limbum.pth'), 'valid': ('/home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/valid.Ghomala-Limbum.Ghomala.pth', '/home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/valid.Ghomala-Limbum.Limbum.pth'), 'test': ('/home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/test.Ghomala-Limbum.Ghomala.pth', '/home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/test.Ghomala-Limbum.Limbum.pth')}}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': True, 'valid': True, 'test': True}\n",
      "                                     remove_long_sentences_test: True\n",
      "                                     remove_long_sentences_train: True\n",
      "                                     remove_long_sentences_valid: True\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml\n",
      "                                     \n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaLimbum --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed' --lgs 'Ghomala-Limbum' --clm_steps '' --mlm_steps 'Ghomala,Limbum,Ghomala-Limbum' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 6333 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/29/20 19:27:59 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - ============ langs: Ghomala, Limbum\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - ============ Monolingual data (Ghomala)\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/valid.Ghomala.pth ...\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - 26470 words (5057 unique) in 791 sentences. 71 unknown words (54 unique) covering 0.27% of the data.\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/test.Ghomala.pth ...\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - 25835 words (5057 unique) in 791 sentences. 63 unknown words (54 unique) covering 0.24% of the data.\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - ============ Monolingual data (Limbum)\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/valid.Limbum.pth ...\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - 32483 words (5057 unique) in 791 sentences. 113 unknown words (83 unique) covering 0.35% of the data.\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/test.Limbum.pth ...\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - 32593 words (5057 unique) in 791 sentences. 87 unknown words (75 unique) covering 0.27% of the data.\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - ============ Parallel data (Ghomala-Limbum)\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/valid.Ghomala-Limbum.Ghomala.pth ...\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - 26470 words (5057 unique) in 791 sentences. 71 unknown words (54 unique) covering 0.27% of the data.\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/valid.Ghomala-Limbum.Limbum.pth ...\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - 32483 words (5057 unique) in 791 sentences. 113 unknown words (83 unique) covering 0.35% of the data.\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - ========================== debug : 5\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - Removed 0 too long sentences.\n",
      "\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/test.Ghomala-Limbum.Ghomala.pth ...\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - 25835 words (5057 unique) in 791 sentences. 63 unknown words (54 unique) covering 0.24% of the data.\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/test.Ghomala-Limbum.Limbum.pth ...\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - 32593 words (5057 unique) in 791 sentences. 87 unknown words (75 unique) covering 0.27% of the data.\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - ========================== debug : 6\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - Removed 4 too long sentences.\n",
      "\n",
      "\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - Monolingual data   - valid -      Ghomala:       791\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - Monolingual data   -  test -      Ghomala:       791\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - Monolingual data   - valid -       Limbum:       791\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - Monolingual data   -  test -       Limbum:       791\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - Parallel data      - valid - Ghomala-Limbum:       791\n",
      "INFO - 05/29/20 19:27:59 - 0:00:00 - Parallel data      -  test - Ghomala-Limbum:       787\n",
      "\n",
      "INFO - 05/29/20 19:28:00 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(5057, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=5057, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/29/20 19:28:00 - 0:00:01 - Number of parameters (model): 81289153\n",
      "INFO - 05/29/20 19:28:04 - 0:00:05 - Found 0 memories.\n",
      "INFO - 05/29/20 19:28:04 - 0:00:05 - Found 6 FFN.\n",
      "INFO - 05/29/20 19:28:04 - 0:00:05 - Found 102 parameters in model.\n",
      "INFO - 05/29/20 19:28:04 - 0:00:05 - Optimizers: model\n",
      "WARNING - 05/29/20 19:28:04 - 0:00:05 - Reloading checkpoint from /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml/checkpoint.pth ...\n",
      "WARNING - 05/29/20 19:28:14 - 0:00:15 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/29/20 19:28:14 - 0:00:15 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/29/20 19:28:14 - 0:00:15 - Checkpoint reloaded. Resuming at epoch 52 / iteration 13728 ...\n",
      "INFO - 05/29/20 19:28:19 - 0:00:20 - epoch -> 52.000000\n",
      "INFO - 05/29/20 19:28:19 - 0:00:20 - valid_Ghomala_mlm_ppl -> 18.664649\n",
      "INFO - 05/29/20 19:28:19 - 0:00:20 - valid_Ghomala_mlm_acc -> 47.409326\n",
      "INFO - 05/29/20 19:28:19 - 0:00:20 - valid_Limbum_mlm_ppl -> 9.009061\n",
      "INFO - 05/29/20 19:28:19 - 0:00:20 - valid_Limbum_mlm_acc -> 56.217617\n",
      "INFO - 05/29/20 19:28:19 - 0:00:20 - valid_Ghomala_Limbum_mlm_ppl -> 9.967367\n",
      "INFO - 05/29/20 19:28:19 - 0:00:20 - valid_Ghomala_Limbum_mlm_acc -> 55.276992\n",
      "INFO - 05/29/20 19:28:19 - 0:00:20 - valid_mlm_ppl -> 13.836855\n",
      "INFO - 05/29/20 19:28:19 - 0:00:20 - valid_mlm_acc -> 51.813472\n",
      "INFO - 05/29/20 19:28:19 - 0:00:20 - test_Ghomala_mlm_ppl -> 12.592416\n",
      "INFO - 05/29/20 19:28:19 - 0:00:20 - test_Ghomala_mlm_acc -> 51.554404\n",
      "INFO - 05/29/20 19:28:19 - 0:00:20 - test_Limbum_mlm_ppl -> 7.248203\n",
      "INFO - 05/29/20 19:28:19 - 0:00:20 - test_Limbum_mlm_acc -> 59.067358\n",
      "INFO - 05/29/20 19:28:19 - 0:00:20 - test_Ghomala_Limbum_mlm_ppl -> 8.992800\n",
      "INFO - 05/29/20 19:28:19 - 0:00:20 - test_Ghomala_Limbum_mlm_acc -> 56.876268\n",
      "INFO - 05/29/20 19:28:19 - 0:00:20 - test_mlm_ppl -> 9.920310\n",
      "INFO - 05/29/20 19:28:19 - 0:00:20 - test_mlm_acc -> 55.310881\n",
      "INFO - 05/29/20 19:28:19 - 0:00:20 - __log__:{\"epoch\": 52, \"valid_Ghomala_mlm_ppl\": 18.66464855238158, \"valid_Ghomala_mlm_acc\": 47.409326424870464, \"valid_Limbum_mlm_ppl\": 9.00906098390884, \"valid_Limbum_mlm_acc\": 56.21761658031088, \"valid_Ghomala_Limbum_mlm_ppl\": 9.967367330062324, \"valid_Ghomala_Limbum_mlm_acc\": 55.27699150828953, \"valid_mlm_ppl\": 13.836854768145209, \"valid_mlm_acc\": 51.81347150259067, \"test_Ghomala_mlm_ppl\": 12.59241605385465, \"test_Ghomala_mlm_acc\": 51.55440414507772, \"test_Limbum_mlm_ppl\": 7.248203081490473, \"test_Limbum_mlm_acc\": 59.067357512953365, \"test_Ghomala_Limbum_mlm_ppl\": 8.992800090326913, \"test_Ghomala_Limbum_mlm_acc\": 56.8762677484787, \"test_mlm_ppl\": 9.920309567672561, \"test_mlm_acc\": 55.310880829015545}\n"
     ]
    }
   ],
   "source": [
    "# stopping criterion (if criterion does not improve 10 times)\n",
    "%env stopping_criterion=_valid_mlm_ppl,10\n",
    "%env eval_bleu=False\n",
    "%env mlm_steps=Ghomala,Limbum,Ghomala-Limbum\n",
    "! python train.py --eval_only $eval_only --exp_name mlm_tlm_GhomalaLimbum --exp_id $exp_id --dump_path $dump_path --data_path $OUTPATH --lgs $lgs --clm_steps '' --mlm_steps $mlm_steps --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout 0.1 --attention_dropout 0.1 --gelu_activation true --batch_size $batch_size --bptt 256 --optimizer adam,lr=0.0001 --epoch_size $epoch_size --max_epoch $max_epoch --validation_metrics _valid_mlm_ppl --stopping_criterion $stopping_criterion --eval_bleu $eval_bleu --remove_long_sentences_train $remove_long_sentences_train --remove_long_sentences_valid $remove_long_sentences_valid --remove_long_sentences_test $remove_long_sentences_test --train_n_samples $train_n_samples --valid_n_samples $valid_n_samples --test_n_samples $test_n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: eval_bleu=True\n",
      "env: stopping_criterion=valid_Ghomala-Limbum_mt_bleu,10\n",
      "env: validation_metrics=valid_Ghomala-Limbum_mt_bleu\n",
      "env: reload_model=/home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml/best-valid_mlm_ppl.pth,/home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml/best-valid_mlm_ppl.pth\n",
      "env: ae_steps=Ghomala,Limbum\n",
      "env: bt_steps=Ghomala-Limbum-Ghomala,Limbum-Ghomala-Limbum\n",
      "env: mt_steps=Ghomala-Limbum,Limbum-Ghomala\n",
      "env: mlm_steps=Ghomala,Limbum,Ghomala-Limbum\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: ['Ghomala', 'Limbum']\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: ['Ghomala', 'Limbum']\n",
      "                                     bt_steps: [('Ghomala', 'Limbum', 'Ghomala'), ('Limbum', 'Ghomala', 'Limbum')]\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --mlm_steps 'Ghomala,Limbum,Ghomala-Limbum' --exp_name SupMT_GhomalaLimbum --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --reload_model '/home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml/best-valid_mlm_ppl.pth,/home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml/best-valid_mlm_ppl.pth' --data_path '/home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed' --lgs 'Ghomala-Limbum' --ae_steps 'Ghomala,Limbum' --mt_steps 'Ghomala-Limbum,Limbum-Ghomala' --bt_steps 'Ghomala-Limbum-Ghomala,Limbum-Ghomala-Limbum' --word_shuffle 3 --word_dropout '0.1' --word_blank '0.1' --lambda_ae '0:1,100000:0.1,300000:0' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --tokens_per_batch 2000 --batch_size 32 --bptt 256 --optimizer 'adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001' --epoch_size 6333 --max_epoch 100 --eval_bleu True --stopping_criterion 'valid_Ghomala-Limbum_mt_bleu,10' --validation_metrics 'valid_Ghomala-Limbum_mt_bleu' --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster3/SupMT_GhomalaLimbum/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: False\n",
      "                                     epoch_size: 6333\n",
      "                                     eval_bleu: True\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: SupMT_GhomalaLimbum\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Ghomala', 1: 'Limbum'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 0:1,100000:0.1,300000:0\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Ghomala': 0, 'Limbum': 1}\n",
      "                                     langs: ['Ghomala', 'Limbum']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Ghomala-Limbum']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Ghomala', None), ('Limbum', None), ('Ghomala', 'Limbum')]\n",
      "                                     mono_dataset: {'Ghomala': {'train': '/home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/train.Ghomala.pth', 'valid': '/home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/valid.Ghomala.pth', 'test': '/home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/test.Ghomala.pth'}, 'Limbum': {'train': '/home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/train.Limbum.pth', 'valid': '/home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/valid.Limbum.pth', 'test': '/home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/test.Limbum.pth'}}\n",
      "                                     mt_steps: [('Ghomala', 'Limbum'), ('Limbum', 'Ghomala')]\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001\n",
      "                                     para_dataset: {('Ghomala', 'Limbum'): {'train': ('/home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/train.Ghomala-Limbum.Ghomala.pth', '/home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/train.Ghomala-Limbum.Limbum.pth'), 'valid': ('/home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/valid.Ghomala-Limbum.Ghomala.pth', '/home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/valid.Ghomala-Limbum.Limbum.pth'), 'test': ('/home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/test.Ghomala-Limbum.Ghomala.pth', '/home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/test.Ghomala-Limbum.Limbum.pth')}}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml/best-valid_mlm_ppl.pth,/home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml/best-valid_mlm_ppl.pth\n",
      "                                     remove_long_sentences: {'train': True, 'valid': True, 'test': True}\n",
      "                                     remove_long_sentences_test: True\n",
      "                                     remove_long_sentences_train: True\n",
      "                                     remove_long_sentences_valid: True\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: valid_Ghomala-Limbum_mt_bleu,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: 2000\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: valid_Ghomala-Limbum_mt_bleu\n",
      "                                     word_blank: 0.1\n",
      "                                     word_dropout: 0.1\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 3.0\n",
      "                                     world_size: 1\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster3/SupMT_GhomalaLimbum/maml\n",
      "                                     \n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - Running command: python train.py --eval_only True --mlm_steps 'Ghomala,Limbum,Ghomala-Limbum' --exp_name SupMT_GhomalaLimbum --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --reload_model '/home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml/best-valid_mlm_ppl.pth,/home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml/best-valid_mlm_ppl.pth' --data_path '/home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed' --lgs 'Ghomala-Limbum' --ae_steps 'Ghomala,Limbum' --mt_steps 'Ghomala-Limbum,Limbum-Ghomala' --bt_steps 'Ghomala-Limbum-Ghomala,Limbum-Ghomala-Limbum' --word_shuffle 3 --word_dropout '0.1' --word_blank '0.1' --lambda_ae '0:1,100000:0.1,300000:0' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --tokens_per_batch 2000 --batch_size 32 --bptt 256 --optimizer 'adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001' --epoch_size 6333 --max_epoch 100 --eval_bleu True --stopping_criterion 'valid_Ghomala-Limbum_mt_bleu,10' --validation_metrics 'valid_Ghomala-Limbum_mt_bleu' --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True\n",
      "\n",
      "WARNING - 05/29/20 19:30:22 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - ============ langs: Ghomala, Limbum\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - ============ Monolingual data (Ghomala)\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/valid.Ghomala.pth ...\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - 26470 words (5057 unique) in 791 sentences. 71 unknown words (54 unique) covering 0.27% of the data.\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - ========================== debug : 1\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - ========================== debug : 2\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - Removed 0 too long sentences.\n",
      "\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/test.Ghomala.pth ...\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - 25835 words (5057 unique) in 791 sentences. 63 unknown words (54 unique) covering 0.24% of the data.\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - ========================== debug : 3\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - ========================== debug : 4\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - Removed 0 too long sentences.\n",
      "\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - ============ Monolingual data (Limbum)\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/valid.Limbum.pth ...\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - 32483 words (5057 unique) in 791 sentences. 113 unknown words (83 unique) covering 0.35% of the data.\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - ========================== debug : 5\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - ========================== debug : 6\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - Removed 0 too long sentences.\n",
      "\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/test.Limbum.pth ...\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - 32593 words (5057 unique) in 791 sentences. 87 unknown words (75 unique) covering 0.27% of the data.\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - ========================== debug : 7\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - ========================== debug : 8\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - Removed 4 too long sentences.\n",
      "\n",
      "\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - ============ Parallel data (Ghomala-Limbum)\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/valid.Ghomala-Limbum.Ghomala.pth ...\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - 26470 words (5057 unique) in 791 sentences. 71 unknown words (54 unique) covering 0.27% of the data.\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/valid.Ghomala-Limbum.Limbum.pth ...\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - 32483 words (5057 unique) in 791 sentences. 113 unknown words (83 unique) covering 0.35% of the data.\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - ========================== debug : 9\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - Removed 0 too long sentences.\n",
      "\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/test.Ghomala-Limbum.Ghomala.pth ...\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - 25835 words (5057 unique) in 791 sentences. 63 unknown words (54 unique) covering 0.24% of the data.\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/test.Ghomala-Limbum.Limbum.pth ...\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - 32593 words (5057 unique) in 791 sentences. 87 unknown words (75 unique) covering 0.27% of the data.\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - ========================== debug : 10\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - Removed 4 too long sentences.\n",
      "\n",
      "\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - Monolingual data   - valid -      Ghomala:       791\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - Monolingual data   -  test -      Ghomala:       791\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - Monolingual data   - valid -       Limbum:       791\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - Monolingual data   -  test -       Limbum:       791\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - Parallel data      - valid - Ghomala-Limbum:       791\n",
      "INFO - 05/29/20 19:30:22 - 0:00:00 - Parallel data      -  test - Ghomala-Limbum:       787\n",
      "\n",
      "INFO - 05/29/20 19:30:24 - 0:00:02 - Reloading encoder from /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml/best-valid_mlm_ppl.pth ...\n",
      "INFO - 05/29/20 19:30:32 - 0:00:09 - Reloading decoder from /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml/best-valid_mlm_ppl.pth ...\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter layer_norm15.0.weight not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter layer_norm15.0.bias not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.0.q_lin.weight not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.0.q_lin.bias not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.0.k_lin.weight not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.0.k_lin.bias not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.0.v_lin.weight not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.0.v_lin.bias not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.0.out_lin.weight not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.0.out_lin.bias not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter layer_norm15.1.weight not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter layer_norm15.1.bias not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.1.q_lin.weight not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.1.q_lin.bias not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.1.k_lin.weight not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.1.k_lin.bias not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.1.v_lin.weight not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.1.v_lin.bias not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.1.out_lin.weight not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.1.out_lin.bias not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter layer_norm15.2.weight not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter layer_norm15.2.bias not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.2.q_lin.weight not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.2.q_lin.bias not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.2.k_lin.weight not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.2.k_lin.bias not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.2.v_lin.weight not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.2.v_lin.bias not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.2.out_lin.weight not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.2.out_lin.bias not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter layer_norm15.3.weight not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter layer_norm15.3.bias not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.3.q_lin.weight not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.3.q_lin.bias not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.3.k_lin.weight not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.3.k_lin.bias not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.3.v_lin.weight not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.3.v_lin.bias not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.3.out_lin.weight not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.3.out_lin.bias not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter layer_norm15.4.weight not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter layer_norm15.4.bias not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.4.q_lin.weight not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.4.q_lin.bias not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.4.k_lin.weight not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.4.k_lin.bias not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.4.v_lin.weight not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.4.v_lin.bias not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.4.out_lin.weight not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.4.out_lin.bias not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter layer_norm15.5.weight not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter layer_norm15.5.bias not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.5.q_lin.weight not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.5.q_lin.bias not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.5.k_lin.weight not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.5.k_lin.bias not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.5.v_lin.weight not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.5.v_lin.bias not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.5.out_lin.weight not found.\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Parameter encoder_attn.5.out_lin.bias not found.\n",
      "INFO - 05/29/20 19:30:32 - 0:00:10 - Number of parameters (encoder): 81289153\n",
      "INFO - 05/29/20 19:30:32 - 0:00:10 - Number of parameters (decoder): 106491841\n",
      "INFO - 05/29/20 19:30:32 - 0:00:10 - Found 0 memories.\n",
      "INFO - 05/29/20 19:30:32 - 0:00:10 - Found 12 FFN.\n",
      "INFO - 05/29/20 19:30:32 - 0:00:10 - Found 264 parameters in model.\n",
      "INFO - 05/29/20 19:30:32 - 0:00:10 - Optimizers: model\n",
      "WARNING - 05/29/20 19:30:32 - 0:00:10 - Reloading checkpoint from /home/jupyter/models/africa/cluster3/SupMT_GhomalaLimbum/maml/checkpoint.pth ...\n",
      "WARNING - 05/29/20 19:30:53 - 0:00:31 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/29/20 19:30:53 - 0:00:31 - Reloading 'num_updates' and 'lr' for optimizer model.\n",
      "WARNING - 05/29/20 19:30:53 - 0:00:31 - Checkpoint reloaded. Resuming at epoch 90 / iteration 3060 ...\n",
      "INFO - 05/29/20 19:31:37 - 0:01:15 - BLEU /home/jupyter/models/africa/cluster3/SupMT_GhomalaLimbum/maml/hypotheses/hyp90.Ghomala-Limbum.valid.txt /home/jupyter/models/africa/cluster3/SupMT_GhomalaLimbum/maml/hypotheses/ref.Ghomala-Limbum.valid.txt : 16.230000\n",
      "INFO - 05/29/20 19:32:13 - 0:01:51 - BLEU /home/jupyter/models/africa/cluster3/SupMT_GhomalaLimbum/maml/hypotheses/hyp90.Limbum-Ghomala.valid.txt /home/jupyter/models/africa/cluster3/SupMT_GhomalaLimbum/maml/hypotheses/ref.Limbum-Ghomala.valid.txt : 11.410000\n",
      "INFO - 05/29/20 19:32:38 - 0:02:16 - BLEU /home/jupyter/models/africa/cluster3/SupMT_GhomalaLimbum/maml/hypotheses/hyp90.Ghomala-Limbum.test.txt /home/jupyter/models/africa/cluster3/SupMT_GhomalaLimbum/maml/hypotheses/ref.Ghomala-Limbum.test.txt : 18.310000\n",
      "INFO - 05/29/20 19:32:58 - 0:02:36 - BLEU /home/jupyter/models/africa/cluster3/SupMT_GhomalaLimbum/maml/hypotheses/hyp90.Limbum-Ghomala.test.txt /home/jupyter/models/africa/cluster3/SupMT_GhomalaLimbum/maml/hypotheses/ref.Limbum-Ghomala.test.txt : 12.290000\n",
      "INFO - 05/29/20 19:32:58 - 0:02:36 - epoch -> 90.000000\n",
      "INFO - 05/29/20 19:32:58 - 0:02:36 - valid_Ghomala_mlm_ppl -> 84.290896\n",
      "INFO - 05/29/20 19:32:58 - 0:02:36 - valid_Ghomala_mlm_acc -> 30.051813\n",
      "INFO - 05/29/20 19:32:58 - 0:02:36 - valid_Limbum_mlm_ppl -> 50.457349\n",
      "INFO - 05/29/20 19:32:58 - 0:02:36 - valid_Limbum_mlm_acc -> 35.233161\n",
      "INFO - 05/29/20 19:32:58 - 0:02:36 - valid_Ghomala_Limbum_mlm_ppl -> 114.137167\n",
      "INFO - 05/29/20 19:32:58 - 0:02:36 - valid_Ghomala_Limbum_mlm_acc -> 22.038011\n",
      "INFO - 05/29/20 19:32:58 - 0:02:36 - valid_Ghomala-Limbum_mt_ppl -> 60.070412\n",
      "INFO - 05/29/20 19:32:58 - 0:02:36 - valid_Ghomala-Limbum_mt_acc -> 47.800084\n",
      "INFO - 05/29/20 19:32:58 - 0:02:36 - valid_Ghomala-Limbum_mt_bleu -> 16.230000\n",
      "INFO - 05/29/20 19:32:58 - 0:02:36 - valid_Limbum-Ghomala_mt_ppl -> 165.954177\n",
      "INFO - 05/29/20 19:32:58 - 0:02:36 - valid_Limbum-Ghomala_mt_acc -> 40.592788\n",
      "INFO - 05/29/20 19:32:58 - 0:02:36 - valid_Limbum-Ghomala_mt_bleu -> 11.410000\n",
      "INFO - 05/29/20 19:32:58 - 0:02:36 - valid_mlm_ppl -> 67.374123\n",
      "INFO - 05/29/20 19:32:58 - 0:02:36 - valid_mlm_acc -> 32.642487\n",
      "INFO - 05/29/20 19:32:58 - 0:02:36 - test_Ghomala_mlm_ppl -> 89.526628\n",
      "INFO - 05/29/20 19:32:58 - 0:02:36 - test_Ghomala_mlm_acc -> 24.093264\n",
      "INFO - 05/29/20 19:32:58 - 0:02:36 - test_Limbum_mlm_ppl -> 84.133136\n",
      "INFO - 05/29/20 19:32:58 - 0:02:36 - test_Limbum_mlm_acc -> 30.310881\n",
      "INFO - 05/29/20 19:32:58 - 0:02:36 - test_Ghomala_Limbum_mlm_ppl -> 109.328478\n",
      "INFO - 05/29/20 19:32:58 - 0:02:36 - test_Ghomala_Limbum_mlm_acc -> 20.933063\n",
      "INFO - 05/29/20 19:32:58 - 0:02:36 - test_Ghomala-Limbum_mt_ppl -> 49.488625\n",
      "INFO - 05/29/20 19:32:58 - 0:02:36 - test_Ghomala-Limbum_mt_acc -> 49.250600\n",
      "INFO - 05/29/20 19:32:58 - 0:02:36 - test_Ghomala-Limbum_mt_bleu -> 18.310000\n",
      "INFO - 05/29/20 19:32:58 - 0:02:36 - test_Limbum-Ghomala_mt_ppl -> 141.788700\n",
      "INFO - 05/29/20 19:32:58 - 0:02:36 - test_Limbum-Ghomala_mt_acc -> 41.649602\n",
      "INFO - 05/29/20 19:32:58 - 0:02:36 - test_Limbum-Ghomala_mt_bleu -> 12.290000\n",
      "INFO - 05/29/20 19:32:58 - 0:02:36 - test_mlm_ppl -> 86.829882\n",
      "INFO - 05/29/20 19:32:58 - 0:02:36 - test_mlm_acc -> 27.202073\n",
      "INFO - 05/29/20 19:32:58 - 0:02:36 - __log__:{\"epoch\": 90, \"valid_Ghomala_mlm_ppl\": 84.29089629405752, \"valid_Ghomala_mlm_acc\": 30.05181347150259, \"valid_Limbum_mlm_ppl\": 50.4573489386975, \"valid_Limbum_mlm_acc\": 35.233160621761655, \"valid_Ghomala_Limbum_mlm_ppl\": 114.1371672853313, \"valid_Ghomala_Limbum_mlm_acc\": 22.0380105135463, \"valid_Ghomala-Limbum_mt_ppl\": 60.07041230298429, \"valid_Ghomala-Limbum_mt_acc\": 47.80008414978662, \"valid_Ghomala-Limbum_mt_bleu\": 16.23, \"valid_Limbum-Ghomala_mt_ppl\": 165.95417662420658, \"valid_Limbum-Ghomala_mt_acc\": 40.59278823227321, \"valid_Limbum-Ghomala_mt_bleu\": 11.41, \"valid_mlm_ppl\": 67.3741226163775, \"valid_mlm_acc\": 32.64248704663213, \"test_Ghomala_mlm_ppl\": 89.52662758518315, \"test_Ghomala_mlm_acc\": 24.093264248704664, \"test_Limbum_mlm_ppl\": 84.13313631176776, \"test_Limbum_mlm_acc\": 30.310880829015545, \"test_Ghomala_Limbum_mlm_ppl\": 109.3284776423275, \"test_Ghomala_Limbum_mlm_acc\": 20.933062880324545, \"test_Ghomala-Limbum_mt_ppl\": 49.48862496620382, \"test_Ghomala-Limbum_mt_acc\": 49.25060043170279, \"test_Ghomala-Limbum_mt_bleu\": 18.31, \"test_Limbum-Ghomala_mt_ppl\": 141.78869963895986, \"test_Limbum-Ghomala_mt_acc\": 41.64960182025028, \"test_Limbum-Ghomala_mt_bleu\": 12.29, \"test_mlm_ppl\": 86.82988194847545, \"test_mlm_acc\": 27.202072538860104}\n"
     ]
    }
   ],
   "source": [
    "%env eval_bleu=True\n",
    "! chmod +x src/evaluation/multi-bleu.perl\n",
    "\n",
    "%env stopping_criterion=valid_Ghomala-Limbum_mt_bleu,10\n",
    "%env validation_metrics=valid_Ghomala-Limbum_mt_bleu\n",
    "%env reload_model=/home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml/best-valid_mlm_ppl.pth,/home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml/best-valid_mlm_ppl.pth\n",
    "%env ae_steps=Ghomala,Limbum\n",
    "%env bt_steps=Ghomala-Limbum-Ghomala,Limbum-Ghomala-Limbum\n",
    "%env mt_steps=Ghomala-Limbum,Limbum-Ghomala  \n",
    "\n",
    "%env mlm_steps=Ghomala,Limbum,Ghomala-Limbum     \n",
    "! python train.py --eval_only $eval_only --mlm_steps $mlm_steps --exp_name SupMT_GhomalaLimbum --exp_id $exp_id  --dump_path $dump_path --reload_model $reload_model --data_path $OUTPATH --lgs $lgs --ae_steps $ae_steps --mt_steps $mt_steps --bt_steps $bt_steps --word_shuffle 3 --word_dropout 0.1 --word_blank 0.1 --lambda_ae '0:1,100000:0.1,300000:0' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout 0.1 --attention_dropout 0.1 --gelu_activation true --tokens_per_batch 2000 --batch_size $batch_size --bptt 256 --optimizer adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001 --epoch_size $epoch_size --max_epoch $max_epoch --eval_bleu $eval_bleu --stopping_criterion $stopping_criterion --validation_metrics $validation_metrics --train_n_samples $train_n_samples --valid_n_samples $valid_n_samples --test_n_samples $test_n_samples --remove_long_sentences_train $remove_long_sentences_train --remove_long_sentences_valid $remove_long_sentences_valid --remove_long_sentences_test $remove_long_sentences_test   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XLM Ghomala Ngiemboon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OUTPATH=/home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed\n",
      "env: epoch_size=6339\n",
      "env: lgs=Ghomala-Ngiemboon\n",
      "env: batch_size=32\n",
      "env: max_epoch=100\n",
      "env: dump_path=/home/jupyter/models/africa/cluster3\n"
     ]
    }
   ],
   "source": [
    "%env OUTPATH=/home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed\n",
    "%env epoch_size=6339\n",
    "%env lgs=Ghomala-Ngiemboon\n",
    "%env batch_size=32\n",
    "%env max_epoch=100\n",
    "%env dump_path=/home/jupyter/models/africa/cluster3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLM + TLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: stopping_criterion=_valid_mlm_ppl,10\n",
      "env: eval_bleu=False\n",
      "env: mlm_steps=Ghomala,Ngiemboon,Ghomala-Ngiemboon\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed' --lgs 'Ghomala-Ngiemboon' --clm_steps '' --mlm_steps 'Ghomala,Ngiemboon,Ghomala-Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 6339 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 6339\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_GhomalaNgiemboon\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Ghomala', 1: 'Ngiemboon'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Ghomala': 0, 'Ngiemboon': 1}\n",
      "                                     langs: ['Ghomala', 'Ngiemboon']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Ghomala-Ngiemboon']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Ghomala', None), ('Ngiemboon', None), ('Ghomala', 'Ngiemboon')]\n",
      "                                     mono_dataset: {'Ghomala': {'train': '/home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/train.Ghomala.pth', 'valid': '/home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/valid.Ghomala.pth', 'test': '/home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/test.Ghomala.pth'}, 'Ngiemboon': {'train': '/home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/train.Ngiemboon.pth', 'valid': '/home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/valid.Ngiemboon.pth', 'test': '/home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/test.Ngiemboon.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {('Ghomala', 'Ngiemboon'): {'train': ('/home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/train.Ghomala-Ngiemboon.Ghomala.pth', '/home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/train.Ghomala-Ngiemboon.Ngiemboon.pth'), 'valid': ('/home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/valid.Ghomala-Ngiemboon.Ghomala.pth', '/home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/valid.Ghomala-Ngiemboon.Ngiemboon.pth'), 'test': ('/home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/test.Ghomala-Ngiemboon.Ghomala.pth', '/home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/test.Ghomala-Ngiemboon.Ngiemboon.pth')}}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': True, 'valid': True, 'test': True}\n",
      "                                     remove_long_sentences_test: True\n",
      "                                     remove_long_sentences_train: True\n",
      "                                     remove_long_sentences_valid: True\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml\n",
      "                                     \n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed' --lgs 'Ghomala-Ngiemboon' --clm_steps '' --mlm_steps 'Ghomala,Ngiemboon,Ghomala-Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 6339 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/29/20 19:34:05 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - ============ langs: Ghomala, Ngiemboon\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - ============ Monolingual data (Ghomala)\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/valid.Ghomala.pth ...\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - 26226 words (6715 unique) in 792 sentences. 5 unknown words (5 unique) covering 0.02% of the data.\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/test.Ghomala.pth ...\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - 26371 words (6715 unique) in 792 sentences. 7 unknown words (6 unique) covering 0.03% of the data.\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - ============ Monolingual data (Ngiemboon)\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/valid.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - 27961 words (6715 unique) in 792 sentences. 10 unknown words (6 unique) covering 0.04% of the data.\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/test.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - 28421 words (6715 unique) in 792 sentences. 17 unknown words (14 unique) covering 0.06% of the data.\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - ============ Parallel data (Ghomala-Ngiemboon)\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/valid.Ghomala-Ngiemboon.Ghomala.pth ...\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - 26226 words (6715 unique) in 792 sentences. 5 unknown words (5 unique) covering 0.02% of the data.\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/valid.Ghomala-Ngiemboon.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - 27961 words (6715 unique) in 792 sentences. 10 unknown words (6 unique) covering 0.04% of the data.\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - ========================== debug : 5\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - Removed 2 too long sentences.\n",
      "\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/test.Ghomala-Ngiemboon.Ghomala.pth ...\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - 26371 words (6715 unique) in 792 sentences. 7 unknown words (6 unique) covering 0.03% of the data.\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/test.Ghomala-Ngiemboon.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - 28421 words (6715 unique) in 792 sentences. 17 unknown words (14 unique) covering 0.06% of the data.\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - ========================== debug : 6\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - Removed 1 too long sentences.\n",
      "\n",
      "\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - Monolingual data   - valid -      Ghomala:       792\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - Monolingual data   -  test -      Ghomala:       792\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - Monolingual data   - valid -    Ngiemboon:       792\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - Monolingual data   -  test -    Ngiemboon:       792\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - Parallel data      - valid - Ghomala-Ngiemboon:       790\n",
      "INFO - 05/29/20 19:34:05 - 0:00:00 - Parallel data      -  test - Ghomala-Ngiemboon:       791\n",
      "\n",
      "INFO - 05/29/20 19:34:06 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(6715, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=6715, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/29/20 19:34:06 - 0:00:01 - Number of parameters (model): 82988603\n",
      "INFO - 05/29/20 19:34:09 - 0:00:04 - Found 0 memories.\n",
      "INFO - 05/29/20 19:34:09 - 0:00:04 - Found 6 FFN.\n",
      "INFO - 05/29/20 19:34:09 - 0:00:04 - Found 102 parameters in model.\n",
      "INFO - 05/29/20 19:34:09 - 0:00:04 - Optimizers: model\n",
      "WARNING - 05/29/20 19:34:09 - 0:00:04 - Reloading checkpoint from /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml/checkpoint.pth ...\n",
      "WARNING - 05/29/20 19:34:19 - 0:00:14 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/29/20 19:34:19 - 0:00:14 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/29/20 19:34:19 - 0:00:14 - Checkpoint reloaded. Resuming at epoch 40 / iteration 10600 ...\n",
      "INFO - 05/29/20 19:34:22 - 0:00:18 - epoch -> 40.000000\n",
      "INFO - 05/29/20 19:34:22 - 0:00:18 - valid_Ghomala_mlm_ppl -> 13.589504\n",
      "INFO - 05/29/20 19:34:22 - 0:00:18 - valid_Ghomala_mlm_acc -> 48.963731\n",
      "INFO - 05/29/20 19:34:22 - 0:00:18 - valid_Ngiemboon_mlm_ppl -> 23.640575\n",
      "INFO - 05/29/20 19:34:22 - 0:00:18 - valid_Ngiemboon_mlm_acc -> 45.854922\n",
      "INFO - 05/29/20 19:34:22 - 0:00:18 - valid_Ghomala_Ngiemboon_mlm_ppl -> 17.008343\n",
      "INFO - 05/29/20 19:34:22 - 0:00:18 - valid_Ghomala_Ngiemboon_mlm_acc -> 47.847100\n",
      "INFO - 05/29/20 19:34:22 - 0:00:18 - valid_mlm_ppl -> 18.615040\n",
      "INFO - 05/29/20 19:34:22 - 0:00:18 - valid_mlm_acc -> 47.409326\n",
      "INFO - 05/29/20 19:34:22 - 0:00:18 - test_Ghomala_mlm_ppl -> 15.253618\n",
      "INFO - 05/29/20 19:34:22 - 0:00:18 - test_Ghomala_mlm_acc -> 48.963731\n",
      "INFO - 05/29/20 19:34:22 - 0:00:18 - test_Ngiemboon_mlm_ppl -> 25.393331\n",
      "INFO - 05/29/20 19:34:22 - 0:00:18 - test_Ngiemboon_mlm_acc -> 44.300518\n",
      "INFO - 05/29/20 19:34:22 - 0:00:18 - test_Ghomala_Ngiemboon_mlm_ppl -> 15.660768\n",
      "INFO - 05/29/20 19:34:22 - 0:00:18 - test_Ghomala_Ngiemboon_mlm_acc -> 50.043253\n",
      "INFO - 05/29/20 19:34:22 - 0:00:18 - test_mlm_ppl -> 20.323474\n",
      "INFO - 05/29/20 19:34:22 - 0:00:18 - test_mlm_acc -> 46.632124\n",
      "INFO - 05/29/20 19:34:22 - 0:00:18 - __log__:{\"epoch\": 40, \"valid_Ghomala_mlm_ppl\": 13.589504034553652, \"valid_Ghomala_mlm_acc\": 48.96373056994819, \"valid_Ngiemboon_mlm_ppl\": 23.64057501468406, \"valid_Ngiemboon_mlm_acc\": 45.854922279792746, \"valid_Ghomala_Ngiemboon_mlm_ppl\": 17.00834328124254, \"valid_Ghomala_Ngiemboon_mlm_acc\": 47.847100175746924, \"valid_mlm_ppl\": 18.615039524618858, \"valid_mlm_acc\": 47.40932642487047, \"test_Ghomala_mlm_ppl\": 15.253618031506726, \"test_Ghomala_mlm_acc\": 48.96373056994819, \"test_Ngiemboon_mlm_ppl\": 25.39333069264686, \"test_Ngiemboon_mlm_acc\": 44.30051813471503, \"test_Ghomala_Ngiemboon_mlm_ppl\": 15.660768016308232, \"test_Ghomala_Ngiemboon_mlm_acc\": 50.04325259515571, \"test_mlm_ppl\": 20.323474362076794, \"test_mlm_acc\": 46.63212435233161}\n"
     ]
    }
   ],
   "source": [
    "%env stopping_criterion=_valid_mlm_ppl,10\n",
    "%env eval_bleu=False\n",
    "%env mlm_steps=Ghomala,Ngiemboon,Ghomala-Ngiemboon\n",
    "! python train.py --eval_only $eval_only --exp_name mlm_tlm_GhomalaNgiemboon --exp_id $exp_id --dump_path $dump_path --data_path $OUTPATH --lgs $lgs --clm_steps '' --mlm_steps $mlm_steps --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout 0.1 --attention_dropout 0.1 --gelu_activation true --batch_size $batch_size --bptt 256 --optimizer adam,lr=0.0001 --epoch_size $epoch_size --max_epoch $max_epoch --validation_metrics _valid_mlm_ppl --stopping_criterion $stopping_criterion --eval_bleu $eval_bleu --remove_long_sentences_train $remove_long_sentences_train --remove_long_sentences_valid $remove_long_sentences_valid --remove_long_sentences_test $remove_long_sentences_test --train_n_samples $train_n_samples --valid_n_samples $valid_n_samples --test_n_samples $test_n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: eval_bleu=True\n",
      "env: stopping_criterion=valid_Ghomala-Ngiemboon_mt_bleu,10\n",
      "env: validation_metrics=valid_Ghomala-Ngiemboon_mt_bleu\n",
      "env: reload_model=/home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml/best-valid_mlm_ppl.pth,/home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml/best-valid_mlm_ppl.pth\n",
      "env: ae_steps=Ghomala,Ngiemboon\n",
      "env: bt_steps=Ghomala-Ngiemboon-Ghomala,Ngiemboon-Ghomala-Ngiemboon\n",
      "env: mt_steps=Ghomala-Ngiemboon,Ngiemboon-Ghomala\n",
      "env: mlm_steps=Ghomala,Ngiemboon,Ghomala-Ngiemboon\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/29/20 19:34:30 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/29/20 19:34:30 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: ['Ghomala', 'Ngiemboon']\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: ['Ghomala', 'Ngiemboon']\n",
      "                                     bt_steps: [('Ghomala', 'Ngiemboon', 'Ghomala'), ('Ngiemboon', 'Ghomala', 'Ngiemboon')]\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --mlm_steps 'Ghomala,Ngiemboon,Ghomala-Ngiemboon' --exp_name SupMT_GhomalaNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --reload_model '/home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml/best-valid_mlm_ppl.pth,/home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml/best-valid_mlm_ppl.pth' --data_path '/home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed' --lgs 'Ghomala-Ngiemboon' --ae_steps 'Ghomala,Ngiemboon' --mt_steps 'Ghomala-Ngiemboon,Ngiemboon-Ghomala' --bt_steps 'Ghomala-Ngiemboon-Ghomala,Ngiemboon-Ghomala-Ngiemboon' --word_shuffle 3 --word_dropout '0.1' --word_blank '0.1' --lambda_ae '0:1,100000:0.1,300000:0' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --tokens_per_batch 2000 --batch_size 32 --bptt 256 --optimizer 'adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001' --epoch_size 6339 --max_epoch 100 --eval_bleu True --stopping_criterion 'valid_Ghomala-Ngiemboon_mt_bleu,10' --validation_metrics 'valid_Ghomala-Ngiemboon_mt_bleu' --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster3/SupMT_GhomalaNgiemboon/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: False\n",
      "                                     epoch_size: 6339\n",
      "                                     eval_bleu: True\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: SupMT_GhomalaNgiemboon\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Ghomala', 1: 'Ngiemboon'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 0:1,100000:0.1,300000:0\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Ghomala': 0, 'Ngiemboon': 1}\n",
      "                                     langs: ['Ghomala', 'Ngiemboon']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Ghomala-Ngiemboon']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Ghomala', None), ('Ngiemboon', None), ('Ghomala', 'Ngiemboon')]\n",
      "                                     mono_dataset: {'Ghomala': {'train': '/home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/train.Ghomala.pth', 'valid': '/home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/valid.Ghomala.pth', 'test': '/home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/test.Ghomala.pth'}, 'Ngiemboon': {'train': '/home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/train.Ngiemboon.pth', 'valid': '/home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/valid.Ngiemboon.pth', 'test': '/home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/test.Ngiemboon.pth'}}\n",
      "                                     mt_steps: [('Ghomala', 'Ngiemboon'), ('Ngiemboon', 'Ghomala')]\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001\n",
      "                                     para_dataset: {('Ghomala', 'Ngiemboon'): {'train': ('/home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/train.Ghomala-Ngiemboon.Ghomala.pth', '/home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/train.Ghomala-Ngiemboon.Ngiemboon.pth'), 'valid': ('/home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/valid.Ghomala-Ngiemboon.Ghomala.pth', '/home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/valid.Ghomala-Ngiemboon.Ngiemboon.pth'), 'test': ('/home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/test.Ghomala-Ngiemboon.Ghomala.pth', '/home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/test.Ghomala-Ngiemboon.Ngiemboon.pth')}}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml/best-valid_mlm_ppl.pth,/home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml/best-valid_mlm_ppl.pth\n",
      "                                     remove_long_sentences: {'train': True, 'valid': True, 'test': True}\n",
      "                                     remove_long_sentences_test: True\n",
      "                                     remove_long_sentences_train: True\n",
      "                                     remove_long_sentences_valid: True\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: valid_Ghomala-Ngiemboon_mt_bleu,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: 2000\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: valid_Ghomala-Ngiemboon_mt_bleu\n",
      "                                     word_blank: 0.1\n",
      "                                     word_dropout: 0.1\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 3.0\n",
      "                                     world_size: 1\n",
      "INFO - 05/29/20 19:34:30 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster3/SupMT_GhomalaNgiemboon/maml\n",
      "                                     \n",
      "INFO - 05/29/20 19:34:30 - 0:00:00 - Running command: python train.py --eval_only True --mlm_steps 'Ghomala,Ngiemboon,Ghomala-Ngiemboon' --exp_name SupMT_GhomalaNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --reload_model '/home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml/best-valid_mlm_ppl.pth,/home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml/best-valid_mlm_ppl.pth' --data_path '/home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed' --lgs 'Ghomala-Ngiemboon' --ae_steps 'Ghomala,Ngiemboon' --mt_steps 'Ghomala-Ngiemboon,Ngiemboon-Ghomala' --bt_steps 'Ghomala-Ngiemboon-Ghomala,Ngiemboon-Ghomala-Ngiemboon' --word_shuffle 3 --word_dropout '0.1' --word_blank '0.1' --lambda_ae '0:1,100000:0.1,300000:0' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --tokens_per_batch 2000 --batch_size 32 --bptt 256 --optimizer 'adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001' --epoch_size 6339 --max_epoch 100 --eval_bleu True --stopping_criterion 'valid_Ghomala-Ngiemboon_mt_bleu,10' --validation_metrics 'valid_Ghomala-Ngiemboon_mt_bleu' --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True\n",
      "\n",
      "WARNING - 05/29/20 19:34:30 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/29/20 19:34:30 - 0:00:00 - ============ langs: Ghomala, Ngiemboon\n",
      "INFO - 05/29/20 19:34:30 - 0:00:00 - ============ Monolingual data (Ghomala)\n",
      "INFO - 05/29/20 19:34:30 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/valid.Ghomala.pth ...\n",
      "INFO - 05/29/20 19:34:30 - 0:00:00 - 26226 words (6715 unique) in 792 sentences. 5 unknown words (5 unique) covering 0.02% of the data.\n",
      "INFO - 05/29/20 19:34:30 - 0:00:00 - ========================== debug : 1\n",
      "INFO - 05/29/20 19:34:30 - 0:00:00 - ========================== debug : 2\n",
      "INFO - 05/29/20 19:34:30 - 0:00:00 - Removed 0 too long sentences.\n",
      "\n",
      "INFO - 05/29/20 19:34:30 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/test.Ghomala.pth ...\n",
      "INFO - 05/29/20 19:34:30 - 0:00:00 - 26371 words (6715 unique) in 792 sentences. 7 unknown words (6 unique) covering 0.03% of the data.\n",
      "INFO - 05/29/20 19:34:30 - 0:00:00 - ========================== debug : 3\n",
      "INFO - 05/29/20 19:34:30 - 0:00:00 - ========================== debug : 4\n",
      "INFO - 05/29/20 19:34:30 - 0:00:00 - Removed 0 too long sentences.\n",
      "\n",
      "INFO - 05/29/20 19:34:30 - 0:00:00 - ============ Monolingual data (Ngiemboon)\n",
      "INFO - 05/29/20 19:34:30 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/valid.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 19:34:30 - 0:00:00 - 27961 words (6715 unique) in 792 sentences. 10 unknown words (6 unique) covering 0.04% of the data.\n",
      "INFO - 05/29/20 19:34:30 - 0:00:00 - ========================== debug : 5\n",
      "INFO - 05/29/20 19:34:30 - 0:00:00 - ========================== debug : 6\n",
      "INFO - 05/29/20 19:34:30 - 0:00:00 - Removed 2 too long sentences.\n",
      "\n",
      "INFO - 05/29/20 19:34:30 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/test.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 19:34:30 - 0:00:00 - 28421 words (6715 unique) in 792 sentences. 17 unknown words (14 unique) covering 0.06% of the data.\n",
      "INFO - 05/29/20 19:34:30 - 0:00:00 - ========================== debug : 7\n",
      "INFO - 05/29/20 19:34:30 - 0:00:00 - ========================== debug : 8\n",
      "INFO - 05/29/20 19:34:30 - 0:00:00 - Removed 1 too long sentences.\n",
      "\n",
      "\n",
      "INFO - 05/29/20 19:34:30 - 0:00:00 - ============ Parallel data (Ghomala-Ngiemboon)\n",
      "INFO - 05/29/20 19:34:30 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/valid.Ghomala-Ngiemboon.Ghomala.pth ...\n",
      "INFO - 05/29/20 19:34:30 - 0:00:00 - 26226 words (6715 unique) in 792 sentences. 5 unknown words (5 unique) covering 0.02% of the data.\n",
      "INFO - 05/29/20 19:34:30 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/valid.Ghomala-Ngiemboon.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 19:34:31 - 0:00:00 - 27961 words (6715 unique) in 792 sentences. 10 unknown words (6 unique) covering 0.04% of the data.\n",
      "INFO - 05/29/20 19:34:31 - 0:00:00 - ========================== debug : 9\n",
      "INFO - 05/29/20 19:34:31 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/29/20 19:34:31 - 0:00:00 - Removed 2 too long sentences.\n",
      "\n",
      "INFO - 05/29/20 19:34:31 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/test.Ghomala-Ngiemboon.Ghomala.pth ...\n",
      "INFO - 05/29/20 19:34:31 - 0:00:00 - 26371 words (6715 unique) in 792 sentences. 7 unknown words (6 unique) covering 0.03% of the data.\n",
      "INFO - 05/29/20 19:34:31 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/test.Ghomala-Ngiemboon.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 19:34:31 - 0:00:00 - 28421 words (6715 unique) in 792 sentences. 17 unknown words (14 unique) covering 0.06% of the data.\n",
      "INFO - 05/29/20 19:34:31 - 0:00:00 - ========================== debug : 10\n",
      "INFO - 05/29/20 19:34:31 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/29/20 19:34:31 - 0:00:00 - Removed 1 too long sentences.\n",
      "\n",
      "\n",
      "INFO - 05/29/20 19:34:31 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/29/20 19:34:31 - 0:00:00 - Monolingual data   - valid -      Ghomala:       792\n",
      "INFO - 05/29/20 19:34:31 - 0:00:00 - Monolingual data   -  test -      Ghomala:       792\n",
      "INFO - 05/29/20 19:34:31 - 0:00:00 - Monolingual data   - valid -    Ngiemboon:       792\n",
      "INFO - 05/29/20 19:34:31 - 0:00:00 - Monolingual data   -  test -    Ngiemboon:       792\n",
      "INFO - 05/29/20 19:34:31 - 0:00:00 - Parallel data      - valid - Ghomala-Ngiemboon:       790\n",
      "INFO - 05/29/20 19:34:31 - 0:00:00 - Parallel data      -  test - Ghomala-Ngiemboon:       791\n",
      "\n",
      "INFO - 05/29/20 19:34:32 - 0:00:02 - Reloading encoder from /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml/best-valid_mlm_ppl.pth ...\n",
      "INFO - 05/29/20 19:34:39 - 0:00:08 - Reloading decoder from /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml/best-valid_mlm_ppl.pth ...\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter layer_norm15.0.weight not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter layer_norm15.0.bias not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.0.q_lin.weight not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.0.q_lin.bias not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.0.k_lin.weight not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.0.k_lin.bias not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.0.v_lin.weight not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.0.v_lin.bias not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.0.out_lin.weight not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.0.out_lin.bias not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter layer_norm15.1.weight not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter layer_norm15.1.bias not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.1.q_lin.weight not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.1.q_lin.bias not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.1.k_lin.weight not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.1.k_lin.bias not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.1.v_lin.weight not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.1.v_lin.bias not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.1.out_lin.weight not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.1.out_lin.bias not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter layer_norm15.2.weight not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter layer_norm15.2.bias not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.2.q_lin.weight not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.2.q_lin.bias not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.2.k_lin.weight not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.2.k_lin.bias not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.2.v_lin.weight not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.2.v_lin.bias not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.2.out_lin.weight not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.2.out_lin.bias not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter layer_norm15.3.weight not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter layer_norm15.3.bias not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.3.q_lin.weight not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.3.q_lin.bias not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.3.k_lin.weight not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.3.k_lin.bias not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.3.v_lin.weight not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.3.v_lin.bias not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.3.out_lin.weight not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.3.out_lin.bias not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter layer_norm15.4.weight not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter layer_norm15.4.bias not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.4.q_lin.weight not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.4.q_lin.bias not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.4.k_lin.weight not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.4.k_lin.bias not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.4.v_lin.weight not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.4.v_lin.bias not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.4.out_lin.weight not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.4.out_lin.bias not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter layer_norm15.5.weight not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter layer_norm15.5.bias not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.5.q_lin.weight not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.5.q_lin.bias not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.5.k_lin.weight not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.5.k_lin.bias not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.5.v_lin.weight not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.5.v_lin.bias not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.5.out_lin.weight not found.\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Parameter encoder_attn.5.out_lin.bias not found.\n",
      "INFO - 05/29/20 19:34:39 - 0:00:09 - Number of parameters (encoder): 82988603\n",
      "INFO - 05/29/20 19:34:39 - 0:00:09 - Number of parameters (decoder): 108191291\n",
      "INFO - 05/29/20 19:34:39 - 0:00:09 - Found 0 memories.\n",
      "INFO - 05/29/20 19:34:39 - 0:00:09 - Found 12 FFN.\n",
      "INFO - 05/29/20 19:34:39 - 0:00:09 - Found 264 parameters in model.\n",
      "INFO - 05/29/20 19:34:39 - 0:00:09 - Optimizers: model\n",
      "WARNING - 05/29/20 19:34:39 - 0:00:09 - Reloading checkpoint from /home/jupyter/models/africa/cluster3/SupMT_GhomalaNgiemboon/maml/checkpoint.pth ...\n",
      "WARNING - 05/29/20 19:35:01 - 0:00:30 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/29/20 19:35:01 - 0:00:30 - Reloading 'num_updates' and 'lr' for optimizer model.\n",
      "WARNING - 05/29/20 19:35:01 - 0:00:30 - Checkpoint reloaded. Resuming at epoch 30 / iteration 1119 ...\n",
      "INFO - 05/29/20 19:35:25 - 0:00:55 - BLEU /home/jupyter/models/africa/cluster3/SupMT_GhomalaNgiemboon/maml/hypotheses/hyp30.Ghomala-Ngiemboon.valid.txt /home/jupyter/models/africa/cluster3/SupMT_GhomalaNgiemboon/maml/hypotheses/ref.Ghomala-Ngiemboon.valid.txt : 5.860000\n",
      "INFO - 05/29/20 19:35:55 - 0:01:25 - BLEU /home/jupyter/models/africa/cluster3/SupMT_GhomalaNgiemboon/maml/hypotheses/hyp30.Ngiemboon-Ghomala.valid.txt /home/jupyter/models/africa/cluster3/SupMT_GhomalaNgiemboon/maml/hypotheses/ref.Ngiemboon-Ghomala.valid.txt : 7.540000\n",
      "INFO - 05/29/20 19:36:31 - 0:02:01 - BLEU /home/jupyter/models/africa/cluster3/SupMT_GhomalaNgiemboon/maml/hypotheses/hyp30.Ghomala-Ngiemboon.test.txt /home/jupyter/models/africa/cluster3/SupMT_GhomalaNgiemboon/maml/hypotheses/ref.Ghomala-Ngiemboon.test.txt : 6.360000\n",
      "INFO - 05/29/20 19:37:09 - 0:02:38 - BLEU /home/jupyter/models/africa/cluster3/SupMT_GhomalaNgiemboon/maml/hypotheses/hyp30.Ngiemboon-Ghomala.test.txt /home/jupyter/models/africa/cluster3/SupMT_GhomalaNgiemboon/maml/hypotheses/ref.Ngiemboon-Ghomala.test.txt : 7.560000\n",
      "INFO - 05/29/20 19:37:09 - 0:02:38 - epoch -> 30.000000\n",
      "INFO - 05/29/20 19:37:09 - 0:02:38 - valid_Ghomala_mlm_ppl -> 68.536744\n",
      "INFO - 05/29/20 19:37:09 - 0:02:38 - valid_Ghomala_mlm_acc -> 27.461140\n",
      "INFO - 05/29/20 19:37:09 - 0:02:38 - valid_Ngiemboon_mlm_ppl -> 139.974046\n",
      "INFO - 05/29/20 19:37:09 - 0:02:38 - valid_Ngiemboon_mlm_acc -> 24.870466\n",
      "INFO - 05/29/20 19:37:09 - 0:02:38 - valid_Ghomala_Ngiemboon_mlm_ppl -> 179.970086\n",
      "INFO - 05/29/20 19:37:09 - 0:02:38 - valid_Ghomala_Ngiemboon_mlm_acc -> 23.506151\n",
      "INFO - 05/29/20 19:37:09 - 0:02:38 - valid_Ghomala-Ngiemboon_mt_ppl -> 86.548442\n",
      "INFO - 05/29/20 19:37:09 - 0:02:38 - valid_Ghomala-Ngiemboon_mt_acc -> 34.783676\n",
      "INFO - 05/29/20 19:37:09 - 0:02:38 - valid_Ghomala-Ngiemboon_mt_bleu -> 5.860000\n",
      "INFO - 05/29/20 19:37:09 - 0:02:38 - valid_Ngiemboon-Ghomala_mt_ppl -> 51.364800\n",
      "INFO - 05/29/20 19:37:09 - 0:02:38 - valid_Ngiemboon-Ghomala_mt_acc -> 37.577224\n",
      "INFO - 05/29/20 19:37:09 - 0:02:38 - valid_Ngiemboon-Ghomala_mt_bleu -> 7.540000\n",
      "INFO - 05/29/20 19:37:09 - 0:02:38 - valid_mlm_ppl -> 104.255395\n",
      "INFO - 05/29/20 19:37:09 - 0:02:38 - valid_mlm_acc -> 26.165803\n",
      "INFO - 05/29/20 19:37:09 - 0:02:38 - test_Ghomala_mlm_ppl -> 84.358487\n",
      "INFO - 05/29/20 19:37:09 - 0:02:38 - test_Ghomala_mlm_acc -> 28.238342\n",
      "INFO - 05/29/20 19:37:09 - 0:02:38 - test_Ngiemboon_mlm_ppl -> 143.215752\n",
      "INFO - 05/29/20 19:37:09 - 0:02:38 - test_Ngiemboon_mlm_acc -> 24.352332\n",
      "INFO - 05/29/20 19:37:09 - 0:02:38 - test_Ghomala_Ngiemboon_mlm_ppl -> 160.923497\n",
      "INFO - 05/29/20 19:37:09 - 0:02:38 - test_Ghomala_Ngiemboon_mlm_acc -> 24.091696\n",
      "INFO - 05/29/20 19:37:09 - 0:02:38 - test_Ghomala-Ngiemboon_mt_ppl -> 96.161124\n",
      "INFO - 05/29/20 19:37:09 - 0:02:38 - test_Ghomala-Ngiemboon_mt_acc -> 34.551826\n",
      "INFO - 05/29/20 19:37:09 - 0:02:38 - test_Ghomala-Ngiemboon_mt_bleu -> 6.360000\n",
      "INFO - 05/29/20 19:37:09 - 0:02:38 - test_Ngiemboon-Ghomala_mt_ppl -> 53.591743\n",
      "INFO - 05/29/20 19:37:09 - 0:02:38 - test_Ngiemboon-Ghomala_mt_acc -> 37.348731\n",
      "INFO - 05/29/20 19:37:09 - 0:02:38 - test_Ngiemboon-Ghomala_mt_bleu -> 7.560000\n",
      "INFO - 05/29/20 19:37:09 - 0:02:38 - test_mlm_ppl -> 113.787119\n",
      "INFO - 05/29/20 19:37:09 - 0:02:38 - test_mlm_acc -> 26.295337\n",
      "INFO - 05/29/20 19:37:09 - 0:02:38 - __log__:{\"epoch\": 30, \"valid_Ghomala_mlm_ppl\": 68.53674412201153, \"valid_Ghomala_mlm_acc\": 27.46113989637306, \"valid_Ngiemboon_mlm_ppl\": 139.97404598037033, \"valid_Ngiemboon_mlm_acc\": 24.870466321243523, \"valid_Ghomala_Ngiemboon_mlm_ppl\": 179.97008585463237, \"valid_Ghomala_Ngiemboon_mlm_acc\": 23.50615114235501, \"valid_Ghomala-Ngiemboon_mt_ppl\": 86.54844169602647, \"valid_Ghomala-Ngiemboon_mt_acc\": 34.78367575906318, \"valid_Ghomala-Ngiemboon_mt_bleu\": 5.86, \"valid_Ngiemboon-Ghomala_mt_ppl\": 51.36480018973592, \"valid_Ngiemboon-Ghomala_mt_acc\": 37.57722366951991, \"valid_Ngiemboon-Ghomala_mt_bleu\": 7.54, \"valid_mlm_ppl\": 104.25539505119093, \"valid_mlm_acc\": 26.16580310880829, \"test_Ghomala_mlm_ppl\": 84.35848713629483, \"test_Ghomala_mlm_acc\": 28.238341968911918, \"test_Ngiemboon_mlm_ppl\": 143.21575161310847, \"test_Ngiemboon_mlm_acc\": 24.352331606217618, \"test_Ghomala_Ngiemboon_mlm_ppl\": 160.92349718180503, \"test_Ghomala_Ngiemboon_mlm_acc\": 24.091695501730104, \"test_Ghomala-Ngiemboon_mt_ppl\": 96.16112432613086, \"test_Ghomala-Ngiemboon_mt_acc\": 34.551826021232, \"test_Ghomala-Ngiemboon_mt_bleu\": 6.36, \"test_Ngiemboon-Ghomala_mt_ppl\": 53.59174287643477, \"test_Ngiemboon-Ghomala_mt_acc\": 37.34873081463991, \"test_Ngiemboon-Ghomala_mt_bleu\": 7.56, \"test_mlm_ppl\": 113.78711937470165, \"test_mlm_acc\": 26.295336787564768}\n"
     ]
    }
   ],
   "source": [
    "%env eval_bleu=True\n",
    "! chmod +x src/evaluation/multi-bleu.perl\n",
    "\n",
    "%env stopping_criterion=valid_Ghomala-Ngiemboon_mt_bleu,10\n",
    "%env validation_metrics=valid_Ghomala-Ngiemboon_mt_bleu\n",
    "%env reload_model=/home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml/best-valid_mlm_ppl.pth,/home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml/best-valid_mlm_ppl.pth\n",
    "%env ae_steps=Ghomala,Ngiemboon\n",
    "%env bt_steps=Ghomala-Ngiemboon-Ghomala,Ngiemboon-Ghomala-Ngiemboon\n",
    "%env mt_steps=Ghomala-Ngiemboon,Ngiemboon-Ghomala       \n",
    "\n",
    "%env mlm_steps=Ghomala,Ngiemboon,Ghomala-Ngiemboon\n",
    "! python train.py --eval_only $eval_only --mlm_steps $mlm_steps --exp_name SupMT_GhomalaNgiemboon --exp_id $exp_id  --dump_path $dump_path --reload_model $reload_model --data_path $OUTPATH --lgs $lgs --ae_steps $ae_steps --mt_steps $mt_steps --bt_steps $bt_steps --word_shuffle 3 --word_dropout 0.1 --word_blank 0.1 --lambda_ae '0:1,100000:0.1,300000:0' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout 0.1 --attention_dropout 0.1 --gelu_activation true --tokens_per_batch 2000 --batch_size $batch_size --bptt 256 --optimizer adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001 --epoch_size $epoch_size --max_epoch $max_epoch --eval_bleu $eval_bleu --stopping_criterion $stopping_criterion --validation_metrics $validation_metrics --train_n_samples $train_n_samples --valid_n_samples $valid_n_samples --test_n_samples $test_n_samples --remove_long_sentences_train $remove_long_sentences_train --remove_long_sentences_valid $remove_long_sentences_valid --remove_long_sentences_test $remove_long_sentences_test    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XLM Limbum Ngiemboon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OUTPATH=/home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed\n",
      "env: epoch_size=6322\n",
      "env: lgs=Limbum-Ngiemboon\n",
      "env: batch_size=32\n",
      "env: max_epoch=100\n",
      "env: dump_path=/home/jupyter/models/africa/cluster3\n"
     ]
    }
   ],
   "source": [
    "%env OUTPATH=/home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed\n",
    "%env epoch_size=6322\n",
    "%env lgs=Limbum-Ngiemboon\n",
    "%env batch_size=32\n",
    "%env max_epoch=100\n",
    "%env dump_path=/home/jupyter/models/africa/cluster3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLM + TLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: stopping_criterion=_valid_mlm_ppl,10\n",
      "env: eval_bleu=False\n",
      "env: mlm_steps=Limbum,Ngiemboon,Limbum-Ngiemboon\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_LimbumNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed' --lgs 'Limbum-Ngiemboon' --clm_steps '' --mlm_steps 'Limbum,Ngiemboon,Limbum-Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 6322 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 6322\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_LimbumNgiemboon\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Limbum', 1: 'Ngiemboon'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Limbum': 0, 'Ngiemboon': 1}\n",
      "                                     langs: ['Limbum', 'Ngiemboon']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Limbum-Ngiemboon']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Limbum', None), ('Ngiemboon', None), ('Limbum', 'Ngiemboon')]\n",
      "                                     mono_dataset: {'Limbum': {'train': '/home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/train.Limbum.pth', 'valid': '/home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/valid.Limbum.pth', 'test': '/home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/test.Limbum.pth'}, 'Ngiemboon': {'train': '/home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/train.Ngiemboon.pth', 'valid': '/home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/valid.Ngiemboon.pth', 'test': '/home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/test.Ngiemboon.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {('Limbum', 'Ngiemboon'): {'train': ('/home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/train.Limbum-Ngiemboon.Limbum.pth', '/home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/train.Limbum-Ngiemboon.Ngiemboon.pth'), 'valid': ('/home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/valid.Limbum-Ngiemboon.Limbum.pth', '/home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/valid.Limbum-Ngiemboon.Ngiemboon.pth'), 'test': ('/home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/test.Limbum-Ngiemboon.Limbum.pth', '/home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/test.Limbum-Ngiemboon.Ngiemboon.pth')}}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': True, 'valid': True, 'test': True}\n",
      "                                     remove_long_sentences_test: True\n",
      "                                     remove_long_sentences_train: True\n",
      "                                     remove_long_sentences_valid: True\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml\n",
      "                                     \n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_LimbumNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed' --lgs 'Limbum-Ngiemboon' --clm_steps '' --mlm_steps 'Limbum,Ngiemboon,Limbum-Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 6322 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/29/20 19:40:57 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - ============ langs: Limbum, Ngiemboon\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - ============ Monolingual data (Limbum)\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/valid.Limbum.pth ...\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - 33215 words (6667 unique) in 790 sentences. 13 unknown words (10 unique) covering 0.04% of the data.\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/test.Limbum.pth ...\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - 32382 words (6667 unique) in 790 sentences. 5 unknown words (4 unique) covering 0.02% of the data.\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - ============ Monolingual data (Ngiemboon)\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/valid.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - 28350 words (6667 unique) in 790 sentences. 21 unknown words (16 unique) covering 0.07% of the data.\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/test.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - 27928 words (6667 unique) in 790 sentences. 19 unknown words (14 unique) covering 0.07% of the data.\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - ============ Parallel data (Limbum-Ngiemboon)\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/valid.Limbum-Ngiemboon.Limbum.pth ...\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - 33215 words (6667 unique) in 790 sentences. 13 unknown words (10 unique) covering 0.04% of the data.\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/valid.Limbum-Ngiemboon.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - 28350 words (6667 unique) in 790 sentences. 21 unknown words (16 unique) covering 0.07% of the data.\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - ========================== debug : 5\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - Removed 4 too long sentences.\n",
      "\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/test.Limbum-Ngiemboon.Limbum.pth ...\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - 32382 words (6667 unique) in 790 sentences. 5 unknown words (4 unique) covering 0.02% of the data.\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/test.Limbum-Ngiemboon.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - 27928 words (6667 unique) in 790 sentences. 19 unknown words (14 unique) covering 0.07% of the data.\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - ========================== debug : 6\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - Removed 2 too long sentences.\n",
      "\n",
      "\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - Monolingual data   - valid -       Limbum:       790\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - Monolingual data   -  test -       Limbum:       790\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - Monolingual data   - valid -    Ngiemboon:       790\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - Monolingual data   -  test -    Ngiemboon:       790\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - Parallel data      - valid - Limbum-Ngiemboon:       786\n",
      "INFO - 05/29/20 19:40:57 - 0:00:00 - Parallel data      -  test - Limbum-Ngiemboon:       788\n",
      "\n",
      "INFO - 05/29/20 19:40:58 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(6667, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=6667, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/29/20 19:40:58 - 0:00:01 - Number of parameters (model): 82939403\n",
      "INFO - 05/29/20 19:41:01 - 0:00:04 - Found 0 memories.\n",
      "INFO - 05/29/20 19:41:01 - 0:00:04 - Found 6 FFN.\n",
      "INFO - 05/29/20 19:41:01 - 0:00:04 - Found 102 parameters in model.\n",
      "INFO - 05/29/20 19:41:01 - 0:00:04 - Optimizers: model\n",
      "WARNING - 05/29/20 19:41:01 - 0:00:04 - Reloading checkpoint from /home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml/checkpoint.pth ...\n",
      "WARNING - 05/29/20 19:41:02 - 0:00:05 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/29/20 19:41:02 - 0:00:05 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/29/20 19:41:02 - 0:00:05 - Checkpoint reloaded. Resuming at epoch 60 / iteration 15840 ...\n",
      "INFO - 05/29/20 19:41:06 - 0:00:08 - epoch -> 60.000000\n",
      "INFO - 05/29/20 19:41:06 - 0:00:08 - valid_Limbum_mlm_ppl -> 9.823700\n",
      "INFO - 05/29/20 19:41:06 - 0:00:08 - valid_Limbum_mlm_acc -> 59.844560\n",
      "INFO - 05/29/20 19:41:06 - 0:00:08 - valid_Ngiemboon_mlm_ppl -> 28.110587\n",
      "INFO - 05/29/20 19:41:06 - 0:00:08 - valid_Ngiemboon_mlm_acc -> 44.300518\n",
      "INFO - 05/29/20 19:41:06 - 0:00:08 - valid_Limbum_Ngiemboon_mlm_ppl -> 11.337517\n",
      "INFO - 05/29/20 19:41:06 - 0:00:08 - valid_Limbum_Ngiemboon_mlm_acc -> 56.542751\n",
      "INFO - 05/29/20 19:41:06 - 0:00:08 - valid_mlm_ppl -> 18.967144\n",
      "INFO - 05/29/20 19:41:06 - 0:00:08 - valid_mlm_acc -> 52.072539\n",
      "INFO - 05/29/20 19:41:06 - 0:00:08 - test_Limbum_mlm_ppl -> 8.898545\n",
      "INFO - 05/29/20 19:41:06 - 0:00:08 - test_Limbum_mlm_acc -> 55.181347\n",
      "INFO - 05/29/20 19:41:06 - 0:00:08 - test_Ngiemboon_mlm_ppl -> 50.216445\n",
      "INFO - 05/29/20 19:41:06 - 0:00:08 - test_Ngiemboon_mlm_acc -> 43.005181\n",
      "INFO - 05/29/20 19:41:06 - 0:00:08 - test_Limbum_Ngiemboon_mlm_ppl -> 10.571068\n",
      "INFO - 05/29/20 19:41:06 - 0:00:08 - test_Limbum_Ngiemboon_mlm_acc -> 57.070114\n",
      "INFO - 05/29/20 19:41:06 - 0:00:08 - test_mlm_ppl -> 29.557495\n",
      "INFO - 05/29/20 19:41:06 - 0:00:08 - test_mlm_acc -> 49.093264\n",
      "INFO - 05/29/20 19:41:06 - 0:00:08 - __log__:{\"epoch\": 60, \"valid_Limbum_mlm_ppl\": 9.823700205265077, \"valid_Limbum_mlm_acc\": 59.84455958549223, \"valid_Ngiemboon_mlm_ppl\": 28.110586905015115, \"valid_Ngiemboon_mlm_acc\": 44.30051813471503, \"valid_Limbum_Ngiemboon_mlm_ppl\": 11.337516658983027, \"valid_Limbum_Ngiemboon_mlm_acc\": 56.54275092936803, \"valid_mlm_ppl\": 18.967143555140098, \"valid_mlm_acc\": 52.07253886010363, \"test_Limbum_mlm_ppl\": 8.898545357145126, \"test_Limbum_mlm_acc\": 55.181347150259064, \"test_Ngiemboon_mlm_ppl\": 50.21644469557096, \"test_Ngiemboon_mlm_acc\": 43.005181347150256, \"test_Limbum_Ngiemboon_mlm_ppl\": 10.571068204141012, \"test_Limbum_Ngiemboon_mlm_acc\": 57.070113591852724, \"test_mlm_ppl\": 29.55749502635804, \"test_mlm_acc\": 49.093264248704656}\n"
     ]
    }
   ],
   "source": [
    "%env stopping_criterion=_valid_mlm_ppl,10\n",
    "%env eval_bleu=False\n",
    "%env mlm_steps=Limbum,Ngiemboon,Limbum-Ngiemboon\n",
    "! python train.py --eval_only $eval_only --exp_name mlm_tlm_LimbumNgiemboon --exp_id $exp_id --dump_path $dump_path --data_path $OUTPATH --lgs $lgs --clm_steps '' --mlm_steps $mlm_steps --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout 0.1 --attention_dropout 0.1 --gelu_activation true --batch_size $batch_size --bptt 256 --optimizer adam,lr=0.0001 --epoch_size $epoch_size --max_epoch $max_epoch --validation_metrics _valid_mlm_ppl --stopping_criterion $stopping_criterion --eval_bleu $eval_bleu --remove_long_sentences_train $remove_long_sentences_train --remove_long_sentences_valid $remove_long_sentences_valid --remove_long_sentences_test $remove_long_sentences_test --train_n_samples $train_n_samples --valid_n_samples $valid_n_samples --test_n_samples $test_n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: eval_bleu=True\n",
      "env: stopping_criterion=valid_Limbum-Ngiemboon_mt_bleu,10\n",
      "env: validation_metrics=valid_Limbum-Ngiemboon_mt_bleu\n",
      "env: reload_model=/home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml/best-valid_mlm_ppl.pth,/home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml/best-valid_mlm_ppl.pth\n",
      "env: ae_steps=Limbum,Ngiemboon\n",
      "env: bt_steps=Limbum-Ngiemboon-Limbum,Ngiemboon-Limbum-Ngiemboon\n",
      "env: mt_steps=Limbum-Ngiemboon,Ngiemboon-Limbum\n",
      "env: mlm_steps=Limbum,Ngiemboon,Limbum-Ngiemboon\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: ['Limbum', 'Ngiemboon']\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: ['Limbum', 'Ngiemboon']\n",
      "                                     bt_steps: [('Limbum', 'Ngiemboon', 'Limbum'), ('Ngiemboon', 'Limbum', 'Ngiemboon')]\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --mlm_steps 'Limbum,Ngiemboon,Limbum-Ngiemboon' --exp_name SupMT_LimbumNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --reload_model '/home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml/best-valid_mlm_ppl.pth,/home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml/best-valid_mlm_ppl.pth' --data_path '/home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed' --lgs 'Limbum-Ngiemboon' --ae_steps 'Limbum,Ngiemboon' --mt_steps 'Limbum-Ngiemboon,Ngiemboon-Limbum' --bt_steps 'Limbum-Ngiemboon-Limbum,Ngiemboon-Limbum-Ngiemboon' --word_shuffle 3 --word_dropout '0.1' --word_blank '0.1' --lambda_ae '0:1,100000:0.1,300000:0' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --tokens_per_batch 2000 --batch_size 32 --bptt 256 --optimizer 'adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001' --epoch_size 6322 --max_epoch 100 --eval_bleu True --stopping_criterion 'valid_Limbum-Ngiemboon_mt_bleu,10' --validation_metrics 'valid_Limbum-Ngiemboon_mt_bleu' --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster3/SupMT_LimbumNgiemboon/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: False\n",
      "                                     epoch_size: 6322\n",
      "                                     eval_bleu: True\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: SupMT_LimbumNgiemboon\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Limbum', 1: 'Ngiemboon'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 0:1,100000:0.1,300000:0\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Limbum': 0, 'Ngiemboon': 1}\n",
      "                                     langs: ['Limbum', 'Ngiemboon']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Limbum-Ngiemboon']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Limbum', None), ('Ngiemboon', None), ('Limbum', 'Ngiemboon')]\n",
      "                                     mono_dataset: {'Limbum': {'train': '/home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/train.Limbum.pth', 'valid': '/home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/valid.Limbum.pth', 'test': '/home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/test.Limbum.pth'}, 'Ngiemboon': {'train': '/home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/train.Ngiemboon.pth', 'valid': '/home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/valid.Ngiemboon.pth', 'test': '/home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/test.Ngiemboon.pth'}}\n",
      "                                     mt_steps: [('Limbum', 'Ngiemboon'), ('Ngiemboon', 'Limbum')]\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001\n",
      "                                     para_dataset: {('Limbum', 'Ngiemboon'): {'train': ('/home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/train.Limbum-Ngiemboon.Limbum.pth', '/home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/train.Limbum-Ngiemboon.Ngiemboon.pth'), 'valid': ('/home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/valid.Limbum-Ngiemboon.Limbum.pth', '/home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/valid.Limbum-Ngiemboon.Ngiemboon.pth'), 'test': ('/home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/test.Limbum-Ngiemboon.Limbum.pth', '/home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/test.Limbum-Ngiemboon.Ngiemboon.pth')}}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: /home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml/best-valid_mlm_ppl.pth,/home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml/best-valid_mlm_ppl.pth\n",
      "                                     remove_long_sentences: {'train': True, 'valid': True, 'test': True}\n",
      "                                     remove_long_sentences_test: True\n",
      "                                     remove_long_sentences_train: True\n",
      "                                     remove_long_sentences_valid: True\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: valid_Limbum-Ngiemboon_mt_bleu,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: 2000\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: valid_Limbum-Ngiemboon_mt_bleu\n",
      "                                     word_blank: 0.1\n",
      "                                     word_dropout: 0.1\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 3.0\n",
      "                                     world_size: 1\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster3/SupMT_LimbumNgiemboon/maml\n",
      "                                     \n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - Running command: python train.py --eval_only True --mlm_steps 'Limbum,Ngiemboon,Limbum-Ngiemboon' --exp_name SupMT_LimbumNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --reload_model '/home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml/best-valid_mlm_ppl.pth,/home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml/best-valid_mlm_ppl.pth' --data_path '/home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed' --lgs 'Limbum-Ngiemboon' --ae_steps 'Limbum,Ngiemboon' --mt_steps 'Limbum-Ngiemboon,Ngiemboon-Limbum' --bt_steps 'Limbum-Ngiemboon-Limbum,Ngiemboon-Limbum-Ngiemboon' --word_shuffle 3 --word_dropout '0.1' --word_blank '0.1' --lambda_ae '0:1,100000:0.1,300000:0' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --tokens_per_batch 2000 --batch_size 32 --bptt 256 --optimizer 'adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001' --epoch_size 6322 --max_epoch 100 --eval_bleu True --stopping_criterion 'valid_Limbum-Ngiemboon_mt_bleu,10' --validation_metrics 'valid_Limbum-Ngiemboon_mt_bleu' --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True\n",
      "\n",
      "WARNING - 05/29/20 19:45:14 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - ============ langs: Limbum, Ngiemboon\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - ============ Monolingual data (Limbum)\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/valid.Limbum.pth ...\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - 33215 words (6667 unique) in 790 sentences. 13 unknown words (10 unique) covering 0.04% of the data.\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - ========================== debug : 1\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - ========================== debug : 2\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - Removed 4 too long sentences.\n",
      "\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/test.Limbum.pth ...\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - 32382 words (6667 unique) in 790 sentences. 5 unknown words (4 unique) covering 0.02% of the data.\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - ========================== debug : 3\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - ========================== debug : 4\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - Removed 1 too long sentences.\n",
      "\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - ============ Monolingual data (Ngiemboon)\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/valid.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - 28350 words (6667 unique) in 790 sentences. 21 unknown words (16 unique) covering 0.07% of the data.\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - ========================== debug : 5\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - ========================== debug : 6\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - Removed 0 too long sentences.\n",
      "\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/test.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - 27928 words (6667 unique) in 790 sentences. 19 unknown words (14 unique) covering 0.07% of the data.\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - ========================== debug : 7\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - ========================== debug : 8\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - Removed 1 too long sentences.\n",
      "\n",
      "\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - ============ Parallel data (Limbum-Ngiemboon)\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/valid.Limbum-Ngiemboon.Limbum.pth ...\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - 33215 words (6667 unique) in 790 sentences. 13 unknown words (10 unique) covering 0.04% of the data.\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/valid.Limbum-Ngiemboon.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - 28350 words (6667 unique) in 790 sentences. 21 unknown words (16 unique) covering 0.07% of the data.\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - ========================== debug : 9\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - Removed 4 too long sentences.\n",
      "\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/test.Limbum-Ngiemboon.Limbum.pth ...\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - 32382 words (6667 unique) in 790 sentences. 5 unknown words (4 unique) covering 0.02% of the data.\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/test.Limbum-Ngiemboon.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - 27928 words (6667 unique) in 790 sentences. 19 unknown words (14 unique) covering 0.07% of the data.\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - ========================== debug : 10\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - Removed 2 too long sentences.\n",
      "\n",
      "\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - Monolingual data   - valid -       Limbum:       790\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - Monolingual data   -  test -       Limbum:       790\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - Monolingual data   - valid -    Ngiemboon:       790\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - Monolingual data   -  test -    Ngiemboon:       790\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - Parallel data      - valid - Limbum-Ngiemboon:       786\n",
      "INFO - 05/29/20 19:45:14 - 0:00:00 - Parallel data      -  test - Limbum-Ngiemboon:       788\n",
      "\n",
      "INFO - 05/29/20 19:45:16 - 0:00:02 - Reloading encoder from /home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml/best-valid_mlm_ppl.pth ...\n",
      "INFO - 05/29/20 19:45:19 - 0:00:05 - Reloading decoder from /home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml/best-valid_mlm_ppl.pth ...\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:05 - Parameter layer_norm15.0.weight not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:05 - Parameter layer_norm15.0.bias not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:05 - Parameter encoder_attn.0.q_lin.weight not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:05 - Parameter encoder_attn.0.q_lin.bias not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:05 - Parameter encoder_attn.0.k_lin.weight not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:05 - Parameter encoder_attn.0.k_lin.bias not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:05 - Parameter encoder_attn.0.v_lin.weight not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:05 - Parameter encoder_attn.0.v_lin.bias not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:05 - Parameter encoder_attn.0.out_lin.weight not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:05 - Parameter encoder_attn.0.out_lin.bias not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:05 - Parameter layer_norm15.1.weight not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:05 - Parameter layer_norm15.1.bias not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:05 - Parameter encoder_attn.1.q_lin.weight not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:05 - Parameter encoder_attn.1.q_lin.bias not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:05 - Parameter encoder_attn.1.k_lin.weight not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter encoder_attn.1.k_lin.bias not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter encoder_attn.1.v_lin.weight not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter encoder_attn.1.v_lin.bias not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter encoder_attn.1.out_lin.weight not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter encoder_attn.1.out_lin.bias not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter layer_norm15.2.weight not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter layer_norm15.2.bias not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter encoder_attn.2.q_lin.weight not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter encoder_attn.2.q_lin.bias not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter encoder_attn.2.k_lin.weight not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter encoder_attn.2.k_lin.bias not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter encoder_attn.2.v_lin.weight not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter encoder_attn.2.v_lin.bias not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter encoder_attn.2.out_lin.weight not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter encoder_attn.2.out_lin.bias not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter layer_norm15.3.weight not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter layer_norm15.3.bias not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter encoder_attn.3.q_lin.weight not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter encoder_attn.3.q_lin.bias not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter encoder_attn.3.k_lin.weight not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter encoder_attn.3.k_lin.bias not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter encoder_attn.3.v_lin.weight not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter encoder_attn.3.v_lin.bias not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter encoder_attn.3.out_lin.weight not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter encoder_attn.3.out_lin.bias not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter layer_norm15.4.weight not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter layer_norm15.4.bias not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter encoder_attn.4.q_lin.weight not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter encoder_attn.4.q_lin.bias not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter encoder_attn.4.k_lin.weight not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter encoder_attn.4.k_lin.bias not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter encoder_attn.4.v_lin.weight not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter encoder_attn.4.v_lin.bias not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter encoder_attn.4.out_lin.weight not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter encoder_attn.4.out_lin.bias not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter layer_norm15.5.weight not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter layer_norm15.5.bias not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter encoder_attn.5.q_lin.weight not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter encoder_attn.5.q_lin.bias not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter encoder_attn.5.k_lin.weight not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter encoder_attn.5.k_lin.bias not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter encoder_attn.5.v_lin.weight not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter encoder_attn.5.v_lin.bias not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter encoder_attn.5.out_lin.weight not found.\n",
      "WARNING - 05/29/20 19:45:19 - 0:00:06 - Parameter encoder_attn.5.out_lin.bias not found.\n",
      "INFO - 05/29/20 19:45:20 - 0:00:06 - Number of parameters (encoder): 82939403\n",
      "INFO - 05/29/20 19:45:20 - 0:00:06 - Number of parameters (decoder): 108142091\n",
      "INFO - 05/29/20 19:45:20 - 0:00:06 - Found 0 memories.\n",
      "INFO - 05/29/20 19:45:20 - 0:00:06 - Found 12 FFN.\n",
      "INFO - 05/29/20 19:45:20 - 0:00:06 - Found 264 parameters in model.\n",
      "INFO - 05/29/20 19:45:20 - 0:00:06 - Optimizers: model\n",
      "WARNING - 05/29/20 19:45:20 - 0:00:06 - Reloading checkpoint from /home/jupyter/models/africa/cluster3/SupMT_LimbumNgiemboon/maml/checkpoint.pth ...\n",
      "WARNING - 05/29/20 19:45:41 - 0:00:27 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/29/20 19:45:41 - 0:00:27 - Reloading 'num_updates' and 'lr' for optimizer model.\n",
      "WARNING - 05/29/20 19:45:41 - 0:00:27 - Checkpoint reloaded. Resuming at epoch 33 / iteration 1122 ...\n",
      "INFO - 05/29/20 19:46:08 - 0:00:54 - BLEU /home/jupyter/models/africa/cluster3/SupMT_LimbumNgiemboon/maml/hypotheses/hyp33.Ngiemboon-Limbum.valid.txt /home/jupyter/models/africa/cluster3/SupMT_LimbumNgiemboon/maml/hypotheses/ref.Ngiemboon-Limbum.valid.txt : 10.580000\n",
      "INFO - 05/29/20 19:46:31 - 0:01:17 - BLEU /home/jupyter/models/africa/cluster3/SupMT_LimbumNgiemboon/maml/hypotheses/hyp33.Limbum-Ngiemboon.valid.txt /home/jupyter/models/africa/cluster3/SupMT_LimbumNgiemboon/maml/hypotheses/ref.Limbum-Ngiemboon.valid.txt : 6.200000\n",
      "INFO - 05/29/20 19:46:56 - 0:01:43 - BLEU /home/jupyter/models/africa/cluster3/SupMT_LimbumNgiemboon/maml/hypotheses/hyp33.Ngiemboon-Limbum.test.txt /home/jupyter/models/africa/cluster3/SupMT_LimbumNgiemboon/maml/hypotheses/ref.Ngiemboon-Limbum.test.txt : 11.290000\n",
      "INFO - 05/29/20 19:47:19 - 0:02:05 - BLEU /home/jupyter/models/africa/cluster3/SupMT_LimbumNgiemboon/maml/hypotheses/hyp33.Limbum-Ngiemboon.test.txt /home/jupyter/models/africa/cluster3/SupMT_LimbumNgiemboon/maml/hypotheses/ref.Limbum-Ngiemboon.test.txt : 6.760000\n",
      "INFO - 05/29/20 19:47:19 - 0:02:05 - epoch -> 33.000000\n",
      "INFO - 05/29/20 19:47:19 - 0:02:05 - valid_Limbum_mlm_ppl -> 29.750539\n",
      "INFO - 05/29/20 19:47:19 - 0:02:05 - valid_Limbum_mlm_acc -> 38.082902\n",
      "INFO - 05/29/20 19:47:19 - 0:02:05 - valid_Ngiemboon_mlm_ppl -> 56.685407\n",
      "INFO - 05/29/20 19:47:19 - 0:02:05 - valid_Ngiemboon_mlm_acc -> 33.419689\n",
      "INFO - 05/29/20 19:47:19 - 0:02:05 - valid_Limbum_Ngiemboon_mlm_ppl -> 81.592313\n",
      "INFO - 05/29/20 19:47:19 - 0:02:05 - valid_Limbum_Ngiemboon_mlm_acc -> 30.929368\n",
      "INFO - 05/29/20 19:47:19 - 0:02:05 - valid_Ngiemboon-Limbum_mt_ppl -> 21.121361\n",
      "INFO - 05/29/20 19:47:19 - 0:02:05 - valid_Ngiemboon-Limbum_mt_acc -> 45.365097\n",
      "INFO - 05/29/20 19:47:19 - 0:02:05 - valid_Ngiemboon-Limbum_mt_bleu -> 10.580000\n",
      "INFO - 05/29/20 19:47:19 - 0:02:05 - valid_Limbum-Ngiemboon_mt_ppl -> 89.753361\n",
      "INFO - 05/29/20 19:47:19 - 0:02:05 - valid_Limbum-Ngiemboon_mt_acc -> 35.464854\n",
      "INFO - 05/29/20 19:47:19 - 0:02:05 - valid_Limbum-Ngiemboon_mt_bleu -> 6.200000\n",
      "INFO - 05/29/20 19:47:19 - 0:02:05 - valid_mlm_ppl -> 43.217973\n",
      "INFO - 05/29/20 19:47:19 - 0:02:05 - valid_mlm_acc -> 35.751295\n",
      "INFO - 05/29/20 19:47:19 - 0:02:05 - test_Limbum_mlm_ppl -> 37.261606\n",
      "INFO - 05/29/20 19:47:19 - 0:02:05 - test_Limbum_mlm_acc -> 39.119171\n",
      "INFO - 05/29/20 19:47:19 - 0:02:05 - test_Ngiemboon_mlm_ppl -> 246.845341\n",
      "INFO - 05/29/20 19:47:19 - 0:02:05 - test_Ngiemboon_mlm_acc -> 28.756477\n",
      "INFO - 05/29/20 19:47:19 - 0:02:05 - test_Limbum_Ngiemboon_mlm_ppl -> 75.280568\n",
      "INFO - 05/29/20 19:47:19 - 0:02:05 - test_Limbum_Ngiemboon_mlm_acc -> 31.374853\n",
      "INFO - 05/29/20 19:47:19 - 0:02:05 - test_Ngiemboon-Limbum_mt_ppl -> 19.504422\n",
      "INFO - 05/29/20 19:47:19 - 0:02:05 - test_Ngiemboon-Limbum_mt_acc -> 45.946931\n",
      "INFO - 05/29/20 19:47:19 - 0:02:05 - test_Ngiemboon-Limbum_mt_bleu -> 11.290000\n",
      "INFO - 05/29/20 19:47:19 - 0:02:05 - test_Limbum-Ngiemboon_mt_ppl -> 84.396960\n",
      "INFO - 05/29/20 19:47:19 - 0:02:05 - test_Limbum-Ngiemboon_mt_acc -> 36.621546\n",
      "INFO - 05/29/20 19:47:19 - 0:02:05 - test_Limbum-Ngiemboon_mt_bleu -> 6.760000\n",
      "INFO - 05/29/20 19:47:19 - 0:02:05 - test_mlm_ppl -> 142.053474\n",
      "INFO - 05/29/20 19:47:19 - 0:02:05 - test_mlm_acc -> 33.937824\n",
      "INFO - 05/29/20 19:47:19 - 0:02:05 - __log__:{\"epoch\": 33, \"valid_Limbum_mlm_ppl\": 29.750538946966532, \"valid_Limbum_mlm_acc\": 38.082901554404145, \"valid_Ngiemboon_mlm_ppl\": 56.68540715809474, \"valid_Ngiemboon_mlm_acc\": 33.41968911917098, \"valid_Limbum_Ngiemboon_mlm_ppl\": 81.59231285304753, \"valid_Limbum_Ngiemboon_mlm_acc\": 30.929368029739777, \"valid_Ngiemboon-Limbum_mt_ppl\": 21.121361487627492, \"valid_Ngiemboon-Limbum_mt_acc\": 45.365097349354485, \"valid_Ngiemboon-Limbum_mt_bleu\": 10.58, \"valid_Limbum-Ngiemboon_mt_ppl\": 89.75336112823682, \"valid_Limbum-Ngiemboon_mt_acc\": 35.464854180393246, \"valid_Limbum-Ngiemboon_mt_bleu\": 6.2, \"valid_mlm_ppl\": 43.21797305253064, \"valid_mlm_acc\": 35.751295336787564, \"test_Limbum_mlm_ppl\": 37.26160627067535, \"test_Limbum_mlm_acc\": 39.119170984455955, \"test_Ngiemboon_mlm_ppl\": 246.84534088617698, \"test_Ngiemboon_mlm_acc\": 28.756476683937823, \"test_Limbum_Ngiemboon_mlm_ppl\": 75.28056800280042, \"test_Limbum_Ngiemboon_mlm_acc\": 31.374853113983548, \"test_Ngiemboon-Limbum_mt_ppl\": 19.504421928502186, \"test_Ngiemboon-Limbum_mt_acc\": 45.946930596878985, \"test_Ngiemboon-Limbum_mt_bleu\": 11.29, \"test_Limbum-Ngiemboon_mt_ppl\": 84.39696021962115, \"test_Limbum-Ngiemboon_mt_acc\": 36.621545798849766, \"test_Limbum-Ngiemboon_mt_bleu\": 6.76, \"test_mlm_ppl\": 142.05347357842618, \"test_mlm_acc\": 33.93782383419689}\n"
     ]
    }
   ],
   "source": [
    "%env eval_bleu=True\n",
    "! chmod +x src/evaluation/multi-bleu.perl\n",
    "\n",
    "%env stopping_criterion=valid_Limbum-Ngiemboon_mt_bleu,10\n",
    "%env validation_metrics=valid_Limbum-Ngiemboon_mt_bleu\n",
    "%env reload_model=/home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml/best-valid_mlm_ppl.pth,/home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml/best-valid_mlm_ppl.pth\n",
    "%env ae_steps=Limbum,Ngiemboon\n",
    "%env bt_steps=Limbum-Ngiemboon-Limbum,Ngiemboon-Limbum-Ngiemboon\n",
    "%env mt_steps=Limbum-Ngiemboon,Ngiemboon-Limbum          \n",
    "\n",
    "%env mlm_steps=Limbum,Ngiemboon,Limbum-Ngiemboon\n",
    "! python train.py --eval_only $eval_only --mlm_steps $mlm_steps --exp_name SupMT_LimbumNgiemboon --exp_id $exp_id  --dump_path $dump_path --reload_model $reload_model --data_path $OUTPATH --lgs $lgs --ae_steps $ae_steps --mt_steps $mt_steps --bt_steps $bt_steps --word_shuffle 3 --word_dropout 0.1 --word_blank 0.1 --lambda_ae '0:1,100000:0.1,300000:0' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout 0.1 --attention_dropout 0.1 --gelu_activation true --tokens_per_batch 2000 --batch_size $batch_size --bptt 256 --optimizer adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001 --epoch_size $epoch_size --max_epoch $max_epoch --eval_bleu $eval_bleu --stopping_criterion $stopping_criterion --validation_metrics $validation_metrics --train_n_samples $train_n_samples --valid_n_samples $valid_n_samples --test_n_samples $test_n_samples --remove_long_sentences_train $remove_long_sentences_train --remove_long_sentences_valid $remove_long_sentences_valid --remove_long_sentences_test $remove_long_sentences_test    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XLM Ghomala Limbum Ngiemboon = XLM_cluster1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OUTPATH=/home/jupyter/models/africa/cluster3/data/XLM_all/processed\n",
      "env: epoch_size=6339\n",
      "env: lgs=Ghomala-Limbum-Ngiemboon\n",
      "env: batch_size=32\n",
      "env: max_epoch=100\n",
      "env: dump_path=/home/jupyter/models/africa/cluster3\n"
     ]
    }
   ],
   "source": [
    "%env OUTPATH=/home/jupyter/models/africa/cluster3/data/XLM_all/processed\n",
    "%env epoch_size=6339\n",
    "%env lgs=Ghomala-Limbum-Ngiemboon\n",
    "%env batch_size=32\n",
    "%env max_epoch=100\n",
    "%env dump_path=/home/jupyter/models/africa/cluster3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLM + TLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: stopping_criterion=_valid_mlm_ppl,10\n",
      "env: eval_bleu=False\n",
      "env: mlm_steps=Ghomala,Limbum,Ngiemboon,Ghomala-Limbum,Ghomala-Ngiemboon,Limbum-Ngiemboon\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/29/20 19:47:34 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/29/20 19:47:34 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaLimbumNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/cluster3/data/XLM_all/processed' --lgs 'Ghomala-Limbum-Ngiemboon' --clm_steps '' --mlm_steps 'Ghomala,Limbum,Ngiemboon,Ghomala-Limbum,Ghomala-Ngiemboon,Limbum-Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 6339 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/cluster3/data/XLM_all/processed\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbumNgiemboon/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 6339\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_GhomalaLimbumNgiemboon\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Ghomala', 1: 'Limbum', 2: 'Ngiemboon'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Ghomala': 0, 'Limbum': 1, 'Ngiemboon': 2}\n",
      "                                     langs: ['Ghomala', 'Limbum', 'Ngiemboon']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Ghomala-Limbum-Ngiemboon']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Ghomala', None), ('Limbum', None), ('Ngiemboon', None), ('Ghomala', 'Limbum'), ('Ghomala', 'Ngiemboon'), ('Limbum', 'Ngiemboon')]\n",
      "                                     mono_dataset: {'Ghomala': {'train': '/home/jupyter/models/africa/cluster3/data/XLM_all/processed/train.Ghomala.pth', 'valid': '/home/jupyter/models/africa/cluster3/data/XLM_all/processed/valid.Ghomala.pth', 'test': '/home/jupyter/models/africa/cluster3/data/XLM_all/processed/test.Ghomala.pth'}, 'Limbum': {'train': '/home/jupyter/models/africa/cluster3/data/XLM_all/processed/train.Limbum.pth', 'valid': '/home/jupyter/models/africa/cluster3/data/XLM_all/processed/valid.Limbum.pth', 'test': '/home/jupyter/models/africa/cluster3/data/XLM_all/processed/test.Limbum.pth'}, 'Ngiemboon': {'train': '/home/jupyter/models/africa/cluster3/data/XLM_all/processed/train.Ngiemboon.pth', 'valid': '/home/jupyter/models/africa/cluster3/data/XLM_all/processed/valid.Ngiemboon.pth', 'test': '/home/jupyter/models/africa/cluster3/data/XLM_all/processed/test.Ngiemboon.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 3\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {('Ghomala', 'Limbum'): {'train': ('/home/jupyter/models/africa/cluster3/data/XLM_all/processed/train.Ghomala-Limbum.Ghomala.pth', '/home/jupyter/models/africa/cluster3/data/XLM_all/processed/train.Ghomala-Limbum.Limbum.pth'), 'valid': ('/home/jupyter/models/africa/cluster3/data/XLM_all/processed/valid.Ghomala-Limbum.Ghomala.pth', '/home/jupyter/models/africa/cluster3/data/XLM_all/processed/valid.Ghomala-Limbum.Limbum.pth'), 'test': ('/home/jupyter/models/africa/cluster3/data/XLM_all/processed/test.Ghomala-Limbum.Ghomala.pth', '/home/jupyter/models/africa/cluster3/data/XLM_all/processed/test.Ghomala-Limbum.Limbum.pth')}, ('Ghomala', 'Ngiemboon'): {'train': ('/home/jupyter/models/africa/cluster3/data/XLM_all/processed/train.Ghomala-Ngiemboon.Ghomala.pth', '/home/jupyter/models/africa/cluster3/data/XLM_all/processed/train.Ghomala-Ngiemboon.Ngiemboon.pth'), 'valid': ('/home/jupyter/models/africa/cluster3/data/XLM_all/processed/valid.Ghomala-Ngiemboon.Ghomala.pth', '/home/jupyter/models/africa/cluster3/data/XLM_all/processed/valid.Ghomala-Ngiemboon.Ngiemboon.pth'), 'test': ('/home/jupyter/models/africa/cluster3/data/XLM_all/processed/test.Ghomala-Ngiemboon.Ghomala.pth', '/home/jupyter/models/africa/cluster3/data/XLM_all/processed/test.Ghomala-Ngiemboon.Ngiemboon.pth')}, ('Limbum', 'Ngiemboon'): {'train': ('/home/jupyter/models/africa/cluster3/data/XLM_all/processed/train.Limbum-Ngiemboon.Limbum.pth', '/home/jupyter/models/africa/cluster3/data/XLM_all/processed/train.Limbum-Ngiemboon.Ngiemboon.pth'), 'valid': ('/home/jupyter/models/africa/cluster3/data/XLM_all/processed/valid.Limbum-Ngiemboon.Limbum.pth', '/home/jupyter/models/africa/cluster3/data/XLM_all/processed/valid.Limbum-Ngiemboon.Ngiemboon.pth'), 'test': ('/home/jupyter/models/africa/cluster3/data/XLM_all/processed/test.Limbum-Ngiemboon.Limbum.pth', '/home/jupyter/models/africa/cluster3/data/XLM_all/processed/test.Limbum-Ngiemboon.Ngiemboon.pth')}}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': True, 'valid': True, 'test': True}\n",
      "                                     remove_long_sentences_test: True\n",
      "                                     remove_long_sentences_train: True\n",
      "                                     remove_long_sentences_valid: True\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/29/20 19:47:34 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbumNgiemboon/maml\n",
      "                                     \n",
      "INFO - 05/29/20 19:47:34 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaLimbumNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/cluster3/data/XLM_all/processed' --lgs 'Ghomala-Limbum-Ngiemboon' --clm_steps '' --mlm_steps 'Ghomala,Limbum,Ngiemboon,Ghomala-Limbum,Ghomala-Ngiemboon,Limbum-Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 6339 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/29/20 19:47:34 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/29/20 19:47:34 - 0:00:00 - ============ langs: Ghomala, Limbum, Ngiemboon\n",
      "INFO - 05/29/20 19:47:34 - 0:00:00 - ============ Monolingual data (Ghomala)\n",
      "INFO - 05/29/20 19:47:34 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/XLM_all/processed/valid.Ghomala.pth ...\n",
      "INFO - 05/29/20 19:47:34 - 0:00:00 - 25976 words (16375 unique) in 792 sentences. 3 unknown words (3 unique) covering 0.01% of the data.\n",
      "INFO - 05/29/20 19:47:34 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/29/20 19:47:34 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/XLM_all/processed/test.Ghomala.pth ...\n",
      "INFO - 05/29/20 19:47:34 - 0:00:00 - 26091 words (16375 unique) in 792 sentences. 5 unknown words (4 unique) covering 0.02% of the data.\n",
      "INFO - 05/29/20 19:47:34 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/29/20 19:47:34 - 0:00:00 - ============ Monolingual data (Limbum)\n",
      "INFO - 05/29/20 19:47:34 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/XLM_all/processed/valid.Limbum.pth ...\n",
      "INFO - 05/29/20 19:47:34 - 0:00:00 - 32833 words (16375 unique) in 790 sentences. 7 unknown words (7 unique) covering 0.02% of the data.\n",
      "INFO - 05/29/20 19:47:34 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/29/20 19:47:34 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/XLM_all/processed/test.Limbum.pth ...\n",
      "INFO - 05/29/20 19:47:34 - 0:00:00 - 32035 words (16375 unique) in 790 sentences. 13 unknown words (12 unique) covering 0.04% of the data.\n",
      "INFO - 05/29/20 19:47:34 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "INFO - 05/29/20 19:47:34 - 0:00:00 - ============ Monolingual data (Ngiemboon)\n",
      "INFO - 05/29/20 19:47:34 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/XLM_all/processed/valid.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 19:47:34 - 0:00:00 - 26849 words (16375 unique) in 790 sentences. 37 unknown words (36 unique) covering 0.14% of the data.\n",
      "INFO - 05/29/20 19:47:34 - 0:00:00 - ========================== debug : 5\n",
      "\n",
      "INFO - 05/29/20 19:47:34 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/XLM_all/processed/test.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 19:47:34 - 0:00:00 - 26488 words (16375 unique) in 790 sentences. 48 unknown words (46 unique) covering 0.18% of the data.\n",
      "INFO - 05/29/20 19:47:35 - 0:00:00 - ========================== debug : 6\n",
      "\n",
      "\n",
      "INFO - 05/29/20 19:47:35 - 0:00:00 - ============ Parallel data (Ghomala-Limbum)\n",
      "INFO - 05/29/20 19:47:35 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/XLM_all/processed/valid.Ghomala-Limbum.Ghomala.pth ...\n",
      "INFO - 05/29/20 19:47:35 - 0:00:00 - 26398 words (16375 unique) in 791 sentences. 6 unknown words (5 unique) covering 0.02% of the data.\n",
      "INFO - 05/29/20 19:47:35 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/XLM_all/processed/valid.Ghomala-Limbum.Limbum.pth ...\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - 32437 words (16375 unique) in 791 sentences. 13 unknown words (12 unique) covering 0.04% of the data.\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - ========================== debug : 7\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - Removed 0 empty sentences.\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - Removed 0 too long sentences.\n",
      "\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - Loading data from /home/jupyter/models/africa/cluster3/data/XLM_all/processed/test.Ghomala-Limbum.Ghomala.pth ...\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - 25790 words (16375 unique) in 791 sentences. 2 unknown words (2 unique) covering 0.01% of the data.\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - Loading data from /home/jupyter/models/africa/cluster3/data/XLM_all/processed/test.Ghomala-Limbum.Limbum.pth ...\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - 32555 words (16375 unique) in 791 sentences. 8 unknown words (7 unique) covering 0.02% of the data.\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - ========================== debug : 8\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - Removed 0 empty sentences.\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - Removed 4 too long sentences.\n",
      "\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - ============ Parallel data (Ghomala-Ngiemboon)\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - Loading data from /home/jupyter/models/africa/cluster3/data/XLM_all/processed/valid.Ghomala-Ngiemboon.Ghomala.pth ...\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - 25976 words (16375 unique) in 792 sentences. 3 unknown words (3 unique) covering 0.01% of the data.\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - Loading data from /home/jupyter/models/africa/cluster3/data/XLM_all/processed/valid.Ghomala-Ngiemboon.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - 26583 words (16375 unique) in 792 sentences. 41 unknown words (41 unique) covering 0.15% of the data.\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - ========================== debug : 9\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - Removed 0 empty sentences.\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - Removed 1 too long sentences.\n",
      "\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - Loading data from /home/jupyter/models/africa/cluster3/data/XLM_all/processed/test.Ghomala-Ngiemboon.Ghomala.pth ...\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - 26091 words (16375 unique) in 792 sentences. 5 unknown words (4 unique) covering 0.02% of the data.\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - Loading data from /home/jupyter/models/africa/cluster3/data/XLM_all/processed/test.Ghomala-Ngiemboon.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - 26922 words (16375 unique) in 792 sentences. 44 unknown words (41 unique) covering 0.16% of the data.\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - ========================== debug : 10\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - Removed 0 empty sentences.\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - Removed 0 too long sentences.\n",
      "\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - ============ Parallel data (Limbum-Ngiemboon)\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - Loading data from /home/jupyter/models/africa/cluster3/data/XLM_all/processed/valid.Limbum-Ngiemboon.Limbum.pth ...\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - 32833 words (16375 unique) in 790 sentences. 7 unknown words (7 unique) covering 0.02% of the data.\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - Loading data from /home/jupyter/models/africa/cluster3/data/XLM_all/processed/valid.Limbum-Ngiemboon.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - 26849 words (16375 unique) in 790 sentences. 37 unknown words (36 unique) covering 0.14% of the data.\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - ========================== debug : 11\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - Removed 0 empty sentences.\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - Removed 4 too long sentences.\n",
      "\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - Loading data from /home/jupyter/models/africa/cluster3/data/XLM_all/processed/test.Limbum-Ngiemboon.Limbum.pth ...\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - 32035 words (16375 unique) in 790 sentences. 13 unknown words (12 unique) covering 0.04% of the data.\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - Loading data from /home/jupyter/models/africa/cluster3/data/XLM_all/processed/test.Limbum-Ngiemboon.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - 26488 words (16375 unique) in 790 sentences. 48 unknown words (46 unique) covering 0.18% of the data.\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - ========================== debug : 12\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - Removed 0 empty sentences.\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - Removed 1 too long sentences.\n",
      "\n",
      "\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - ============ Data summary\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - Monolingual data   - valid -      Ghomala:       792\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - Monolingual data   -  test -      Ghomala:       792\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - Monolingual data   - valid -       Limbum:       790\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - Monolingual data   -  test -       Limbum:       790\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - Monolingual data   - valid -    Ngiemboon:       790\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - Monolingual data   -  test -    Ngiemboon:       790\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - Parallel data      - valid - Ghomala-Limbum:       791\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - Parallel data      -  test - Ghomala-Limbum:       787\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - Parallel data      - valid - Ghomala-Ngiemboon:       791\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - Parallel data      -  test - Ghomala-Ngiemboon:       792\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - Parallel data      - valid - Limbum-Ngiemboon:       786\n",
      "INFO - 05/29/20 19:47:35 - 0:00:01 - Parallel data      -  test - Limbum-Ngiemboon:       789\n",
      "\n",
      "INFO - 05/29/20 19:47:36 - 0:00:02 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(3, 1024)\n",
      "                                       (embeddings): Embedding(16375, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=16375, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/29/20 19:47:36 - 0:00:02 - Number of parameters (model): 92891127\n",
      "INFO - 05/29/20 19:47:40 - 0:00:06 - Found 0 memories.\n",
      "INFO - 05/29/20 19:47:40 - 0:00:06 - Found 6 FFN.\n",
      "INFO - 05/29/20 19:47:40 - 0:00:06 - Found 102 parameters in model.\n",
      "INFO - 05/29/20 19:47:40 - 0:00:06 - Optimizers: model\n",
      "WARNING - 05/29/20 19:47:40 - 0:00:06 - Reloading checkpoint from /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbumNgiemboon/maml/checkpoint.pth ...\n",
      "WARNING - 05/29/20 19:47:51 - 0:00:17 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/29/20 19:47:51 - 0:00:17 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/29/20 19:47:51 - 0:00:17 - Checkpoint reloaded. Resuming at epoch 100 / iteration 6700 ...\n",
      "INFO - 05/29/20 19:48:00 - 0:00:26 - epoch -> 100.000000\n",
      "INFO - 05/29/20 19:48:00 - 0:00:26 - valid_Ghomala_mlm_ppl -> 9.935856\n",
      "INFO - 05/29/20 19:48:00 - 0:00:26 - valid_Ghomala_mlm_acc -> 52.849741\n",
      "INFO - 05/29/20 19:48:00 - 0:00:26 - valid_Limbum_mlm_ppl -> 5.987964\n",
      "INFO - 05/29/20 19:48:00 - 0:00:26 - valid_Limbum_mlm_acc -> 60.621762\n",
      "INFO - 05/29/20 19:48:00 - 0:00:26 - valid_Ngiemboon_mlm_ppl -> 17.122960\n",
      "INFO - 05/29/20 19:48:00 - 0:00:26 - valid_Ngiemboon_mlm_acc -> 51.813472\n",
      "INFO - 05/29/20 19:48:00 - 0:00:26 - valid_Ghomala_Limbum_mlm_ppl -> 3.610534\n",
      "INFO - 05/29/20 19:48:00 - 0:00:26 - valid_Ghomala_Limbum_mlm_acc -> 69.492901\n",
      "INFO - 05/29/20 19:48:00 - 0:00:26 - valid_Ghomala_Ngiemboon_mlm_ppl -> 5.030538\n",
      "INFO - 05/29/20 19:48:00 - 0:00:26 - valid_Ghomala_Ngiemboon_mlm_acc -> 66.556448\n",
      "INFO - 05/29/20 19:48:00 - 0:00:26 - valid_Limbum_Ngiemboon_mlm_ppl -> 5.605463\n",
      "INFO - 05/29/20 19:48:00 - 0:00:26 - valid_Limbum_Ngiemboon_mlm_acc -> 65.725338\n",
      "INFO - 05/29/20 19:48:00 - 0:00:26 - valid_mlm_ppl -> 11.015593\n",
      "INFO - 05/29/20 19:48:00 - 0:00:26 - valid_mlm_acc -> 55.094991\n",
      "INFO - 05/29/20 19:48:00 - 0:00:26 - test_Ghomala_mlm_ppl -> 9.175999\n",
      "INFO - 05/29/20 19:48:00 - 0:00:26 - test_Ghomala_mlm_acc -> 57.512953\n",
      "INFO - 05/29/20 19:48:00 - 0:00:26 - test_Limbum_mlm_ppl -> 5.696082\n",
      "INFO - 05/29/20 19:48:00 - 0:00:26 - test_Limbum_mlm_acc -> 61.917098\n",
      "INFO - 05/29/20 19:48:00 - 0:00:26 - test_Ngiemboon_mlm_ppl -> 17.315049\n",
      "INFO - 05/29/20 19:48:00 - 0:00:26 - test_Ngiemboon_mlm_acc -> 50.777202\n",
      "INFO - 05/29/20 19:48:00 - 0:00:26 - test_Ghomala_Limbum_mlm_ppl -> 3.391891\n",
      "INFO - 05/29/20 19:48:00 - 0:00:26 - test_Ghomala_Limbum_mlm_acc -> 71.806347\n",
      "INFO - 05/29/20 19:48:00 - 0:00:26 - test_Ghomala_Ngiemboon_mlm_ppl -> 6.004230\n",
      "INFO - 05/29/20 19:48:00 - 0:00:26 - test_Ghomala_Ngiemboon_mlm_acc -> 62.926829\n",
      "INFO - 05/29/20 19:48:00 - 0:00:26 - test_Limbum_Ngiemboon_mlm_ppl -> 6.219452\n",
      "INFO - 05/29/20 19:48:00 - 0:00:26 - test_Limbum_Ngiemboon_mlm_acc -> 63.298302\n",
      "INFO - 05/29/20 19:48:00 - 0:00:26 - test_mlm_ppl -> 10.729043\n",
      "INFO - 05/29/20 19:48:00 - 0:00:26 - test_mlm_acc -> 56.735751\n",
      "INFO - 05/29/20 19:48:00 - 0:00:26 - __log__:{\"epoch\": 100, \"valid_Ghomala_mlm_ppl\": 9.93585583355199, \"valid_Ghomala_mlm_acc\": 52.84974093264249, \"valid_Limbum_mlm_ppl\": 5.98796395502128, \"valid_Limbum_mlm_acc\": 60.62176165803109, \"valid_Ngiemboon_mlm_ppl\": 17.122959592021992, \"valid_Ngiemboon_mlm_acc\": 51.81347150259067, \"valid_Ghomala_Limbum_mlm_ppl\": 3.6105342620492014, \"valid_Ghomala_Limbum_mlm_acc\": 69.49290060851926, \"valid_Ghomala_Ngiemboon_mlm_ppl\": 5.030538311738813, \"valid_Ghomala_Ngiemboon_mlm_acc\": 66.55644780349552, \"valid_Limbum_Ngiemboon_mlm_ppl\": 5.605462650677031, \"valid_Limbum_Ngiemboon_mlm_acc\": 65.72533849129594, \"valid_mlm_ppl\": 11.015593126865086, \"valid_mlm_acc\": 55.09499136442142, \"test_Ghomala_mlm_ppl\": 9.17599882656787, \"test_Ghomala_mlm_acc\": 57.512953367875646, \"test_Limbum_mlm_ppl\": 5.696082251692235, \"test_Limbum_mlm_acc\": 61.917098445595855, \"test_Ngiemboon_mlm_ppl\": 17.315049009707298, \"test_Ngiemboon_mlm_acc\": 50.77720207253886, \"test_Ghomala_Limbum_mlm_ppl\": 3.3918910255114096, \"test_Ghomala_Limbum_mlm_acc\": 71.80634662327095, \"test_Ghomala_Ngiemboon_mlm_ppl\": 6.004230345870815, \"test_Ghomala_Ngiemboon_mlm_acc\": 62.926829268292686, \"test_Limbum_Ngiemboon_mlm_ppl\": 6.219452202752607, \"test_Limbum_Ngiemboon_mlm_acc\": 63.29830234438157, \"test_mlm_ppl\": 10.729043362655801, \"test_mlm_acc\": 56.73575129533679}\n"
     ]
    }
   ],
   "source": [
    "%env stopping_criterion=_valid_mlm_ppl,10\n",
    "%env eval_bleu=False\n",
    "%env mlm_steps=Ghomala,Limbum,Ngiemboon,Ghomala-Limbum,Ghomala-Ngiemboon,Limbum-Ngiemboon\n",
    "! python train.py --eval_only $eval_only --exp_name mlm_tlm_GhomalaLimbumNgiemboon --exp_id $exp_id --dump_path $dump_path --data_path $OUTPATH --lgs $lgs --clm_steps '' --mlm_steps $mlm_steps --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout 0.1 --attention_dropout 0.1 --gelu_activation true --batch_size $batch_size --bptt 256 --optimizer adam,lr=0.0001 --epoch_size $epoch_size --max_epoch $max_epoch --validation_metrics _valid_mlm_ppl --stopping_criterion $stopping_criterion --eval_bleu $eval_bleu --remove_long_sentences_train $remove_long_sentences_train --remove_long_sentences_valid $remove_long_sentences_valid --remove_long_sentences_test $remove_long_sentences_test --train_n_samples $train_n_samples --valid_n_samples $valid_n_samples --test_n_samples $test_n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: eval_bleu=True\n",
      "env: stopping_criterion=valid_Ghomala-Limbum_mt_bleu,10\n",
      "env: validation_metrics=valid_Ghomala-Limbum_mt_bleu\n",
      "env: reload_model=/home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbumNgiemboon/maml/best-valid_mlm_ppl.pth,/home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbumNgiemboon/maml/best-valid_mlm_ppl.pth\n",
      "env: ae_steps=Ghomala,Limbum,Ngiemboon\n",
      "env: bt_steps=Ghomala-Limbum-Ghomala,Limbum-Ghomala-Limbum,Limbum-Ngiemboon-Limbum,Ngiemboon-Limbum-Ngiemboon,Ghomala-Ngiemboon-Ghomala,Ngiemboon-Ghomala-Ngiemboon\n",
      "env: mt_steps=Ghomala-Limbum,Limbum-Ghomala,Limbum-Ngiemboon,Ngiemboon-Limbum,Ghomala-Ngiemboon,Ngiemboon-Ghomala\n",
      "env: mlm_steps=Ghomala,Limbum,Ngiemboon,Ghomala-Limbum,Ghomala-Ngiemboon,Limbum-Ngiemboon\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/29/20 19:48:17 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/29/20 19:48:17 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: ['Ghomala', 'Limbum', 'Ngiemboon']\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: ['Ghomala', 'Limbum', 'Limbum', 'Ngiemboon', 'Ghomala', 'Ngiemboon']\n",
      "                                     bt_steps: [('Ghomala', 'Limbum', 'Ghomala'), ('Limbum', 'Ghomala', 'Limbum'), ('Limbum', 'Ngiemboon', 'Limbum'), ('Ngiemboon', 'Limbum', 'Ngiemboon'), ('Ghomala', 'Ngiemboon', 'Ghomala'), ('Ngiemboon', 'Ghomala', 'Ngiemboon')]\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --mlm_steps 'Ghomala,Limbum,Ngiemboon,Ghomala-Limbum,Ghomala-Ngiemboon,Limbum-Ngiemboon' --exp_name SupMT_XLM_all --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --reload_model '/home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbumNgiemboon/maml/best-valid_mlm_ppl.pth,/home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbumNgiemboon/maml/best-valid_mlm_ppl.pth' --data_path '/home/jupyter/models/africa/cluster3/data/XLM_all/processed' --lgs 'Ghomala-Limbum-Ngiemboon' --ae_steps 'Ghomala,Limbum,Ngiemboon' --mt_steps 'Ghomala-Limbum,Limbum-Ghomala,Limbum-Ngiemboon,Ngiemboon-Limbum,Ghomala-Ngiemboon,Ngiemboon-Ghomala' --bt_steps 'Ghomala-Limbum-Ghomala,Limbum-Ghomala-Limbum,Limbum-Ngiemboon-Limbum,Ngiemboon-Limbum-Ngiemboon,Ghomala-Ngiemboon-Ghomala,Ngiemboon-Ghomala-Ngiemboon' --word_shuffle 3 --word_dropout '0.1' --word_blank '0.1' --lambda_ae '0:1,100000:0.1,300000:0' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --tokens_per_batch 2000 --batch_size 32 --bptt 256 --optimizer 'adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001' --epoch_size 6339 --max_epoch 100 --eval_bleu True --stopping_criterion 'valid_Ghomala-Limbum_mt_bleu,10' --validation_metrics 'valid_Ghomala-Limbum_mt_bleu' --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/cluster3/data/XLM_all/processed\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster3/SupMT_XLM_all/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: False\n",
      "                                     epoch_size: 6339\n",
      "                                     eval_bleu: True\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: SupMT_XLM_all\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Ghomala', 1: 'Limbum', 2: 'Ngiemboon'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 0:1,100000:0.1,300000:0\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Ghomala': 0, 'Limbum': 1, 'Ngiemboon': 2}\n",
      "                                     langs: ['Ghomala', 'Limbum', 'Ngiemboon']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Ghomala-Limbum-Ngiemboon']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Ghomala', None), ('Limbum', None), ('Ngiemboon', None), ('Ghomala', 'Limbum'), ('Ghomala', 'Ngiemboon'), ('Limbum', 'Ngiemboon')]\n",
      "                                     mono_dataset: {'Ghomala': {'train': '/home/jupyter/models/africa/cluster3/data/XLM_all/processed/train.Ghomala.pth', 'valid': '/home/jupyter/models/africa/cluster3/data/XLM_all/processed/valid.Ghomala.pth', 'test': '/home/jupyter/models/africa/cluster3/data/XLM_all/processed/test.Ghomala.pth'}, 'Limbum': {'train': '/home/jupyter/models/africa/cluster3/data/XLM_all/processed/train.Limbum.pth', 'valid': '/home/jupyter/models/africa/cluster3/data/XLM_all/processed/valid.Limbum.pth', 'test': '/home/jupyter/models/africa/cluster3/data/XLM_all/processed/test.Limbum.pth'}, 'Ngiemboon': {'train': '/home/jupyter/models/africa/cluster3/data/XLM_all/processed/train.Ngiemboon.pth', 'valid': '/home/jupyter/models/africa/cluster3/data/XLM_all/processed/valid.Ngiemboon.pth', 'test': '/home/jupyter/models/africa/cluster3/data/XLM_all/processed/test.Ngiemboon.pth'}}\n",
      "                                     mt_steps: [('Ghomala', 'Limbum'), ('Limbum', 'Ghomala'), ('Limbum', 'Ngiemboon'), ('Ngiemboon', 'Limbum'), ('Ghomala', 'Ngiemboon'), ('Ngiemboon', 'Ghomala')]\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 3\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001\n",
      "                                     para_dataset: {('Ghomala', 'Limbum'): {'train': ('/home/jupyter/models/africa/cluster3/data/XLM_all/processed/train.Ghomala-Limbum.Ghomala.pth', '/home/jupyter/models/africa/cluster3/data/XLM_all/processed/train.Ghomala-Limbum.Limbum.pth'), 'valid': ('/home/jupyter/models/africa/cluster3/data/XLM_all/processed/valid.Ghomala-Limbum.Ghomala.pth', '/home/jupyter/models/africa/cluster3/data/XLM_all/processed/valid.Ghomala-Limbum.Limbum.pth'), 'test': ('/home/jupyter/models/africa/cluster3/data/XLM_all/processed/test.Ghomala-Limbum.Ghomala.pth', '/home/jupyter/models/africa/cluster3/data/XLM_all/processed/test.Ghomala-Limbum.Limbum.pth')}, ('Ghomala', 'Ngiemboon'): {'train': ('/home/jupyter/models/africa/cluster3/data/XLM_all/processed/train.Ghomala-Ngiemboon.Ghomala.pth', '/home/jupyter/models/africa/cluster3/data/XLM_all/processed/train.Ghomala-Ngiemboon.Ngiemboon.pth'), 'valid': ('/home/jupyter/models/africa/cluster3/data/XLM_all/processed/valid.Ghomala-Ngiemboon.Ghomala.pth', '/home/jupyter/models/africa/cluster3/data/XLM_all/processed/valid.Ghomala-Ngiemboon.Ngiemboon.pth'), 'test': ('/home/jupyter/models/africa/cluster3/data/XLM_all/processed/test.Ghomala-Ngiemboon.Ghomala.pth', '/home/jupyter/models/africa/cluster3/data/XLM_all/processed/test.Ghomala-Ngiemboon.Ngiemboon.pth')}, ('Limbum', 'Ngiemboon'): {'train': ('/home/jupyter/models/africa/cluster3/data/XLM_all/processed/train.Limbum-Ngiemboon.Limbum.pth', '/home/jupyter/models/africa/cluster3/data/XLM_all/processed/train.Limbum-Ngiemboon.Ngiemboon.pth'), 'valid': ('/home/jupyter/models/africa/cluster3/data/XLM_all/processed/valid.Limbum-Ngiemboon.Limbum.pth', '/home/jupyter/models/africa/cluster3/data/XLM_all/processed/valid.Limbum-Ngiemboon.Ngiemboon.pth'), 'test': ('/home/jupyter/models/africa/cluster3/data/XLM_all/processed/test.Limbum-Ngiemboon.Limbum.pth', '/home/jupyter/models/africa/cluster3/data/XLM_all/processed/test.Limbum-Ngiemboon.Ngiemboon.pth')}}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbumNgiemboon/maml/best-valid_mlm_ppl.pth,/home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbumNgiemboon/maml/best-valid_mlm_ppl.pth\n",
      "                                     remove_long_sentences: {'train': True, 'valid': True, 'test': True}\n",
      "                                     remove_long_sentences_test: True\n",
      "                                     remove_long_sentences_train: True\n",
      "                                     remove_long_sentences_valid: True\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: valid_Ghomala-Limbum_mt_bleu,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: 2000\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: valid_Ghomala-Limbum_mt_bleu\n",
      "                                     word_blank: 0.1\n",
      "                                     word_dropout: 0.1\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 3.0\n",
      "                                     world_size: 1\n",
      "INFO - 05/29/20 19:48:17 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster3/SupMT_XLM_all/maml\n",
      "                                     \n",
      "INFO - 05/29/20 19:48:17 - 0:00:00 - Running command: python train.py --eval_only True --mlm_steps 'Ghomala,Limbum,Ngiemboon,Ghomala-Limbum,Ghomala-Ngiemboon,Limbum-Ngiemboon' --exp_name SupMT_XLM_all --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --reload_model '/home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbumNgiemboon/maml/best-valid_mlm_ppl.pth,/home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbumNgiemboon/maml/best-valid_mlm_ppl.pth' --data_path '/home/jupyter/models/africa/cluster3/data/XLM_all/processed' --lgs 'Ghomala-Limbum-Ngiemboon' --ae_steps 'Ghomala,Limbum,Ngiemboon' --mt_steps 'Ghomala-Limbum,Limbum-Ghomala,Limbum-Ngiemboon,Ngiemboon-Limbum,Ghomala-Ngiemboon,Ngiemboon-Ghomala' --bt_steps 'Ghomala-Limbum-Ghomala,Limbum-Ghomala-Limbum,Limbum-Ngiemboon-Limbum,Ngiemboon-Limbum-Ngiemboon,Ghomala-Ngiemboon-Ghomala,Ngiemboon-Ghomala-Ngiemboon' --word_shuffle 3 --word_dropout '0.1' --word_blank '0.1' --lambda_ae '0:1,100000:0.1,300000:0' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --tokens_per_batch 2000 --batch_size 32 --bptt 256 --optimizer 'adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001' --epoch_size 6339 --max_epoch 100 --eval_bleu True --stopping_criterion 'valid_Ghomala-Limbum_mt_bleu,10' --validation_metrics 'valid_Ghomala-Limbum_mt_bleu' --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True\n",
      "\n",
      "WARNING - 05/29/20 19:48:17 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/29/20 19:48:17 - 0:00:00 - ============ langs: Ghomala, Limbum, Ngiemboon\n",
      "INFO - 05/29/20 19:48:17 - 0:00:00 - ============ Monolingual data (Ghomala)\n",
      "INFO - 05/29/20 19:48:17 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/XLM_all/processed/valid.Ghomala.pth ...\n",
      "INFO - 05/29/20 19:48:17 - 0:00:00 - 25976 words (16375 unique) in 792 sentences. 3 unknown words (3 unique) covering 0.01% of the data.\n",
      "INFO - 05/29/20 19:48:17 - 0:00:00 - ========================== debug : 1\n",
      "INFO - 05/29/20 19:48:17 - 0:00:00 - ========================== debug : 2\n",
      "INFO - 05/29/20 19:48:17 - 0:00:00 - Removed 0 too long sentences.\n",
      "\n",
      "INFO - 05/29/20 19:48:17 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/XLM_all/processed/test.Ghomala.pth ...\n",
      "INFO - 05/29/20 19:48:17 - 0:00:00 - 26091 words (16375 unique) in 792 sentences. 5 unknown words (4 unique) covering 0.02% of the data.\n",
      "INFO - 05/29/20 19:48:17 - 0:00:00 - ========================== debug : 3\n",
      "INFO - 05/29/20 19:48:17 - 0:00:00 - ========================== debug : 4\n",
      "INFO - 05/29/20 19:48:17 - 0:00:00 - Removed 0 too long sentences.\n",
      "\n",
      "INFO - 05/29/20 19:48:17 - 0:00:00 - ============ Monolingual data (Limbum)\n",
      "INFO - 05/29/20 19:48:17 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/XLM_all/processed/valid.Limbum.pth ...\n",
      "INFO - 05/29/20 19:48:17 - 0:00:00 - 32833 words (16375 unique) in 790 sentences. 7 unknown words (7 unique) covering 0.02% of the data.\n",
      "INFO - 05/29/20 19:48:17 - 0:00:00 - ========================== debug : 5\n",
      "INFO - 05/29/20 19:48:17 - 0:00:00 - ========================== debug : 6\n",
      "INFO - 05/29/20 19:48:17 - 0:00:00 - Removed 4 too long sentences.\n",
      "\n",
      "INFO - 05/29/20 19:48:17 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/XLM_all/processed/test.Limbum.pth ...\n",
      "INFO - 05/29/20 19:48:17 - 0:00:00 - 32035 words (16375 unique) in 790 sentences. 13 unknown words (12 unique) covering 0.04% of the data.\n",
      "INFO - 05/29/20 19:48:17 - 0:00:00 - ========================== debug : 7\n",
      "INFO - 05/29/20 19:48:17 - 0:00:00 - ========================== debug : 8\n",
      "INFO - 05/29/20 19:48:17 - 0:00:00 - Removed 1 too long sentences.\n",
      "\n",
      "INFO - 05/29/20 19:48:17 - 0:00:00 - ============ Monolingual data (Ngiemboon)\n",
      "INFO - 05/29/20 19:48:17 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/XLM_all/processed/valid.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 19:48:17 - 0:00:00 - 26849 words (16375 unique) in 790 sentences. 37 unknown words (36 unique) covering 0.14% of the data.\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - ========================== debug : 9\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - ========================== debug : 10\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - Removed 0 too long sentences.\n",
      "\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/XLM_all/processed/test.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - 26488 words (16375 unique) in 790 sentences. 48 unknown words (46 unique) covering 0.18% of the data.\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - ========================== debug : 11\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - ========================== debug : 12\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - Removed 0 too long sentences.\n",
      "\n",
      "\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - ============ Parallel data (Ghomala-Limbum)\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/XLM_all/processed/valid.Ghomala-Limbum.Ghomala.pth ...\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - 26398 words (16375 unique) in 791 sentences. 6 unknown words (5 unique) covering 0.02% of the data.\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/XLM_all/processed/valid.Ghomala-Limbum.Limbum.pth ...\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - 32437 words (16375 unique) in 791 sentences. 13 unknown words (12 unique) covering 0.04% of the data.\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - ========================== debug : 13\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - Removed 0 too long sentences.\n",
      "\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/XLM_all/processed/test.Ghomala-Limbum.Ghomala.pth ...\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - 25790 words (16375 unique) in 791 sentences. 2 unknown words (2 unique) covering 0.01% of the data.\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/XLM_all/processed/test.Ghomala-Limbum.Limbum.pth ...\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - 32555 words (16375 unique) in 791 sentences. 8 unknown words (7 unique) covering 0.02% of the data.\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - ========================== debug : 14\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - Removed 4 too long sentences.\n",
      "\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - ============ Parallel data (Ghomala-Ngiemboon)\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/XLM_all/processed/valid.Ghomala-Ngiemboon.Ghomala.pth ...\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - 25976 words (16375 unique) in 792 sentences. 3 unknown words (3 unique) covering 0.01% of the data.\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/XLM_all/processed/valid.Ghomala-Ngiemboon.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - 26583 words (16375 unique) in 792 sentences. 41 unknown words (41 unique) covering 0.15% of the data.\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - ========================== debug : 15\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - Removed 1 too long sentences.\n",
      "\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/XLM_all/processed/test.Ghomala-Ngiemboon.Ghomala.pth ...\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - 26091 words (16375 unique) in 792 sentences. 5 unknown words (4 unique) covering 0.02% of the data.\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/XLM_all/processed/test.Ghomala-Ngiemboon.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - 26922 words (16375 unique) in 792 sentences. 44 unknown words (41 unique) covering 0.16% of the data.\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - ========================== debug : 16\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - Removed 0 too long sentences.\n",
      "\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - ============ Parallel data (Limbum-Ngiemboon)\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/XLM_all/processed/valid.Limbum-Ngiemboon.Limbum.pth ...\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - 32833 words (16375 unique) in 790 sentences. 7 unknown words (7 unique) covering 0.02% of the data.\n",
      "INFO - 05/29/20 19:48:18 - 0:00:00 - Loading data from /home/jupyter/models/africa/cluster3/data/XLM_all/processed/valid.Limbum-Ngiemboon.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 19:48:18 - 0:00:01 - 26849 words (16375 unique) in 790 sentences. 37 unknown words (36 unique) covering 0.14% of the data.\n",
      "INFO - 05/29/20 19:48:18 - 0:00:01 - ========================== debug : 17\n",
      "INFO - 05/29/20 19:48:18 - 0:00:01 - Removed 0 empty sentences.\n",
      "INFO - 05/29/20 19:48:18 - 0:00:01 - Removed 4 too long sentences.\n",
      "\n",
      "INFO - 05/29/20 19:48:18 - 0:00:01 - Loading data from /home/jupyter/models/africa/cluster3/data/XLM_all/processed/test.Limbum-Ngiemboon.Limbum.pth ...\n",
      "INFO - 05/29/20 19:48:18 - 0:00:01 - 32035 words (16375 unique) in 790 sentences. 13 unknown words (12 unique) covering 0.04% of the data.\n",
      "INFO - 05/29/20 19:48:18 - 0:00:01 - Loading data from /home/jupyter/models/africa/cluster3/data/XLM_all/processed/test.Limbum-Ngiemboon.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 19:48:18 - 0:00:01 - 26488 words (16375 unique) in 790 sentences. 48 unknown words (46 unique) covering 0.18% of the data.\n",
      "INFO - 05/29/20 19:48:18 - 0:00:01 - ========================== debug : 18\n",
      "INFO - 05/29/20 19:48:18 - 0:00:01 - Removed 0 empty sentences.\n",
      "INFO - 05/29/20 19:48:18 - 0:00:01 - Removed 1 too long sentences.\n",
      "\n",
      "\n",
      "INFO - 05/29/20 19:48:18 - 0:00:01 - ============ Data summary\n",
      "INFO - 05/29/20 19:48:18 - 0:00:01 - Monolingual data   - valid -      Ghomala:       792\n",
      "INFO - 05/29/20 19:48:18 - 0:00:01 - Monolingual data   -  test -      Ghomala:       792\n",
      "INFO - 05/29/20 19:48:18 - 0:00:01 - Monolingual data   - valid -       Limbum:       790\n",
      "INFO - 05/29/20 19:48:18 - 0:00:01 - Monolingual data   -  test -       Limbum:       790\n",
      "INFO - 05/29/20 19:48:18 - 0:00:01 - Monolingual data   - valid -    Ngiemboon:       790\n",
      "INFO - 05/29/20 19:48:18 - 0:00:01 - Monolingual data   -  test -    Ngiemboon:       790\n",
      "INFO - 05/29/20 19:48:18 - 0:00:01 - Parallel data      - valid - Ghomala-Limbum:       791\n",
      "INFO - 05/29/20 19:48:18 - 0:00:01 - Parallel data      -  test - Ghomala-Limbum:       787\n",
      "INFO - 05/29/20 19:48:18 - 0:00:01 - Parallel data      - valid - Ghomala-Ngiemboon:       791\n",
      "INFO - 05/29/20 19:48:18 - 0:00:01 - Parallel data      -  test - Ghomala-Ngiemboon:       792\n",
      "INFO - 05/29/20 19:48:18 - 0:00:01 - Parallel data      - valid - Limbum-Ngiemboon:       786\n",
      "INFO - 05/29/20 19:48:18 - 0:00:01 - Parallel data      -  test - Limbum-Ngiemboon:       789\n",
      "\n",
      "INFO - 05/29/20 19:48:20 - 0:00:03 - Reloading encoder from /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbumNgiemboon/maml/best-valid_mlm_ppl.pth ...\n",
      "INFO - 05/29/20 19:48:27 - 0:00:10 - Reloading decoder from /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbumNgiemboon/maml/best-valid_mlm_ppl.pth ...\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter layer_norm15.0.weight not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter layer_norm15.0.bias not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.0.q_lin.weight not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.0.q_lin.bias not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.0.k_lin.weight not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.0.k_lin.bias not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.0.v_lin.weight not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.0.v_lin.bias not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.0.out_lin.weight not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.0.out_lin.bias not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter layer_norm15.1.weight not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter layer_norm15.1.bias not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.1.q_lin.weight not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.1.q_lin.bias not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.1.k_lin.weight not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.1.k_lin.bias not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.1.v_lin.weight not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.1.v_lin.bias not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.1.out_lin.weight not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.1.out_lin.bias not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter layer_norm15.2.weight not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter layer_norm15.2.bias not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.2.q_lin.weight not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.2.q_lin.bias not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.2.k_lin.weight not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.2.k_lin.bias not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.2.v_lin.weight not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.2.v_lin.bias not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.2.out_lin.weight not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.2.out_lin.bias not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter layer_norm15.3.weight not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter layer_norm15.3.bias not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.3.q_lin.weight not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.3.q_lin.bias not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.3.k_lin.weight not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.3.k_lin.bias not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.3.v_lin.weight not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.3.v_lin.bias not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.3.out_lin.weight not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.3.out_lin.bias not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter layer_norm15.4.weight not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter layer_norm15.4.bias not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.4.q_lin.weight not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.4.q_lin.bias not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.4.k_lin.weight not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.4.k_lin.bias not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.4.v_lin.weight not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.4.v_lin.bias not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.4.out_lin.weight not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.4.out_lin.bias not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter layer_norm15.5.weight not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter layer_norm15.5.bias not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.5.q_lin.weight not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.5.q_lin.bias not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.5.k_lin.weight not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.5.k_lin.bias not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.5.v_lin.weight not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.5.v_lin.bias not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.5.out_lin.weight not found.\n",
      "WARNING - 05/29/20 19:48:27 - 0:00:10 - Parameter encoder_attn.5.out_lin.bias not found.\n",
      "INFO - 05/29/20 19:48:28 - 0:00:10 - Number of parameters (encoder): 92891127\n",
      "INFO - 05/29/20 19:48:28 - 0:00:10 - Number of parameters (decoder): 118093815\n",
      "INFO - 05/29/20 19:48:28 - 0:00:11 - Found 0 memories.\n",
      "INFO - 05/29/20 19:48:28 - 0:00:11 - Found 12 FFN.\n",
      "INFO - 05/29/20 19:48:28 - 0:00:11 - Found 264 parameters in model.\n",
      "INFO - 05/29/20 19:48:28 - 0:00:11 - Optimizers: model\n",
      "WARNING - 05/29/20 19:48:28 - 0:00:11 - Reloading checkpoint from /home/jupyter/models/africa/cluster3/SupMT_XLM_all/maml/checkpoint.pth ...\n",
      "WARNING - 05/29/20 19:48:51 - 0:00:34 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/29/20 19:48:51 - 0:00:34 - Reloading 'num_updates' and 'lr' for optimizer model.\n",
      "WARNING - 05/29/20 19:48:51 - 0:00:34 - Checkpoint reloaded. Resuming at epoch 85 / iteration 1275 ...\n",
      "INFO - 05/29/20 19:49:24 - 0:01:07 - BLEU /home/jupyter/models/africa/cluster3/SupMT_XLM_all/maml/hypotheses/hyp85.Ngiemboon-Limbum.valid.txt /home/jupyter/models/africa/cluster3/SupMT_XLM_all/maml/hypotheses/ref.Ngiemboon-Limbum.valid.txt : 11.630000\n",
      "INFO - 05/29/20 19:49:47 - 0:01:30 - BLEU /home/jupyter/models/africa/cluster3/SupMT_XLM_all/maml/hypotheses/hyp85.Ghomala-Ngiemboon.valid.txt /home/jupyter/models/africa/cluster3/SupMT_XLM_all/maml/hypotheses/ref.Ghomala-Ngiemboon.valid.txt : 16.730000\n",
      "INFO - 05/29/20 19:50:11 - 0:01:54 - BLEU /home/jupyter/models/africa/cluster3/SupMT_XLM_all/maml/hypotheses/hyp85.Ghomala-Limbum.valid.txt /home/jupyter/models/africa/cluster3/SupMT_XLM_all/maml/hypotheses/ref.Ghomala-Limbum.valid.txt : 23.700000\n",
      "INFO - 05/29/20 19:50:34 - 0:02:16 - BLEU /home/jupyter/models/africa/cluster3/SupMT_XLM_all/maml/hypotheses/hyp85.Ngiemboon-Ghomala.valid.txt /home/jupyter/models/africa/cluster3/SupMT_XLM_all/maml/hypotheses/ref.Ngiemboon-Ghomala.valid.txt : 9.730000\n",
      "INFO - 05/29/20 19:50:58 - 0:02:40 - BLEU /home/jupyter/models/africa/cluster3/SupMT_XLM_all/maml/hypotheses/hyp85.Limbum-Ngiemboon.valid.txt /home/jupyter/models/africa/cluster3/SupMT_XLM_all/maml/hypotheses/ref.Limbum-Ngiemboon.valid.txt : 8.280000\n",
      "INFO - 05/29/20 19:51:21 - 0:03:03 - BLEU /home/jupyter/models/africa/cluster3/SupMT_XLM_all/maml/hypotheses/hyp85.Limbum-Ghomala.valid.txt /home/jupyter/models/africa/cluster3/SupMT_XLM_all/maml/hypotheses/ref.Limbum-Ghomala.valid.txt : 22.440000\n",
      "INFO - 05/29/20 19:51:49 - 0:03:32 - BLEU /home/jupyter/models/africa/cluster3/SupMT_XLM_all/maml/hypotheses/hyp85.Ngiemboon-Limbum.test.txt /home/jupyter/models/africa/cluster3/SupMT_XLM_all/maml/hypotheses/ref.Ngiemboon-Limbum.test.txt : 13.480000\n",
      "INFO - 05/29/20 19:52:12 - 0:03:55 - BLEU /home/jupyter/models/africa/cluster3/SupMT_XLM_all/maml/hypotheses/hyp85.Ghomala-Ngiemboon.test.txt /home/jupyter/models/africa/cluster3/SupMT_XLM_all/maml/hypotheses/ref.Ghomala-Ngiemboon.test.txt : 16.620000\n",
      "INFO - 05/29/20 19:52:37 - 0:04:20 - BLEU /home/jupyter/models/africa/cluster3/SupMT_XLM_all/maml/hypotheses/hyp85.Ghomala-Limbum.test.txt /home/jupyter/models/africa/cluster3/SupMT_XLM_all/maml/hypotheses/ref.Ghomala-Limbum.test.txt : 27.260000\n",
      "INFO - 05/29/20 19:52:59 - 0:04:42 - BLEU /home/jupyter/models/africa/cluster3/SupMT_XLM_all/maml/hypotheses/hyp85.Ngiemboon-Ghomala.test.txt /home/jupyter/models/africa/cluster3/SupMT_XLM_all/maml/hypotheses/ref.Ngiemboon-Ghomala.test.txt : 8.550000\n",
      "INFO - 05/29/20 19:53:22 - 0:05:05 - BLEU /home/jupyter/models/africa/cluster3/SupMT_XLM_all/maml/hypotheses/hyp85.Limbum-Ngiemboon.test.txt /home/jupyter/models/africa/cluster3/SupMT_XLM_all/maml/hypotheses/ref.Limbum-Ngiemboon.test.txt : 7.390000\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - BLEU /home/jupyter/models/africa/cluster3/SupMT_XLM_all/maml/hypotheses/hyp85.Limbum-Ghomala.test.txt /home/jupyter/models/africa/cluster3/SupMT_XLM_all/maml/hypotheses/ref.Limbum-Ghomala.test.txt : 24.820000\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - epoch -> 85.000000\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - valid_Ghomala_mlm_ppl -> 552.947545\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - valid_Ghomala_mlm_acc -> 15.025907\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - valid_Limbum_mlm_ppl -> 341.626713\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - valid_Limbum_mlm_acc -> 21.243523\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - valid_Ngiemboon_mlm_ppl -> 536.883752\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - valid_Ngiemboon_mlm_acc -> 17.875648\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - valid_Ghomala_Limbum_mlm_ppl -> 355.363740\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - valid_Ghomala_Limbum_mlm_acc -> 19.188641\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - valid_Ghomala_Ngiemboon_mlm_ppl -> 453.054928\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - valid_Ghomala_Ngiemboon_mlm_acc -> 18.469532\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - valid_Limbum_Ngiemboon_mlm_ppl -> 394.190708\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - valid_Limbum_Ngiemboon_mlm_acc -> 19.613153\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - valid_Ngiemboon-Limbum_mt_ppl -> 4.332418\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - valid_Ngiemboon-Limbum_mt_acc -> 72.635166\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - valid_Ngiemboon-Limbum_mt_bleu -> 11.630000\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - valid_Ghomala-Ngiemboon_mt_ppl -> 3.850104\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - valid_Ghomala-Ngiemboon_mt_acc -> 82.691320\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - valid_Ghomala-Ngiemboon_mt_bleu -> 16.730000\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - valid_Ghomala-Limbum_mt_ppl -> 2.548575\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - valid_Ghomala-Limbum_mt_acc -> 84.853136\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - valid_Ghomala-Limbum_mt_bleu -> 23.700000\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - valid_Ngiemboon-Ghomala_mt_ppl -> 5.374104\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - valid_Ngiemboon-Ghomala_mt_acc -> 72.697319\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - valid_Ngiemboon-Ghomala_mt_bleu -> 9.730000\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - valid_Limbum-Ngiemboon_mt_ppl -> 9.652221\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - valid_Limbum-Ngiemboon_mt_acc -> 67.947218\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - valid_Limbum-Ngiemboon_mt_bleu -> 8.280000\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - valid_Limbum-Ghomala_mt_ppl -> 3.067048\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - valid_Limbum-Ghomala_mt_acc -> 83.938357\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - valid_Limbum-Ghomala_mt_bleu -> 22.440000\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - valid_mlm_ppl -> 477.152670\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - valid_mlm_acc -> 18.048359\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - test_Ghomala_mlm_ppl -> 832.475405\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - test_Ghomala_mlm_acc -> 15.544041\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - test_Limbum_mlm_ppl -> 361.583173\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - test_Limbum_mlm_acc -> 23.575130\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - test_Ngiemboon_mlm_ppl -> 701.061697\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - test_Ngiemboon_mlm_acc -> 19.170984\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - test_Ghomala_Limbum_mlm_ppl -> 310.892459\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - test_Ghomala_Limbum_mlm_acc -> 20.504475\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - test_Ghomala_Ngiemboon_mlm_ppl -> 494.863820\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - test_Ghomala_Ngiemboon_mlm_acc -> 16.984479\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - test_Limbum_Ngiemboon_mlm_ppl -> 405.909994\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - test_Limbum_Ngiemboon_mlm_acc -> 18.755053\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - test_Ngiemboon-Limbum_mt_ppl -> 4.077641\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - test_Ngiemboon-Limbum_mt_acc -> 73.654894\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - test_Ngiemboon-Limbum_mt_bleu -> 13.480000\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - test_Ghomala-Ngiemboon_mt_ppl -> 4.146952\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - test_Ghomala-Ngiemboon_mt_acc -> 81.987443\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - test_Ghomala-Ngiemboon_mt_bleu -> 16.620000\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - test_Ghomala-Limbum_mt_ppl -> 2.188965\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - test_Ghomala-Limbum_mt_acc -> 87.101290\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - test_Ghomala-Limbum_mt_bleu -> 27.260000\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - test_Ngiemboon-Ghomala_mt_ppl -> 4.892762\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - test_Ngiemboon-Ghomala_mt_acc -> 73.235874\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - test_Ngiemboon-Ghomala_mt_bleu -> 8.550000\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - test_Limbum-Ngiemboon_mt_ppl -> 9.268886\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - test_Limbum-Ngiemboon_mt_acc -> 68.507413\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - test_Limbum-Ngiemboon_mt_bleu -> 7.390000\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - test_Limbum-Ghomala_mt_ppl -> 2.704557\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - test_Limbum-Ghomala_mt_acc -> 85.211776\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - test_Limbum-Ghomala_mt_bleu -> 24.820000\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - test_mlm_ppl -> 631.706758\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - test_mlm_acc -> 19.430052\n",
      "INFO - 05/29/20 19:53:45 - 0:05:27 - __log__:{\"epoch\": 85, \"valid_Ghomala_mlm_ppl\": 552.9475449632031, \"valid_Ghomala_mlm_acc\": 15.025906735751295, \"valid_Limbum_mlm_ppl\": 341.6267128652497, \"valid_Limbum_mlm_acc\": 21.243523316062177, \"valid_Ngiemboon_mlm_ppl\": 536.8837515536037, \"valid_Ngiemboon_mlm_acc\": 17.875647668393782, \"valid_Ghomala_Limbum_mlm_ppl\": 355.36373983861193, \"valid_Ghomala_Limbum_mlm_acc\": 19.188640973630832, \"valid_Ghomala_Ngiemboon_mlm_ppl\": 453.05492787735966, \"valid_Ghomala_Ngiemboon_mlm_acc\": 18.469532357109117, \"valid_Limbum_Ngiemboon_mlm_ppl\": 394.1907079532105, \"valid_Limbum_Ngiemboon_mlm_acc\": 19.613152804642166, \"valid_Ngiemboon-Limbum_mt_ppl\": 4.332417897987636, \"valid_Ngiemboon-Limbum_mt_acc\": 72.63516569671019, \"valid_Ngiemboon-Limbum_mt_bleu\": 11.63, \"valid_Ghomala-Ngiemboon_mt_ppl\": 3.850103549548208, \"valid_Ghomala-Ngiemboon_mt_acc\": 82.69131997945557, \"valid_Ghomala-Ngiemboon_mt_bleu\": 16.73, \"valid_Ghomala-Limbum_mt_ppl\": 2.5485750093947703, \"valid_Ghomala-Limbum_mt_acc\": 84.85313590947393, \"valid_Ghomala-Limbum_mt_bleu\": 23.7, \"valid_Ngiemboon-Ghomala_mt_ppl\": 5.374103580458748, \"valid_Ngiemboon-Ghomala_mt_acc\": 72.69731915530927, \"valid_Ngiemboon-Ghomala_mt_bleu\": 9.73, \"valid_Limbum-Ngiemboon_mt_ppl\": 9.652220905190788, \"valid_Limbum-Ngiemboon_mt_acc\": 67.9472183639155, \"valid_Limbum-Ngiemboon_mt_bleu\": 8.28, \"valid_Limbum-Ghomala_mt_ppl\": 3.067047513257482, \"valid_Limbum-Ghomala_mt_acc\": 83.93835742395822, \"valid_Limbum-Ghomala_mt_bleu\": 22.44, \"valid_mlm_ppl\": 477.1526697940188, \"valid_mlm_acc\": 18.048359240069086, \"test_Ghomala_mlm_ppl\": 832.4754049601092, \"test_Ghomala_mlm_acc\": 15.544041450777202, \"test_Limbum_mlm_ppl\": 361.5831732848086, \"test_Limbum_mlm_acc\": 23.575129533678755, \"test_Ngiemboon_mlm_ppl\": 701.0616968641718, \"test_Ngiemboon_mlm_acc\": 19.17098445595855, \"test_Ghomala_Limbum_mlm_ppl\": 310.89245899582676, \"test_Ghomala_Limbum_mlm_acc\": 20.50447518307567, \"test_Ghomala_Ngiemboon_mlm_ppl\": 494.8638197105313, \"test_Ghomala_Ngiemboon_mlm_acc\": 16.984478935698448, \"test_Limbum_Ngiemboon_mlm_ppl\": 405.9099940465992, \"test_Limbum_Ngiemboon_mlm_acc\": 18.755052546483427, \"test_Ngiemboon-Limbum_mt_ppl\": 4.077641189127198, \"test_Ngiemboon-Limbum_mt_acc\": 73.65489379935117, \"test_Ngiemboon-Limbum_mt_bleu\": 13.48, \"test_Ghomala-Ngiemboon_mt_ppl\": 4.1469517030700365, \"test_Ghomala-Ngiemboon_mt_acc\": 81.9874431695172, \"test_Ghomala-Ngiemboon_mt_bleu\": 16.62, \"test_Ghomala-Limbum_mt_ppl\": 2.1889653464308063, \"test_Ghomala-Limbum_mt_acc\": 87.10129047966886, \"test_Ghomala-Limbum_mt_bleu\": 27.26, \"test_Ngiemboon-Ghomala_mt_ppl\": 4.892761687743636, \"test_Ngiemboon-Ghomala_mt_acc\": 73.23587397239892, \"test_Ngiemboon-Ghomala_mt_bleu\": 8.55, \"test_Limbum-Ngiemboon_mt_ppl\": 9.26888552761597, \"test_Limbum-Ngiemboon_mt_acc\": 68.50741326662006, \"test_Limbum-Ngiemboon_mt_bleu\": 7.39, \"test_Limbum-Ghomala_mt_ppl\": 2.70455741940689, \"test_Limbum-Ghomala_mt_acc\": 85.21177587844255, \"test_Limbum-Ghomala_mt_bleu\": 24.82, \"test_mlm_ppl\": 631.7067583696966, \"test_mlm_acc\": 19.430051813471504}\n"
     ]
    }
   ],
   "source": [
    "%env eval_bleu=True\n",
    "! chmod +x src/evaluation/multi-bleu.perl\n",
    "\n",
    "%env stopping_criterion=valid_Ghomala-Limbum_mt_bleu,10\n",
    "%env validation_metrics=valid_Ghomala-Limbum_mt_bleu \n",
    "%env reload_model=/home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbumNgiemboon/maml/best-valid_mlm_ppl.pth,/home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbumNgiemboon/maml/best-valid_mlm_ppl.pth\n",
    "%env ae_steps=Ghomala,Limbum,Ngiemboon\n",
    "%env bt_steps=Ghomala-Limbum-Ghomala,Limbum-Ghomala-Limbum,Limbum-Ngiemboon-Limbum,Ngiemboon-Limbum-Ngiemboon,Ghomala-Ngiemboon-Ghomala,Ngiemboon-Ghomala-Ngiemboon\n",
    "%env mt_steps=Ghomala-Limbum,Limbum-Ghomala,Limbum-Ngiemboon,Ngiemboon-Limbum,Ghomala-Ngiemboon,Ngiemboon-Ghomala           \n",
    "\n",
    "%env mlm_steps=Ghomala,Limbum,Ngiemboon,Ghomala-Limbum,Ghomala-Ngiemboon,Limbum-Ngiemboon\n",
    "! python train.py --eval_only $eval_only --mlm_steps $mlm_steps --exp_name SupMT_XLM_all --exp_id $exp_id  --dump_path $dump_path --reload_model $reload_model --data_path $OUTPATH --lgs $lgs --ae_steps $ae_steps --mt_steps $mt_steps --bt_steps $bt_steps --word_shuffle 3 --word_dropout 0.1 --word_blank 0.1 --lambda_ae '0:1,100000:0.1,300000:0' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout 0.1 --attention_dropout 0.1 --gelu_activation true --tokens_per_batch 2000 --batch_size $batch_size --bptt 256 --optimizer adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001 --epoch_size $epoch_size --max_epoch $max_epoch --eval_bleu $eval_bleu --stopping_criterion $stopping_criterion --validation_metrics $validation_metrics --train_n_samples $train_n_samples --valid_n_samples $valid_n_samples --test_n_samples $test_n_samples --remove_long_sentences_train $remove_long_sentences_train --remove_long_sentences_valid $remove_long_sentences_valid --remove_long_sentences_test $remove_long_sentences_test    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
