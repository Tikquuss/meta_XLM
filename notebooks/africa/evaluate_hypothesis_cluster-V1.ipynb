{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(bookmark:HOME) -> /home/jupyter/meta_XLM/XLM\n",
      "/home/jupyter/meta_XLM/XLM\n"
     ]
    }
   ],
   "source": [
    "%bookmark HOME \"/home/jupyter/meta_XLM/XLM\" \n",
    "%cd -b HOME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: csv_path=/home/jupyter\n",
      "env: output_dir=/home/jupyter/data/evaluation_hypothesis\n",
      "env: data_type=mono\n"
     ]
    }
   ],
   "source": [
    "%env csv_path=/home/jupyter\n",
    "%env output_dir=/home/jupyter/data/evaluation_hypothesis\n",
    "%env data_type=mono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: languages=Bafia,Bafia\n",
      "Bafia\n",
      "======= Read 7950 totals samples\n",
      "======= Delete 0 samples\n",
      "======= Save 7950 samples\n",
      "env: languages=Bulu,Bulu\n",
      "Bulu\n",
      "======= Read 7950 totals samples\n",
      "======= Delete 4 samples\n",
      "======= Save 7946 samples\n",
      "env: languages=MKPAMAN_AMVOE_Ewondo,MKPAMAN_AMVOE_Ewondo\n",
      "MKPAMAN_AMVOE_Ewondo\n",
      "======= Read 7950 totals samples\n",
      "======= Delete 6 samples\n",
      "======= Save 7944 samples\n",
      "env: languages=Ghomala,Ghomala\n",
      "Ghomala\n",
      "======= Read 7950 totals samples\n",
      "======= Delete 8 samples\n",
      "======= Save 7942 samples\n",
      "env: languages=Limbum,Limbum\n",
      "Limbum\n",
      "======= Read 7950 totals samples\n",
      "======= Delete 31 samples\n",
      "======= Save 7919 samples\n",
      "env: languages=Ngiemboon,Ngiemboon\n",
      "Ngiemboon\n",
      "======= Read 7950 totals samples\n",
      "======= Delete 21 samples\n",
      "======= Save 7929 samples\n"
     ]
    }
   ],
   "source": [
    "# \"--new_only\" True To make sure our corpora's the same size.\n",
    "%env languages=Bafia,Bafia\n",
    "! python ../bible.py --csv_path $csv_path --output_dir $output_dir --data_type $data_type --languages $languages --new_only True\n",
    "\n",
    "%env languages=Bulu,Bulu\n",
    "! python ../bible.py --csv_path $csv_path --output_dir $output_dir --data_type $data_type --languages $languages --new_only True\n",
    "\n",
    "%env languages=MKPAMAN_AMVOE_Ewondo,MKPAMAN_AMVOE_Ewondo\n",
    "! python ../bible.py --csv_path $csv_path --output_dir $output_dir --data_type $data_type --languages $languages --new_only True\n",
    "\n",
    "%env languages=Ghomala,Ghomala\n",
    "! python ../bible.py --csv_path $csv_path --output_dir $output_dir --data_type $data_type --languages $languages --new_only True\n",
    "\n",
    "%env languages=Limbum,Limbum\n",
    "! python ../bible.py --csv_path $csv_path --output_dir $output_dir --data_type $data_type --languages $languages --new_only True\n",
    "\n",
    "%env languages=Ngiemboon,Ngiemboon\n",
    "! python ../bible.py --csv_path $csv_path --output_dir $output_dir --data_type $data_type --languages $languages --new_only True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select below each language pair trained for the purpose of MLM+TLM, and for this pair we use its code and vocabulary to process the other pairs in order to start the evaluation; and in this evaluation we rename the evaluated pair data files into the data files of the evaluating pairs.\n",
    "That's long enough!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: src_path=/home/jupyter/data/evaluation_hypothesis\n",
      "env: threads_for_tokenizer=16\n",
      "env: n_samples=-1\n",
      "env: test_size=100\n",
      "env: val_size=0\n",
      "env: TOKENIZE=tools/tokenizer_our.sh\n",
      "env: LOWER_REMOVE_ACCENT=tools/lowercase_and_remove_accent.py\n",
      "env: FASTBPE=tools/fastBPE/fast\n",
      "env: duplicate=True\n"
     ]
    }
   ],
   "source": [
    "%env src_path=/home/jupyter/data/evaluation_hypothesis\n",
    "%env threads_for_tokenizer=16 \n",
    "%env n_samples=-1\n",
    "# No need for training data\n",
    "%env test_size=100            \n",
    "%env val_size=0\n",
    "# tools paths\n",
    "%env TOKENIZE=tools/tokenizer_our.sh\n",
    "%env LOWER_REMOVE_ACCENT=tools/lowercase_and_remove_accent.py\n",
    "%env FASTBPE=tools/fastBPE/fast\n",
    "! chmod +x $FASTBPE\n",
    "! chmod +x tools/mosesdecoder/scripts/tokenizer/*.perl\n",
    "! chmod +x ../build_evaluate_data.sh\n",
    "\n",
    "################\n",
    "%env duplicate=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir /home/jupyter/models/africa/evaluation_hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bafia_Bulu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu\n",
      "env: CODE_VOCAB_PATH=/home/jupyter/models/africa/cluster1/data/Bafia_Bulu/processed\n",
      "*** Cleaning and tokenizing MKPAMAN_AMVOE_Ewondo data ... ***\n",
      "Tokenizer Version 1.1\n",
      "Language: MKPAMAN_AMVOE_Ewondo\n",
      "Number of threads: 16\n",
      "WARNING: No known abbreviations for language 'MKPAMAN_AMVOE_Ewondo', attempting fall-back to English version...\n",
      "*** Tokenized (+ lowercase + accent-removal) MKPAMAN_AMVOE_Ewondo data to /home/jupyter/data/evaluation_hypothesis/MKPAMAN_AMVOE_Ewondo.all ***\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Bulu/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/MKPAMAN_AMVOE_Ewondo.all ...\n",
      "Read 185471 words (7559 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/MKPAMAN_AMVOE_Ewondo.all ...\n",
      "Modified 185471 words from text file.\n",
      "INFO - 05/30/20 15:56:14 - 0:00:00 - Read 8683 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/30/20 15:56:14 - 0:00:00 - 278935 words (8683 unique) in 7944 sentences.\n",
      "INFO - 05/30/20 15:56:14 - 0:00:00 - 20549 unknown words (363 unique), covering 7.37% of the data.\n",
      "*** Cleaning and tokenizing Ghomala data ... ***\n",
      "Tokenizer Version 1.1\n",
      "Language: Ghomala\n",
      "Number of threads: 16\n",
      "WARNING: No known abbreviations for language 'Ghomala', attempting fall-back to English version...\n",
      "*** Tokenized (+ lowercase + accent-removal) Ghomala data to /home/jupyter/data/evaluation_hypothesis/Ghomala.all ***\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Bulu/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/Ghomala.all ...\n",
      "Read 259016 words (2525 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/Ghomala.all ...\n",
      "Modified 259016 words from text file.\n",
      "INFO - 05/30/20 15:56:16 - 0:00:00 - Read 8683 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Ghomala.pth ...\n",
      "INFO - 05/30/20 15:56:16 - 0:00:00 - 474256 words (8683 unique) in 7942 sentences.\n",
      "INFO - 05/30/20 15:56:16 - 0:00:00 - 133505 unknown words (178 unique), covering 28.15% of the data.\n",
      "*** Cleaning and tokenizing Limbum data ... ***\n",
      "Tokenizer Version 1.1\n",
      "Language: Limbum\n",
      "Number of threads: 16\n",
      "WARNING: No known abbreviations for language 'Limbum', attempting fall-back to English version...\n",
      "*** Tokenized (+ lowercase + accent-removal) Limbum data to /home/jupyter/data/evaluation_hypothesis/Limbum.all ***\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Bulu/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/Limbum.all ...\n",
      "Read 321354 words (2890 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/Limbum.all ...\n",
      "Modified 321354 words from text file.\n",
      "INFO - 05/30/20 15:56:18 - 0:00:00 - Read 8683 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Limbum.pth ...\n",
      "INFO - 05/30/20 15:56:18 - 0:00:00 - 488768 words (8683 unique) in 7919 sentences.\n",
      "INFO - 05/30/20 15:56:18 - 0:00:00 - 28300 unknown words (173 unique), covering 5.79% of the data.\n",
      "*** Cleaning and tokenizing Ngiemboon data ... ***\n",
      "Tokenizer Version 1.1\n",
      "Language: Ngiemboon\n",
      "Number of threads: 16\n",
      "WARNING: No known abbreviations for language 'Ngiemboon', attempting fall-back to English version...\n",
      "*** Tokenized (+ lowercase + accent-removal) Ngiemboon data to /home/jupyter/data/evaluation_hypothesis/Ngiemboon.all ***\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Bulu/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/Ngiemboon.all ...\n",
      "Read 256452 words (12658 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/Ngiemboon.all ...\n",
      "Modified 256452 words from text file.\n",
      "INFO - 05/30/20 15:56:20 - 0:00:00 - Read 8683 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Ngiemboon.pth ...\n",
      "INFO - 05/30/20 15:56:21 - 0:00:01 - 808302 words (8683 unique) in 7929 sentences.\n",
      "INFO - 05/30/20 15:56:21 - 0:00:01 - 236821 unknown words (171 unique), covering 29.30% of the data.\n"
     ]
    }
   ],
   "source": [
    "%env tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu\n",
    "%env CODE_VOCAB_PATH=/home/jupyter/models/africa/cluster1/data/Bafia_Bulu/processed\n",
    "! ../build_evaluate_data.sh MKPAMAN_AMVOE_Ewondo $n_samples\n",
    "! ../build_evaluate_data.sh Ghomala $n_samples\n",
    "! ../build_evaluate_data.sh Limbum $n_samples\n",
    "! ../build_evaluate_data.sh Ngiemboon $n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bafia_Ewondo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo\n",
      "env: CODE_VOCAB_PATH=/home/jupyter/models/africa/cluster1/data/Bafia_Ewondo/processed\n",
      "*** Cleaning and tokenizing Bulu data ... ***\n",
      "Tokenizer Version 1.1\n",
      "Language: Bulu\n",
      "Number of threads: 16\n",
      "WARNING: No known abbreviations for language 'Bulu', attempting fall-back to English version...\n",
      "*** Tokenized (+ lowercase + accent-removal) Bulu data to /home/jupyter/data/evaluation_hypothesis/Bulu.all ***\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/Bulu.all ...\n",
      "Read 225215 words (5295 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/Bulu.all ...\n",
      "Modified 225215 words from text file.\n",
      "INFO - 05/30/20 15:57:12 - 0:00:00 - Read 9128 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.Bulu.pth ...\n",
      "INFO - 05/30/20 15:57:13 - 0:00:00 - 322451 words (9128 unique) in 7946 sentences.\n",
      "INFO - 05/30/20 15:57:13 - 0:00:00 - 19871 unknown words (220 unique), covering 6.16% of the data.\n",
      "*** Cleaning and tokenizing Ghomala data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Ghomala.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/Ghomala.all ...\n",
      "Read 259016 words (2525 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/Ghomala.all ...\n",
      "Modified 259016 words from text file.\n",
      "INFO - 05/30/20 15:57:13 - 0:00:00 - Read 9128 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.Ghomala.pth ...\n",
      "INFO - 05/30/20 15:57:14 - 0:00:00 - 473876 words (9128 unique) in 7942 sentences.\n",
      "INFO - 05/30/20 15:57:14 - 0:00:00 - 114756 unknown words (125 unique), covering 24.22% of the data.\n",
      "*** Cleaning and tokenizing Limbum data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Limbum.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/Limbum.all ...\n",
      "Read 321354 words (2890 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/Limbum.all ...\n",
      "Modified 321354 words from text file.\n",
      "INFO - 05/30/20 15:57:15 - 0:00:00 - Read 9128 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.Limbum.pth ...\n",
      "INFO - 05/30/20 15:57:15 - 0:00:00 - 489888 words (9128 unique) in 7919 sentences.\n",
      "INFO - 05/30/20 15:57:15 - 0:00:00 - 12558 unknown words (116 unique), covering 2.56% of the data.\n",
      "*** Cleaning and tokenizing Ngiemboon data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Ngiemboon.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/Ngiemboon.all ...\n",
      "Read 256452 words (12658 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/Ngiemboon.all ...\n",
      "Modified 256452 words from text file.\n",
      "INFO - 05/30/20 15:57:16 - 0:00:00 - Read 9128 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.Ngiemboon.pth ...\n",
      "INFO - 05/30/20 15:57:16 - 0:00:01 - 824300 words (9128 unique) in 7929 sentences.\n",
      "INFO - 05/30/20 15:57:16 - 0:00:01 - 215073 unknown words (130 unique), covering 26.09% of the data.\n"
     ]
    }
   ],
   "source": [
    "%env tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo\n",
    "%env CODE_VOCAB_PATH=/home/jupyter/models/africa/cluster1/data/Bafia_Ewondo/processed\n",
    "! ../build_evaluate_data.sh Bulu $n_samples\n",
    "! ../build_evaluate_data.sh Ghomala $n_samples\n",
    "! ../build_evaluate_data.sh Limbum $n_samples\n",
    "! ../build_evaluate_data.sh Ngiemboon $n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bulu_Ewondo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo\n",
      "env: CODE_VOCAB_PATH=/home/jupyter/models/africa/cluster1/data/Bulu_MKPAMAN_AMVOE_Ewondo/processed\n",
      "*** Cleaning and tokenizing Bafia data ... ***\n",
      "Tokenizer Version 1.1\n",
      "Language: Bafia\n",
      "Number of threads: 16\n",
      "WARNING: No known abbreviations for language 'Bafia', attempting fall-back to English version...\n",
      "*** Tokenized (+ lowercase + accent-removal) Bafia data to /home/jupyter/data/evaluation_hypothesis/Bafia.all ***\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bulu_MKPAMAN_AMVOE_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/Bafia.all ...\n",
      "Read 342491 words (5192 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/Bafia.all ...\n",
      "Modified 342491 words from text file.\n",
      "INFO - 05/30/20 15:59:15 - 0:00:00 - Read 9061 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.Bafia.pth ...\n",
      "INFO - 05/30/20 15:59:16 - 0:00:00 - 621128 words (9061 unique) in 7950 sentences.\n",
      "INFO - 05/30/20 15:59:16 - 0:00:00 - 214574 unknown words (147 unique), covering 34.55% of the data.\n",
      "*** Cleaning and tokenizing Ghomala data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Ghomala.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bulu_MKPAMAN_AMVOE_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/Ghomala.all ...\n",
      "Read 259016 words (2525 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/Ghomala.all ...\n",
      "Modified 259016 words from text file.\n",
      "INFO - 05/30/20 15:59:17 - 0:00:00 - Read 9061 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.Ghomala.pth ...\n",
      "INFO - 05/30/20 15:59:17 - 0:00:00 - 519914 words (9061 unique) in 7942 sentences.\n",
      "INFO - 05/30/20 15:59:17 - 0:00:00 - 168025 unknown words (130 unique), covering 32.32% of the data.\n",
      "*** Cleaning and tokenizing Limbum data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Limbum.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bulu_MKPAMAN_AMVOE_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/Limbum.all ...\n",
      "Read 321354 words (2890 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/Limbum.all ...\n",
      "Modified 321354 words from text file.\n",
      "INFO - 05/30/20 15:59:18 - 0:00:00 - Read 9061 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.Limbum.pth ...\n",
      "INFO - 05/30/20 15:59:18 - 0:00:00 - 532918 words (9061 unique) in 7919 sentences.\n",
      "INFO - 05/30/20 15:59:18 - 0:00:00 - 30235 unknown words (121 unique), covering 5.67% of the data.\n",
      "*** Cleaning and tokenizing Ngiemboon data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Ngiemboon.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bulu_MKPAMAN_AMVOE_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/Ngiemboon.all ...\n",
      "Read 256452 words (12658 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/Ngiemboon.all ...\n",
      "Modified 256452 words from text file.\n",
      "INFO - 05/30/20 15:59:19 - 0:00:00 - Read 9061 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.Ngiemboon.pth ...\n",
      "INFO - 05/30/20 15:59:20 - 0:00:01 - 845145 words (9061 unique) in 7929 sentences.\n",
      "INFO - 05/30/20 15:59:20 - 0:00:01 - 262724 unknown words (160 unique), covering 31.09% of the data.\n"
     ]
    }
   ],
   "source": [
    "%env tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo\n",
    "%env CODE_VOCAB_PATH=/home/jupyter/models/africa/cluster1/data/Bulu_MKPAMAN_AMVOE_Ewondo/processed\n",
    "! ../build_evaluate_data.sh Bafia $n_samples\n",
    "! ../build_evaluate_data.sh Ghomala $n_samples\n",
    "! ../build_evaluate_data.sh Limbum $n_samples\n",
    "! ../build_evaluate_data.sh Ngiemboon $n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ghomala_Limbum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum\n",
      "env: CODE_VOCAB_PATH=/home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed\n",
      "*** Cleaning and tokenizing Bafia data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Bafia.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/Bafia.all ...\n",
      "Read 342491 words (5192 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/Bafia.all ...\n",
      "Modified 342491 words from text file.\n",
      "INFO - 05/30/20 15:59:34 - 0:00:00 - Read 5057 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Bafia.pth ...\n",
      "INFO - 05/30/20 15:59:34 - 0:00:00 - 552895 words (5057 unique) in 7950 sentences.\n",
      "INFO - 05/30/20 15:59:34 - 0:00:00 - 171055 unknown words (374 unique), covering 30.94% of the data.\n",
      "*** Cleaning and tokenizing Bulu data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Bulu.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/Bulu.all ...\n",
      "Read 225215 words (5295 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/Bulu.all ...\n",
      "Modified 225215 words from text file.\n",
      "INFO - 05/30/20 15:59:35 - 0:00:00 - Read 5057 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Bulu.pth ...\n",
      "INFO - 05/30/20 15:59:35 - 0:00:00 - 379166 words (5057 unique) in 7946 sentences.\n",
      "INFO - 05/30/20 15:59:35 - 0:00:00 - 60291 unknown words (386 unique), covering 15.90% of the data.\n",
      "*** Cleaning and tokenizing MKPAMAN_AMVOE_Ewondo data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/MKPAMAN_AMVOE_Ewondo.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/MKPAMAN_AMVOE_Ewondo.all ...\n",
      "Read 185471 words (7559 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/MKPAMAN_AMVOE_Ewondo.all ...\n",
      "Modified 185471 words from text file.\n",
      "INFO - 05/30/20 15:59:36 - 0:00:00 - Read 5057 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/30/20 15:59:36 - 0:00:00 - 351765 words (5057 unique) in 7944 sentences.\n",
      "INFO - 05/30/20 15:59:36 - 0:00:00 - 62708 unknown words (397 unique), covering 17.83% of the data.\n",
      "*** Cleaning and tokenizing Ngiemboon data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Ngiemboon.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/Ngiemboon.all ...\n",
      "Read 256452 words (12658 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/Ngiemboon.all ...\n",
      "Modified 256452 words from text file.\n",
      "INFO - 05/30/20 15:59:37 - 0:00:00 - Read 5057 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Ngiemboon.pth ...\n",
      "INFO - 05/30/20 15:59:38 - 0:00:01 - 807431 words (5057 unique) in 7929 sentences.\n",
      "INFO - 05/30/20 15:59:38 - 0:00:01 - 325627 unknown words (322 unique), covering 40.33% of the data.\n"
     ]
    }
   ],
   "source": [
    "%env tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum\n",
    "%env CODE_VOCAB_PATH=/home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed\n",
    "! ../build_evaluate_data.sh Bafia $n_samples\n",
    "! ../build_evaluate_data.sh Bulu $n_samples\n",
    "! ../build_evaluate_data.sh MKPAMAN_AMVOE_Ewondo $n_samples\n",
    "! ../build_evaluate_data.sh Ngiemboon $n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ghomala_Ngiemboon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon\n",
      "env: CODE_VOCAB_PATH=/home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed\n",
      "*** Cleaning and tokenizing Bafia data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Bafia.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/Bafia.all ...\n",
      "Read 342491 words (5192 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/Bafia.all ...\n",
      "Modified 342491 words from text file.\n",
      "INFO - 05/30/20 15:59:49 - 0:00:00 - Read 6715 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Bafia.pth ...\n",
      "INFO - 05/30/20 15:59:50 - 0:00:00 - 613668 words (6715 unique) in 7950 sentences.\n",
      "INFO - 05/30/20 15:59:50 - 0:00:00 - 81452 unknown words (70 unique), covering 13.27% of the data.\n",
      "*** Cleaning and tokenizing Bulu data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Bulu.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/Bulu.all ...\n",
      "Read 225215 words (5295 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/Bulu.all ...\n",
      "Modified 225215 words from text file.\n",
      "INFO - 05/30/20 15:59:51 - 0:00:00 - Read 6715 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Bulu.pth ...\n",
      "INFO - 05/30/20 15:59:51 - 0:00:00 - 430245 words (6715 unique) in 7946 sentences.\n",
      "INFO - 05/30/20 15:59:51 - 0:00:00 - 15757 unknown words (86 unique), covering 3.66% of the data.\n",
      "*** Cleaning and tokenizing MKPAMAN_AMVOE_Ewondo data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/MKPAMAN_AMVOE_Ewondo.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/MKPAMAN_AMVOE_Ewondo.all ...\n",
      "Read 185471 words (7559 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/MKPAMAN_AMVOE_Ewondo.all ...\n",
      "Modified 185471 words from text file.\n",
      "INFO - 05/30/20 15:59:52 - 0:00:00 - Read 6715 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/30/20 15:59:52 - 0:00:00 - 385319 words (6715 unique) in 7944 sentences.\n",
      "INFO - 05/30/20 15:59:52 - 0:00:00 - 11543 unknown words (72 unique), covering 3.00% of the data.\n",
      "*** Cleaning and tokenizing Limbum data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Limbum.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/Limbum.all ...\n",
      "Read 321354 words (2890 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/Limbum.all ...\n",
      "Modified 321354 words from text file.\n",
      "INFO - 05/30/20 15:59:53 - 0:00:00 - Read 6715 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Limbum.pth ...\n",
      "INFO - 05/30/20 15:59:53 - 0:00:00 - 578283 words (6715 unique) in 7919 sentences.\n",
      "INFO - 05/30/20 15:59:53 - 0:00:00 - 2848 unknown words (46 unique), covering 0.49% of the data.\n"
     ]
    }
   ],
   "source": [
    "%env tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon\n",
    "%env CODE_VOCAB_PATH=/home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed\n",
    "! ../build_evaluate_data.sh Bafia $n_samples\n",
    "! ../build_evaluate_data.sh Bulu $n_samples\n",
    "! ../build_evaluate_data.sh MKPAMAN_AMVOE_Ewondo $n_samples\n",
    "! ../build_evaluate_data.sh Limbum $n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limbum_Ngiemboon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon\n",
      "env: CODE_VOCAB_PATH=/home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed\n",
      "*** Cleaning and tokenizing Bafia data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Bafia.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/Bafia.all ...\n",
      "Read 342491 words (5192 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/Bafia.all ...\n",
      "Modified 342491 words from text file.\n",
      "INFO - 05/30/20 16:00:02 - 0:00:00 - Read 6667 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Bafia.pth ...\n",
      "INFO - 05/30/20 16:00:02 - 0:00:00 - 625171 words (6667 unique) in 7950 sentences.\n",
      "INFO - 05/30/20 16:00:02 - 0:00:00 - 206249 unknown words (52 unique), covering 32.99% of the data.\n",
      "*** Cleaning and tokenizing Bulu data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Bulu.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/Bulu.all ...\n",
      "Read 225215 words (5295 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/Bulu.all ...\n",
      "Modified 225215 words from text file.\n",
      "INFO - 05/30/20 16:00:03 - 0:00:00 - Read 6667 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Bulu.pth ...\n",
      "INFO - 05/30/20 16:00:04 - 0:00:00 - 400009 words (6667 unique) in 7946 sentences.\n",
      "INFO - 05/30/20 16:00:04 - 0:00:00 - 15365 unknown words (65 unique), covering 3.84% of the data.\n",
      "*** Cleaning and tokenizing MKPAMAN_AMVOE_Ewondo data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/MKPAMAN_AMVOE_Ewondo.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/MKPAMAN_AMVOE_Ewondo.all ...\n",
      "Read 185471 words (7559 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/MKPAMAN_AMVOE_Ewondo.all ...\n",
      "Modified 185471 words from text file.\n",
      "INFO - 05/30/20 16:00:04 - 0:00:00 - Read 6667 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/30/20 16:00:05 - 0:00:00 - 384402 words (6667 unique) in 7944 sentences.\n",
      "INFO - 05/30/20 16:00:05 - 0:00:00 - 3374 unknown words (56 unique), covering 0.88% of the data.\n",
      "*** Cleaning and tokenizing Ghomala data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Ghomala.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/Ghomala.all ...\n",
      "Read 259016 words (2525 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/Ghomala.all ...\n",
      "Modified 259016 words from text file.\n",
      "INFO - 05/30/20 16:00:06 - 0:00:00 - Read 6667 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Ghomala.pth ...\n",
      "INFO - 05/30/20 16:00:06 - 0:00:00 - 500482 words (6667 unique) in 7942 sentences.\n",
      "INFO - 05/30/20 16:00:06 - 0:00:00 - 159492 unknown words (57 unique), covering 31.87% of the data.\n"
     ]
    }
   ],
   "source": [
    "%env tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon\n",
    "%env CODE_VOCAB_PATH=/home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed\n",
    "! ../build_evaluate_data.sh Bafia $n_samples\n",
    "! ../build_evaluate_data.sh Bulu $n_samples\n",
    "! ../build_evaluate_data.sh MKPAMAN_AMVOE_Ewondo $n_samples\n",
    "! ../build_evaluate_data.sh Ghomala $n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: exp_id=maml\n",
      "env: batch_size=32\n",
      "env: max_epoch=100\n",
      "env: stopping_criterion=_valid_mlm_ppl,10\n",
      "env: eval_bleu=False\n",
      "env: remove_long_sentences_train=False\n",
      "env: remove_long_sentences_valid=False\n",
      "env: remove_long_sentences_test=False\n",
      "env: train_n_samples=-1\n",
      "env: valid_n_samples=-1\n",
      "env: test_n_samples=-1\n",
      "env: epoch_size=5000\n"
     ]
    }
   ],
   "source": [
    "%env exp_id=maml\n",
    "%env batch_size=32\n",
    "%env max_epoch=100\n",
    "%env stopping_criterion=_valid_mlm_ppl,10\n",
    "%env eval_bleu=False\n",
    "%env remove_long_sentences_train=False\n",
    "%env remove_long_sentences_valid=False\n",
    "%env remove_long_sentences_test=False\n",
    "%env train_n_samples=-1\n",
    "%env valid_n_samples=-1\n",
    "%env test_n_samples=-1\n",
    "%env epoch_size=5000\n",
    "\n",
    "! chmod +x ../duplicate.sh\n",
    "! chmod +x ../evaluate.sh\n",
    "! chmod +x ../delete.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bafia_Bulu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: dump_path=/home/jupyter/models/africa/cluster1\n",
      "env: exp_name=mlm_tlm_BafiaBulu\n",
      "env: data_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu\n",
      "env: lgs=Bafia-Bulu\n",
      "env: mlm_steps=Bafia,Bulu\n",
      "env: tgt_pair=Bafia-Bulu\n",
      "env: src_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu\n",
      "env: tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu\n",
      "=====================\n",
      "MKPAMAN_AMVOE_Ewondo to Bafia\n",
      "MKPAMAN_AMVOE_Ewondo to Bulu\n",
      "=====================\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/30/20 16:30:51 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/30/20 16:30:51 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_BafiaBulu --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu' --lgs 'Bafia-Bulu' --clm_steps '' --mlm_steps 'Bafia,Bulu' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaBulu/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_BafiaBulu\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Bafia', 1: 'Bulu'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Bafia': 0, 'Bulu': 1}\n",
      "                                     langs: ['Bafia', 'Bulu']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Bafia-Bulu']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Bafia', None), ('Bulu', None)]\n",
      "                                     mono_dataset: {'Bafia': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/train.Bafia.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bafia.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bafia.pth'}, 'Bulu': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/train.Bulu.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bulu.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bulu.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': False, 'valid': False, 'test': False}\n",
      "                                     remove_long_sentences_test: False\n",
      "                                     remove_long_sentences_train: False\n",
      "                                     remove_long_sentences_valid: False\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/30/20 16:30:51 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaBulu/maml\n",
      "                                     \n",
      "INFO - 05/30/20 16:30:51 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_BafiaBulu --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu' --lgs 'Bafia-Bulu' --clm_steps '' --mlm_steps 'Bafia,Bulu' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/30/20 16:30:51 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/30/20 16:30:51 - 0:00:00 - ============ langs: Bafia, Bulu\n",
      "INFO - 05/30/20 16:30:51 - 0:00:00 - ============ Monolingual data (Bafia)\n",
      "INFO - 05/30/20 16:30:51 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bafia.pth ...\n",
      "INFO - 05/30/20 16:30:51 - 0:00:00 - 278935 words (8683 unique) in 7944 sentences. 20549 unknown words (363 unique) covering 7.37% of the data.\n",
      "INFO - 05/30/20 16:30:51 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/30/20 16:30:51 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bafia.pth ...\n",
      "INFO - 05/30/20 16:30:51 - 0:00:00 - 278935 words (8683 unique) in 7944 sentences. 20549 unknown words (363 unique) covering 7.37% of the data.\n",
      "INFO - 05/30/20 16:30:51 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/30/20 16:30:51 - 0:00:00 - ============ Monolingual data (Bulu)\n",
      "INFO - 05/30/20 16:30:51 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bulu.pth ...\n",
      "INFO - 05/30/20 16:30:51 - 0:00:00 - 278935 words (8683 unique) in 7944 sentences. 20549 unknown words (363 unique) covering 7.37% of the data.\n",
      "INFO - 05/30/20 16:30:51 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/30/20 16:30:51 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bulu.pth ...\n",
      "INFO - 05/30/20 16:30:51 - 0:00:00 - 278935 words (8683 unique) in 7944 sentences. 20549 unknown words (363 unique) covering 7.37% of the data.\n",
      "INFO - 05/30/20 16:30:51 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/30/20 16:30:51 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/30/20 16:30:51 - 0:00:00 - Monolingual data   - valid -        Bafia:      7944\n",
      "INFO - 05/30/20 16:30:51 - 0:00:00 - Monolingual data   -  test -        Bafia:      7944\n",
      "INFO - 05/30/20 16:30:51 - 0:00:00 - Monolingual data   - valid -         Bulu:      7944\n",
      "INFO - 05/30/20 16:30:51 - 0:00:00 - Monolingual data   -  test -         Bulu:      7944\n",
      "\n",
      "INFO - 05/30/20 16:30:52 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(8683, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=8683, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/30/20 16:30:52 - 0:00:01 - Number of parameters (model): 85005803\n",
      "INFO - 05/30/20 16:30:58 - 0:00:06 - Found 0 memories.\n",
      "INFO - 05/30/20 16:30:58 - 0:00:06 - Found 6 FFN.\n",
      "INFO - 05/30/20 16:30:58 - 0:00:06 - Found 102 parameters in model.\n",
      "INFO - 05/30/20 16:30:58 - 0:00:06 - Optimizers: model\n",
      "WARNING - 05/30/20 16:30:58 - 0:00:06 - Reloading checkpoint from /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaBulu/maml/checkpoint.pth ...\n",
      "WARNING - 05/30/20 16:30:58 - 0:00:07 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/30/20 16:30:58 - 0:00:07 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/30/20 16:30:58 - 0:00:07 - Checkpoint reloaded. Resuming at epoch 68 / iteration 4556 ...\n",
      "INFO - 05/30/20 16:31:01 - 0:00:09 - epoch -> 68.000000\n",
      "INFO - 05/30/20 16:31:01 - 0:00:09 - valid_Bafia_mlm_ppl -> 17941.734492\n",
      "INFO - 05/30/20 16:31:01 - 0:00:09 - valid_Bafia_mlm_acc -> 9.844560\n",
      "INFO - 05/30/20 16:31:01 - 0:00:09 - valid_Bulu_mlm_ppl -> 5266.841718\n",
      "INFO - 05/30/20 16:31:01 - 0:00:09 - valid_Bulu_mlm_acc -> 15.284974\n",
      "INFO - 05/30/20 16:31:01 - 0:00:09 - valid_mlm_ppl -> 11604.288105\n",
      "INFO - 05/30/20 16:31:01 - 0:00:09 - valid_mlm_acc -> 12.564767\n",
      "INFO - 05/30/20 16:31:01 - 0:00:09 - test_Bafia_mlm_ppl -> 17941.734492\n",
      "INFO - 05/30/20 16:31:01 - 0:00:09 - test_Bafia_mlm_acc -> 9.844560\n",
      "INFO - 05/30/20 16:31:01 - 0:00:09 - test_Bulu_mlm_ppl -> 5266.841718\n",
      "INFO - 05/30/20 16:31:01 - 0:00:09 - test_Bulu_mlm_acc -> 15.284974\n",
      "INFO - 05/30/20 16:31:01 - 0:00:09 - test_mlm_ppl -> 11604.288105\n",
      "INFO - 05/30/20 16:31:01 - 0:00:09 - test_mlm_acc -> 12.564767\n",
      "INFO - 05/30/20 16:31:01 - 0:00:09 - __log__:{\"epoch\": 68, \"valid_Bafia_mlm_ppl\": 17941.73449199054, \"valid_Bafia_mlm_acc\": 9.844559585492227, \"valid_Bulu_mlm_ppl\": 5266.841718033005, \"valid_Bulu_mlm_acc\": 15.284974093264248, \"valid_mlm_ppl\": 11604.288105011772, \"valid_mlm_acc\": 12.564766839378237, \"test_Bafia_mlm_ppl\": 17941.73449199054, \"test_Bafia_mlm_acc\": 9.844559585492227, \"test_Bulu_mlm_ppl\": 5266.841718033005, \"test_Bulu_mlm_acc\": 15.284974093264248, \"test_mlm_ppl\": 11604.288105011772, \"test_mlm_acc\": 12.564766839378237}\n",
      "=====================\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bafia.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bafia.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bulu.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bulu.pth\n",
      "=====================\n",
      "=====================\n",
      "Limbum to Bafia\n",
      "Limbum to Bulu\n",
      "=====================\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/30/20 16:31:02 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/30/20 16:31:02 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_BafiaBulu --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu' --lgs 'Bafia-Bulu' --clm_steps '' --mlm_steps 'Bafia,Bulu' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaBulu/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_BafiaBulu\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Bafia', 1: 'Bulu'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Bafia': 0, 'Bulu': 1}\n",
      "                                     langs: ['Bafia', 'Bulu']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Bafia-Bulu']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Bafia', None), ('Bulu', None)]\n",
      "                                     mono_dataset: {'Bafia': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/train.Bafia.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bafia.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bafia.pth'}, 'Bulu': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/train.Bulu.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bulu.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bulu.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': False, 'valid': False, 'test': False}\n",
      "                                     remove_long_sentences_test: False\n",
      "                                     remove_long_sentences_train: False\n",
      "                                     remove_long_sentences_valid: False\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/30/20 16:31:02 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaBulu/maml\n",
      "                                     \n",
      "INFO - 05/30/20 16:31:02 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_BafiaBulu --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu' --lgs 'Bafia-Bulu' --clm_steps '' --mlm_steps 'Bafia,Bulu' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/30/20 16:31:02 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/30/20 16:31:02 - 0:00:00 - ============ langs: Bafia, Bulu\n",
      "INFO - 05/30/20 16:31:02 - 0:00:00 - ============ Monolingual data (Bafia)\n",
      "INFO - 05/30/20 16:31:02 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bafia.pth ...\n",
      "INFO - 05/30/20 16:31:02 - 0:00:00 - 488768 words (8683 unique) in 7919 sentences. 28300 unknown words (173 unique) covering 5.79% of the data.\n",
      "INFO - 05/30/20 16:31:02 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/30/20 16:31:02 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bafia.pth ...\n",
      "INFO - 05/30/20 16:31:02 - 0:00:00 - 488768 words (8683 unique) in 7919 sentences. 28300 unknown words (173 unique) covering 5.79% of the data.\n",
      "INFO - 05/30/20 16:31:02 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/30/20 16:31:02 - 0:00:00 - ============ Monolingual data (Bulu)\n",
      "INFO - 05/30/20 16:31:02 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bulu.pth ...\n",
      "INFO - 05/30/20 16:31:02 - 0:00:00 - 488768 words (8683 unique) in 7919 sentences. 28300 unknown words (173 unique) covering 5.79% of the data.\n",
      "INFO - 05/30/20 16:31:02 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/30/20 16:31:02 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bulu.pth ...\n",
      "INFO - 05/30/20 16:31:02 - 0:00:00 - 488768 words (8683 unique) in 7919 sentences. 28300 unknown words (173 unique) covering 5.79% of the data.\n",
      "INFO - 05/30/20 16:31:02 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/30/20 16:31:02 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/30/20 16:31:02 - 0:00:00 - Monolingual data   - valid -        Bafia:      7919\n",
      "INFO - 05/30/20 16:31:02 - 0:00:00 - Monolingual data   -  test -        Bafia:      7919\n",
      "INFO - 05/30/20 16:31:02 - 0:00:00 - Monolingual data   - valid -         Bulu:      7919\n",
      "INFO - 05/30/20 16:31:02 - 0:00:00 - Monolingual data   -  test -         Bulu:      7919\n",
      "\n",
      "INFO - 05/30/20 16:31:03 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(8683, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=8683, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/30/20 16:31:03 - 0:00:01 - Number of parameters (model): 85005803\n",
      "INFO - 05/30/20 16:31:09 - 0:00:07 - Found 0 memories.\n",
      "INFO - 05/30/20 16:31:09 - 0:00:07 - Found 6 FFN.\n",
      "INFO - 05/30/20 16:31:09 - 0:00:07 - Found 102 parameters in model.\n",
      "INFO - 05/30/20 16:31:09 - 0:00:07 - Optimizers: model\n",
      "WARNING - 05/30/20 16:31:09 - 0:00:07 - Reloading checkpoint from /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaBulu/maml/checkpoint.pth ...\n",
      "WARNING - 05/30/20 16:31:10 - 0:00:08 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/30/20 16:31:10 - 0:00:08 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/30/20 16:31:10 - 0:00:08 - Checkpoint reloaded. Resuming at epoch 68 / iteration 4556 ...\n",
      "INFO - 05/30/20 16:31:12 - 0:00:10 - epoch -> 68.000000\n",
      "INFO - 05/30/20 16:31:12 - 0:00:10 - valid_Bafia_mlm_ppl -> 9207.021610\n",
      "INFO - 05/30/20 16:31:12 - 0:00:10 - valid_Bafia_mlm_acc -> 11.917098\n",
      "INFO - 05/30/20 16:31:12 - 0:00:10 - valid_Bulu_mlm_ppl -> 5108.940717\n",
      "INFO - 05/30/20 16:31:12 - 0:00:10 - valid_Bulu_mlm_acc -> 12.694301\n",
      "INFO - 05/30/20 16:31:12 - 0:00:10 - valid_mlm_ppl -> 7157.981164\n",
      "INFO - 05/30/20 16:31:12 - 0:00:10 - valid_mlm_acc -> 12.305699\n",
      "INFO - 05/30/20 16:31:12 - 0:00:10 - test_Bafia_mlm_ppl -> 9207.021610\n",
      "INFO - 05/30/20 16:31:12 - 0:00:10 - test_Bafia_mlm_acc -> 11.917098\n",
      "INFO - 05/30/20 16:31:12 - 0:00:10 - test_Bulu_mlm_ppl -> 5108.940717\n",
      "INFO - 05/30/20 16:31:12 - 0:00:10 - test_Bulu_mlm_acc -> 12.694301\n",
      "INFO - 05/30/20 16:31:12 - 0:00:10 - test_mlm_ppl -> 7157.981164\n",
      "INFO - 05/30/20 16:31:12 - 0:00:10 - test_mlm_acc -> 12.305699\n",
      "INFO - 05/30/20 16:31:12 - 0:00:10 - __log__:{\"epoch\": 68, \"valid_Bafia_mlm_ppl\": 9207.021610367206, \"valid_Bafia_mlm_acc\": 11.917098445595855, \"valid_Bulu_mlm_ppl\": 5108.940716937399, \"valid_Bulu_mlm_acc\": 12.694300518134716, \"valid_mlm_ppl\": 7157.981163652303, \"valid_mlm_acc\": 12.305699481865286, \"test_Bafia_mlm_ppl\": 9207.021610367206, \"test_Bafia_mlm_acc\": 11.917098445595855, \"test_Bulu_mlm_ppl\": 5108.940716937399, \"test_Bulu_mlm_acc\": 12.694300518134716, \"test_mlm_ppl\": 7157.981163652303, \"test_mlm_acc\": 12.305699481865286}\n",
      "=====================\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bafia.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bafia.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bulu.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bulu.pth\n",
      "=====================\n",
      "=====================\n",
      "Limbum to Bafia\n",
      "Limbum to Bulu\n",
      "=====================\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/30/20 16:31:13 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/30/20 16:31:13 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_BafiaBulu --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu' --lgs 'Bafia-Bulu' --clm_steps '' --mlm_steps 'Bafia,Bulu' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaBulu/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_BafiaBulu\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Bafia', 1: 'Bulu'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Bafia': 0, 'Bulu': 1}\n",
      "                                     langs: ['Bafia', 'Bulu']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Bafia-Bulu']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Bafia', None), ('Bulu', None)]\n",
      "                                     mono_dataset: {'Bafia': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/train.Bafia.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bafia.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bafia.pth'}, 'Bulu': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/train.Bulu.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bulu.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bulu.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': False, 'valid': False, 'test': False}\n",
      "                                     remove_long_sentences_test: False\n",
      "                                     remove_long_sentences_train: False\n",
      "                                     remove_long_sentences_valid: False\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/30/20 16:31:13 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaBulu/maml\n",
      "                                     \n",
      "INFO - 05/30/20 16:31:13 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_BafiaBulu --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu' --lgs 'Bafia-Bulu' --clm_steps '' --mlm_steps 'Bafia,Bulu' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/30/20 16:31:13 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/30/20 16:31:13 - 0:00:00 - ============ langs: Bafia, Bulu\n",
      "INFO - 05/30/20 16:31:13 - 0:00:00 - ============ Monolingual data (Bafia)\n",
      "INFO - 05/30/20 16:31:13 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bafia.pth ...\n",
      "INFO - 05/30/20 16:31:13 - 0:00:00 - 488768 words (8683 unique) in 7919 sentences. 28300 unknown words (173 unique) covering 5.79% of the data.\n",
      "INFO - 05/30/20 16:31:13 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/30/20 16:31:13 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bafia.pth ...\n",
      "INFO - 05/30/20 16:31:13 - 0:00:00 - 488768 words (8683 unique) in 7919 sentences. 28300 unknown words (173 unique) covering 5.79% of the data.\n",
      "INFO - 05/30/20 16:31:13 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/30/20 16:31:13 - 0:00:00 - ============ Monolingual data (Bulu)\n",
      "INFO - 05/30/20 16:31:13 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bulu.pth ...\n",
      "INFO - 05/30/20 16:31:13 - 0:00:00 - 488768 words (8683 unique) in 7919 sentences. 28300 unknown words (173 unique) covering 5.79% of the data.\n",
      "INFO - 05/30/20 16:31:13 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/30/20 16:31:13 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bulu.pth ...\n",
      "INFO - 05/30/20 16:31:13 - 0:00:00 - 488768 words (8683 unique) in 7919 sentences. 28300 unknown words (173 unique) covering 5.79% of the data.\n",
      "INFO - 05/30/20 16:31:13 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/30/20 16:31:13 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/30/20 16:31:13 - 0:00:00 - Monolingual data   - valid -        Bafia:      7919\n",
      "INFO - 05/30/20 16:31:13 - 0:00:00 - Monolingual data   -  test -        Bafia:      7919\n",
      "INFO - 05/30/20 16:31:13 - 0:00:00 - Monolingual data   - valid -         Bulu:      7919\n",
      "INFO - 05/30/20 16:31:13 - 0:00:00 - Monolingual data   -  test -         Bulu:      7919\n",
      "\n",
      "INFO - 05/30/20 16:31:14 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(8683, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=8683, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/30/20 16:31:14 - 0:00:01 - Number of parameters (model): 85005803\n",
      "INFO - 05/30/20 16:31:20 - 0:00:07 - Found 0 memories.\n",
      "INFO - 05/30/20 16:31:20 - 0:00:07 - Found 6 FFN.\n",
      "INFO - 05/30/20 16:31:20 - 0:00:07 - Found 102 parameters in model.\n",
      "INFO - 05/30/20 16:31:20 - 0:00:07 - Optimizers: model\n",
      "WARNING - 05/30/20 16:31:20 - 0:00:07 - Reloading checkpoint from /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaBulu/maml/checkpoint.pth ...\n",
      "WARNING - 05/30/20 16:31:21 - 0:00:07 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/30/20 16:31:21 - 0:00:07 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/30/20 16:31:21 - 0:00:07 - Checkpoint reloaded. Resuming at epoch 68 / iteration 4556 ...\n",
      "INFO - 05/30/20 16:31:24 - 0:00:10 - epoch -> 68.000000\n",
      "INFO - 05/30/20 16:31:24 - 0:00:10 - valid_Bafia_mlm_ppl -> 9207.021610\n",
      "INFO - 05/30/20 16:31:24 - 0:00:10 - valid_Bafia_mlm_acc -> 11.917098\n",
      "INFO - 05/30/20 16:31:24 - 0:00:10 - valid_Bulu_mlm_ppl -> 5108.940717\n",
      "INFO - 05/30/20 16:31:24 - 0:00:10 - valid_Bulu_mlm_acc -> 12.694301\n",
      "INFO - 05/30/20 16:31:24 - 0:00:10 - valid_mlm_ppl -> 7157.981164\n",
      "INFO - 05/30/20 16:31:24 - 0:00:10 - valid_mlm_acc -> 12.305699\n",
      "INFO - 05/30/20 16:31:24 - 0:00:10 - test_Bafia_mlm_ppl -> 9207.021610\n",
      "INFO - 05/30/20 16:31:24 - 0:00:10 - test_Bafia_mlm_acc -> 11.917098\n",
      "INFO - 05/30/20 16:31:24 - 0:00:10 - test_Bulu_mlm_ppl -> 5108.940717\n",
      "INFO - 05/30/20 16:31:24 - 0:00:10 - test_Bulu_mlm_acc -> 12.694301\n",
      "INFO - 05/30/20 16:31:24 - 0:00:10 - test_mlm_ppl -> 7157.981164\n",
      "INFO - 05/30/20 16:31:24 - 0:00:10 - test_mlm_acc -> 12.305699\n",
      "INFO - 05/30/20 16:31:24 - 0:00:10 - __log__:{\"epoch\": 68, \"valid_Bafia_mlm_ppl\": 9207.021610367206, \"valid_Bafia_mlm_acc\": 11.917098445595855, \"valid_Bulu_mlm_ppl\": 5108.940716937399, \"valid_Bulu_mlm_acc\": 12.694300518134716, \"valid_mlm_ppl\": 7157.981163652303, \"valid_mlm_acc\": 12.305699481865286, \"test_Bafia_mlm_ppl\": 9207.021610367206, \"test_Bafia_mlm_acc\": 11.917098445595855, \"test_Bulu_mlm_ppl\": 5108.940716937399, \"test_Bulu_mlm_acc\": 12.694300518134716, \"test_mlm_ppl\": 7157.981163652303, \"test_mlm_acc\": 12.305699481865286}\n",
      "=====================\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bafia.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bafia.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bulu.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bulu.pth\n",
      "=====================\n",
      "=====================\n",
      "Limbum to Bafia\n",
      "Limbum to Bulu\n",
      "=====================\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/30/20 16:31:25 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/30/20 16:31:25 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_BafiaBulu --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu' --lgs 'Bafia-Bulu' --clm_steps '' --mlm_steps 'Bafia,Bulu' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaBulu/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_BafiaBulu\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Bafia', 1: 'Bulu'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Bafia': 0, 'Bulu': 1}\n",
      "                                     langs: ['Bafia', 'Bulu']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Bafia-Bulu']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Bafia', None), ('Bulu', None)]\n",
      "                                     mono_dataset: {'Bafia': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/train.Bafia.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bafia.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bafia.pth'}, 'Bulu': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/train.Bulu.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bulu.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bulu.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': False, 'valid': False, 'test': False}\n",
      "                                     remove_long_sentences_test: False\n",
      "                                     remove_long_sentences_train: False\n",
      "                                     remove_long_sentences_valid: False\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/30/20 16:31:25 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaBulu/maml\n",
      "                                     \n",
      "INFO - 05/30/20 16:31:25 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_BafiaBulu --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu' --lgs 'Bafia-Bulu' --clm_steps '' --mlm_steps 'Bafia,Bulu' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/30/20 16:31:25 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/30/20 16:31:25 - 0:00:00 - ============ langs: Bafia, Bulu\n",
      "INFO - 05/30/20 16:31:25 - 0:00:00 - ============ Monolingual data (Bafia)\n",
      "INFO - 05/30/20 16:31:25 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bafia.pth ...\n",
      "INFO - 05/30/20 16:31:25 - 0:00:00 - 488768 words (8683 unique) in 7919 sentences. 28300 unknown words (173 unique) covering 5.79% of the data.\n",
      "INFO - 05/30/20 16:31:25 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/30/20 16:31:25 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bafia.pth ...\n",
      "INFO - 05/30/20 16:31:25 - 0:00:00 - 488768 words (8683 unique) in 7919 sentences. 28300 unknown words (173 unique) covering 5.79% of the data.\n",
      "INFO - 05/30/20 16:31:25 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/30/20 16:31:25 - 0:00:00 - ============ Monolingual data (Bulu)\n",
      "INFO - 05/30/20 16:31:25 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bulu.pth ...\n",
      "INFO - 05/30/20 16:31:25 - 0:00:00 - 488768 words (8683 unique) in 7919 sentences. 28300 unknown words (173 unique) covering 5.79% of the data.\n",
      "INFO - 05/30/20 16:31:25 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/30/20 16:31:25 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bulu.pth ...\n",
      "INFO - 05/30/20 16:31:25 - 0:00:00 - 488768 words (8683 unique) in 7919 sentences. 28300 unknown words (173 unique) covering 5.79% of the data.\n",
      "INFO - 05/30/20 16:31:25 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/30/20 16:31:25 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/30/20 16:31:25 - 0:00:00 - Monolingual data   - valid -        Bafia:      7919\n",
      "INFO - 05/30/20 16:31:25 - 0:00:00 - Monolingual data   -  test -        Bafia:      7919\n",
      "INFO - 05/30/20 16:31:25 - 0:00:00 - Monolingual data   - valid -         Bulu:      7919\n",
      "INFO - 05/30/20 16:31:25 - 0:00:00 - Monolingual data   -  test -         Bulu:      7919\n",
      "\n",
      "INFO - 05/30/20 16:31:26 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(8683, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=8683, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/30/20 16:31:26 - 0:00:01 - Number of parameters (model): 85005803\n",
      "INFO - 05/30/20 16:31:31 - 0:00:07 - Found 0 memories.\n",
      "INFO - 05/30/20 16:31:31 - 0:00:07 - Found 6 FFN.\n",
      "INFO - 05/30/20 16:31:31 - 0:00:07 - Found 102 parameters in model.\n",
      "INFO - 05/30/20 16:31:32 - 0:00:07 - Optimizers: model\n",
      "WARNING - 05/30/20 16:31:32 - 0:00:07 - Reloading checkpoint from /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaBulu/maml/checkpoint.pth ...\n",
      "WARNING - 05/30/20 16:31:32 - 0:00:07 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/30/20 16:31:32 - 0:00:07 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/30/20 16:31:32 - 0:00:07 - Checkpoint reloaded. Resuming at epoch 68 / iteration 4556 ...\n",
      "INFO - 05/30/20 16:31:34 - 0:00:09 - epoch -> 68.000000\n",
      "INFO - 05/30/20 16:31:34 - 0:00:09 - valid_Bafia_mlm_ppl -> 9207.021610\n",
      "INFO - 05/30/20 16:31:34 - 0:00:09 - valid_Bafia_mlm_acc -> 11.917098\n",
      "INFO - 05/30/20 16:31:34 - 0:00:09 - valid_Bulu_mlm_ppl -> 5108.940717\n",
      "INFO - 05/30/20 16:31:34 - 0:00:09 - valid_Bulu_mlm_acc -> 12.694301\n",
      "INFO - 05/30/20 16:31:34 - 0:00:09 - valid_mlm_ppl -> 7157.981164\n",
      "INFO - 05/30/20 16:31:34 - 0:00:09 - valid_mlm_acc -> 12.305699\n",
      "INFO - 05/30/20 16:31:34 - 0:00:09 - test_Bafia_mlm_ppl -> 9207.021610\n",
      "INFO - 05/30/20 16:31:34 - 0:00:09 - test_Bafia_mlm_acc -> 11.917098\n",
      "INFO - 05/30/20 16:31:34 - 0:00:09 - test_Bulu_mlm_ppl -> 5108.940717\n",
      "INFO - 05/30/20 16:31:34 - 0:00:09 - test_Bulu_mlm_acc -> 12.694301\n",
      "INFO - 05/30/20 16:31:34 - 0:00:09 - test_mlm_ppl -> 7157.981164\n",
      "INFO - 05/30/20 16:31:34 - 0:00:09 - test_mlm_acc -> 12.305699\n",
      "INFO - 05/30/20 16:31:34 - 0:00:09 - __log__:{\"epoch\": 68, \"valid_Bafia_mlm_ppl\": 9207.021610367206, \"valid_Bafia_mlm_acc\": 11.917098445595855, \"valid_Bulu_mlm_ppl\": 5108.940716937399, \"valid_Bulu_mlm_acc\": 12.694300518134716, \"valid_mlm_ppl\": 7157.981163652303, \"valid_mlm_acc\": 12.305699481865286, \"test_Bafia_mlm_ppl\": 9207.021610367206, \"test_Bafia_mlm_acc\": 11.917098445595855, \"test_Bulu_mlm_ppl\": 5108.940716937399, \"test_Bulu_mlm_acc\": 12.694300518134716, \"test_mlm_ppl\": 7157.981163652303, \"test_mlm_acc\": 12.305699481865286}\n",
      "=====================\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bafia.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bafia.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bulu.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bulu.pth\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "%env dump_path=/home/jupyter/models/africa/cluster1\n",
    "%env exp_name=mlm_tlm_BafiaBulu\n",
    "%env data_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu\n",
    "%env lgs=Bafia-Bulu\n",
    "%env mlm_steps=Bafia,Bulu\n",
    "%env tgt_pair=Bafia-Bulu\n",
    "%env src_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu\n",
    "%env tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu\n",
    "\n",
    "# creation of the dummy files so that the experiment does not bug\n",
    "! touch /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/train.Bafia.pth\n",
    "! touch /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/train.Bulu.pth\n",
    "\n",
    "###### Bafia_Bulu vs Ewondo\n",
    "! ../duplicate.sh $src_path $tgt_path MKPAMAN_AMVOE_Ewondo $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : delete the rename file\n",
    "! ../delete.sh $tgt_path $tgt_pair\n",
    "\n",
    "###### Bafia_Bulu vs Limbum\n",
    "! ../duplicate.sh $src_path $tgt_path Limbum $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : delete the rename file\n",
    "! ../delete.sh $tgt_path $tgt_pair\n",
    "\n",
    "###### Bafia_Bulu vs Ghomala\n",
    "! ../duplicate.sh $src_path $tgt_path Limbum $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : delete the rename file\n",
    "! ../delete.sh $tgt_path $tgt_pair\n",
    "\n",
    "###### Bafia_Bulu vs Ngiemboon\n",
    "! ../duplicate.sh $src_path $tgt_path Limbum $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : delete the rename file\n",
    "! ../delete.sh $tgt_path $tgt_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bafia_Ewondo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: dump_path=/home/jupyter/models/africa/cluster1\n",
      "env: exp_name=mlm_tlm_BafiaEwondo\n",
      "env: data_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo\n",
      "env: lgs=Bafia-MKPAMAN_AMVOE_Ewondo\n",
      "env: mlm_steps=Bafia,MKPAMAN_AMVOE_Ewondo\n",
      "env: tgt_pair=Bafia-MKPAMAN_AMVOE_Ewondo\n",
      "env: src_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo\n",
      "env: tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo\n",
      "=====================\n",
      "Bulu to Bafia\n",
      "Bulu to MKPAMAN_AMVOE_Ewondo\n",
      "=====================\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/30/20 16:32:41 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/30/20 16:32:41 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_BafiaEwondo --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo' --lgs 'Bafia-MKPAMAN_AMVOE_Ewondo' --clm_steps '' --mlm_steps 'Bafia,MKPAMAN_AMVOE_Ewondo' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaEwondo/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_BafiaEwondo\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Bafia', 1: 'MKPAMAN_AMVOE_Ewondo'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Bafia': 0, 'MKPAMAN_AMVOE_Ewondo': 1}\n",
      "                                     langs: ['Bafia', 'MKPAMAN_AMVOE_Ewondo']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Bafia-MKPAMAN_AMVOE_Ewondo']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Bafia', None), ('MKPAMAN_AMVOE_Ewondo', None)]\n",
      "                                     mono_dataset: {'Bafia': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/train.Bafia.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.Bafia.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.Bafia.pth'}, 'MKPAMAN_AMVOE_Ewondo': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/train.MKPAMAN_AMVOE_Ewondo.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': False, 'valid': False, 'test': False}\n",
      "                                     remove_long_sentences_test: False\n",
      "                                     remove_long_sentences_train: False\n",
      "                                     remove_long_sentences_valid: False\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/30/20 16:32:41 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaEwondo/maml\n",
      "                                     \n",
      "INFO - 05/30/20 16:32:41 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_BafiaEwondo --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo' --lgs 'Bafia-MKPAMAN_AMVOE_Ewondo' --clm_steps '' --mlm_steps 'Bafia,MKPAMAN_AMVOE_Ewondo' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/30/20 16:32:41 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/30/20 16:32:41 - 0:00:00 - ============ langs: Bafia, MKPAMAN_AMVOE_Ewondo\n",
      "INFO - 05/30/20 16:32:41 - 0:00:00 - ============ Monolingual data (Bafia)\n",
      "INFO - 05/30/20 16:32:41 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.Bafia.pth ...\n",
      "INFO - 05/30/20 16:32:41 - 0:00:00 - 322451 words (9128 unique) in 7946 sentences. 19871 unknown words (220 unique) covering 6.16% of the data.\n",
      "INFO - 05/30/20 16:32:41 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/30/20 16:32:41 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.Bafia.pth ...\n",
      "INFO - 05/30/20 16:32:41 - 0:00:00 - 322451 words (9128 unique) in 7946 sentences. 19871 unknown words (220 unique) covering 6.16% of the data.\n",
      "INFO - 05/30/20 16:32:41 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/30/20 16:32:41 - 0:00:00 - ============ Monolingual data (MKPAMAN_AMVOE_Ewondo)\n",
      "INFO - 05/30/20 16:32:41 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/30/20 16:32:41 - 0:00:00 - 322451 words (9128 unique) in 7946 sentences. 19871 unknown words (220 unique) covering 6.16% of the data.\n",
      "INFO - 05/30/20 16:32:41 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/30/20 16:32:41 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/30/20 16:32:41 - 0:00:00 - 322451 words (9128 unique) in 7946 sentences. 19871 unknown words (220 unique) covering 6.16% of the data.\n",
      "INFO - 05/30/20 16:32:41 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/30/20 16:32:41 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/30/20 16:32:41 - 0:00:00 - Monolingual data   - valid -        Bafia:      7946\n",
      "INFO - 05/30/20 16:32:41 - 0:00:00 - Monolingual data   -  test -        Bafia:      7946\n",
      "INFO - 05/30/20 16:32:41 - 0:00:00 - Monolingual data   - valid - MKPAMAN_AMVOE_Ewondo:      7946\n",
      "INFO - 05/30/20 16:32:41 - 0:00:00 - Monolingual data   -  test - MKPAMAN_AMVOE_Ewondo:      7946\n",
      "\n",
      "INFO - 05/30/20 16:32:42 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(9128, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=9128, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/30/20 16:32:42 - 0:00:01 - Number of parameters (model): 85461928\n",
      "INFO - 05/30/20 16:32:48 - 0:00:07 - Found 0 memories.\n",
      "INFO - 05/30/20 16:32:48 - 0:00:07 - Found 6 FFN.\n",
      "INFO - 05/30/20 16:32:48 - 0:00:07 - Found 102 parameters in model.\n",
      "INFO - 05/30/20 16:32:48 - 0:00:07 - Optimizers: model\n",
      "WARNING - 05/30/20 16:32:48 - 0:00:07 - Reloading checkpoint from /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaEwondo/maml/checkpoint.pth ...\n",
      "WARNING - 05/30/20 16:32:48 - 0:00:07 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/30/20 16:32:48 - 0:00:07 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/30/20 16:32:48 - 0:00:07 - Checkpoint reloaded. Resuming at epoch 40 / iteration 5320 ...\n",
      "INFO - 05/30/20 16:32:51 - 0:00:10 - epoch -> 40.000000\n",
      "INFO - 05/30/20 16:32:51 - 0:00:10 - valid_Bafia_mlm_ppl -> 8403.607319\n",
      "INFO - 05/30/20 16:32:51 - 0:00:10 - valid_Bafia_mlm_acc -> 9.844560\n",
      "INFO - 05/30/20 16:32:51 - 0:00:10 - valid_MKPAMAN_AMVOE_Ewondo_mlm_ppl -> 5978.399024\n",
      "INFO - 05/30/20 16:32:51 - 0:00:10 - valid_MKPAMAN_AMVOE_Ewondo_mlm_acc -> 11.398964\n",
      "INFO - 05/30/20 16:32:51 - 0:00:10 - valid_mlm_ppl -> 7191.003172\n",
      "INFO - 05/30/20 16:32:51 - 0:00:10 - valid_mlm_acc -> 10.621762\n",
      "INFO - 05/30/20 16:32:51 - 0:00:10 - test_Bafia_mlm_ppl -> 8403.607319\n",
      "INFO - 05/30/20 16:32:51 - 0:00:10 - test_Bafia_mlm_acc -> 9.844560\n",
      "INFO - 05/30/20 16:32:51 - 0:00:10 - test_MKPAMAN_AMVOE_Ewondo_mlm_ppl -> 5978.399024\n",
      "INFO - 05/30/20 16:32:51 - 0:00:10 - test_MKPAMAN_AMVOE_Ewondo_mlm_acc -> 11.398964\n",
      "INFO - 05/30/20 16:32:51 - 0:00:10 - test_mlm_ppl -> 7191.003172\n",
      "INFO - 05/30/20 16:32:51 - 0:00:10 - test_mlm_acc -> 10.621762\n",
      "INFO - 05/30/20 16:32:51 - 0:00:10 - __log__:{\"epoch\": 40, \"valid_Bafia_mlm_ppl\": 8403.607319350618, \"valid_Bafia_mlm_acc\": 9.844559585492227, \"valid_MKPAMAN_AMVOE_Ewondo_mlm_ppl\": 5978.3990240847825, \"valid_MKPAMAN_AMVOE_Ewondo_mlm_acc\": 11.398963730569948, \"valid_mlm_ppl\": 7191.003171717701, \"valid_mlm_acc\": 10.621761658031087, \"test_Bafia_mlm_ppl\": 8403.607319350618, \"test_Bafia_mlm_acc\": 9.844559585492227, \"test_MKPAMAN_AMVOE_Ewondo_mlm_ppl\": 5978.3990240847825, \"test_MKPAMAN_AMVOE_Ewondo_mlm_acc\": 11.398963730569948, \"test_mlm_ppl\": 7191.003171717701, \"test_mlm_acc\": 10.621761658031087}\n",
      "=====================\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.Bafia.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.Bafia.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth\n",
      "=====================\n",
      "=====================\n",
      "Limbum to Bafia\n",
      "Limbum to MKPAMAN_AMVOE_Ewondo\n",
      "=====================\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/30/20 16:32:52 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/30/20 16:32:52 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_BafiaEwondo --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo' --lgs 'Bafia-MKPAMAN_AMVOE_Ewondo' --clm_steps '' --mlm_steps 'Bafia,MKPAMAN_AMVOE_Ewondo' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaEwondo/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_BafiaEwondo\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Bafia', 1: 'MKPAMAN_AMVOE_Ewondo'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Bafia': 0, 'MKPAMAN_AMVOE_Ewondo': 1}\n",
      "                                     langs: ['Bafia', 'MKPAMAN_AMVOE_Ewondo']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Bafia-MKPAMAN_AMVOE_Ewondo']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Bafia', None), ('MKPAMAN_AMVOE_Ewondo', None)]\n",
      "                                     mono_dataset: {'Bafia': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/train.Bafia.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.Bafia.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.Bafia.pth'}, 'MKPAMAN_AMVOE_Ewondo': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/train.MKPAMAN_AMVOE_Ewondo.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': False, 'valid': False, 'test': False}\n",
      "                                     remove_long_sentences_test: False\n",
      "                                     remove_long_sentences_train: False\n",
      "                                     remove_long_sentences_valid: False\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/30/20 16:32:52 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaEwondo/maml\n",
      "                                     \n",
      "INFO - 05/30/20 16:32:52 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_BafiaEwondo --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo' --lgs 'Bafia-MKPAMAN_AMVOE_Ewondo' --clm_steps '' --mlm_steps 'Bafia,MKPAMAN_AMVOE_Ewondo' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/30/20 16:32:52 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/30/20 16:32:52 - 0:00:00 - ============ langs: Bafia, MKPAMAN_AMVOE_Ewondo\n",
      "INFO - 05/30/20 16:32:52 - 0:00:00 - ============ Monolingual data (Bafia)\n",
      "INFO - 05/30/20 16:32:52 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.Bafia.pth ...\n",
      "INFO - 05/30/20 16:32:52 - 0:00:00 - 489888 words (9128 unique) in 7919 sentences. 12558 unknown words (116 unique) covering 2.56% of the data.\n",
      "INFO - 05/30/20 16:32:52 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/30/20 16:32:52 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.Bafia.pth ...\n",
      "INFO - 05/30/20 16:32:52 - 0:00:00 - 489888 words (9128 unique) in 7919 sentences. 12558 unknown words (116 unique) covering 2.56% of the data.\n",
      "INFO - 05/30/20 16:32:52 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/30/20 16:32:52 - 0:00:00 - ============ Monolingual data (MKPAMAN_AMVOE_Ewondo)\n",
      "INFO - 05/30/20 16:32:52 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/30/20 16:32:52 - 0:00:00 - 489888 words (9128 unique) in 7919 sentences. 12558 unknown words (116 unique) covering 2.56% of the data.\n",
      "INFO - 05/30/20 16:32:52 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/30/20 16:32:52 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/30/20 16:32:52 - 0:00:00 - 489888 words (9128 unique) in 7919 sentences. 12558 unknown words (116 unique) covering 2.56% of the data.\n",
      "INFO - 05/30/20 16:32:52 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/30/20 16:32:52 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/30/20 16:32:52 - 0:00:00 - Monolingual data   - valid -        Bafia:      7919\n",
      "INFO - 05/30/20 16:32:52 - 0:00:00 - Monolingual data   -  test -        Bafia:      7919\n",
      "INFO - 05/30/20 16:32:52 - 0:00:00 - Monolingual data   - valid - MKPAMAN_AMVOE_Ewondo:      7919\n",
      "INFO - 05/30/20 16:32:52 - 0:00:00 - Monolingual data   -  test - MKPAMAN_AMVOE_Ewondo:      7919\n",
      "\n",
      "INFO - 05/30/20 16:32:53 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(9128, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=9128, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/30/20 16:32:53 - 0:00:01 - Number of parameters (model): 85461928\n",
      "INFO - 05/30/20 16:32:59 - 0:00:07 - Found 0 memories.\n",
      "INFO - 05/30/20 16:32:59 - 0:00:07 - Found 6 FFN.\n",
      "INFO - 05/30/20 16:32:59 - 0:00:07 - Found 102 parameters in model.\n",
      "INFO - 05/30/20 16:32:59 - 0:00:07 - Optimizers: model\n",
      "WARNING - 05/30/20 16:32:59 - 0:00:07 - Reloading checkpoint from /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaEwondo/maml/checkpoint.pth ...\n",
      "WARNING - 05/30/20 16:33:00 - 0:00:08 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/30/20 16:33:00 - 0:00:08 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/30/20 16:33:00 - 0:00:08 - Checkpoint reloaded. Resuming at epoch 40 / iteration 5320 ...\n",
      "INFO - 05/30/20 16:33:02 - 0:00:10 - epoch -> 40.000000\n",
      "INFO - 05/30/20 16:33:02 - 0:00:10 - valid_Bafia_mlm_ppl -> 7010.615781\n",
      "INFO - 05/30/20 16:33:02 - 0:00:10 - valid_Bafia_mlm_acc -> 9.067358\n",
      "INFO - 05/30/20 16:33:02 - 0:00:10 - valid_MKPAMAN_AMVOE_Ewondo_mlm_ppl -> 5565.363713\n",
      "INFO - 05/30/20 16:33:02 - 0:00:10 - valid_MKPAMAN_AMVOE_Ewondo_mlm_acc -> 8.808290\n",
      "INFO - 05/30/20 16:33:02 - 0:00:10 - valid_mlm_ppl -> 6287.989747\n",
      "INFO - 05/30/20 16:33:02 - 0:00:10 - valid_mlm_acc -> 8.937824\n",
      "INFO - 05/30/20 16:33:02 - 0:00:10 - test_Bafia_mlm_ppl -> 7010.615781\n",
      "INFO - 05/30/20 16:33:02 - 0:00:10 - test_Bafia_mlm_acc -> 9.067358\n",
      "INFO - 05/30/20 16:33:02 - 0:00:10 - test_MKPAMAN_AMVOE_Ewondo_mlm_ppl -> 5565.363713\n",
      "INFO - 05/30/20 16:33:02 - 0:00:10 - test_MKPAMAN_AMVOE_Ewondo_mlm_acc -> 8.808290\n",
      "INFO - 05/30/20 16:33:02 - 0:00:10 - test_mlm_ppl -> 6287.989747\n",
      "INFO - 05/30/20 16:33:02 - 0:00:10 - test_mlm_acc -> 8.937824\n",
      "INFO - 05/30/20 16:33:02 - 0:00:10 - __log__:{\"epoch\": 40, \"valid_Bafia_mlm_ppl\": 7010.615781160661, \"valid_Bafia_mlm_acc\": 9.067357512953368, \"valid_MKPAMAN_AMVOE_Ewondo_mlm_ppl\": 5565.363713237672, \"valid_MKPAMAN_AMVOE_Ewondo_mlm_acc\": 8.808290155440414, \"valid_mlm_ppl\": 6287.989747199166, \"valid_mlm_acc\": 8.937823834196891, \"test_Bafia_mlm_ppl\": 7010.615781160661, \"test_Bafia_mlm_acc\": 9.067357512953368, \"test_MKPAMAN_AMVOE_Ewondo_mlm_ppl\": 5565.363713237672, \"test_MKPAMAN_AMVOE_Ewondo_mlm_acc\": 8.808290155440414, \"test_mlm_ppl\": 6287.989747199166, \"test_mlm_acc\": 8.937823834196891}\n",
      "=====================\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.Bafia.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.Bafia.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth\n",
      "=====================\n",
      "=====================\n",
      "Limbum to Bafia\n",
      "Limbum to MKPAMAN_AMVOE_Ewondo\n",
      "=====================\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/30/20 16:33:04 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/30/20 16:33:04 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_BafiaEwondo --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo' --lgs 'Bafia-MKPAMAN_AMVOE_Ewondo' --clm_steps '' --mlm_steps 'Bafia,MKPAMAN_AMVOE_Ewondo' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaEwondo/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_BafiaEwondo\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Bafia', 1: 'MKPAMAN_AMVOE_Ewondo'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Bafia': 0, 'MKPAMAN_AMVOE_Ewondo': 1}\n",
      "                                     langs: ['Bafia', 'MKPAMAN_AMVOE_Ewondo']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Bafia-MKPAMAN_AMVOE_Ewondo']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Bafia', None), ('MKPAMAN_AMVOE_Ewondo', None)]\n",
      "                                     mono_dataset: {'Bafia': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/train.Bafia.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.Bafia.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.Bafia.pth'}, 'MKPAMAN_AMVOE_Ewondo': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/train.MKPAMAN_AMVOE_Ewondo.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': False, 'valid': False, 'test': False}\n",
      "                                     remove_long_sentences_test: False\n",
      "                                     remove_long_sentences_train: False\n",
      "                                     remove_long_sentences_valid: False\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/30/20 16:33:04 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaEwondo/maml\n",
      "                                     \n",
      "INFO - 05/30/20 16:33:04 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_BafiaEwondo --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo' --lgs 'Bafia-MKPAMAN_AMVOE_Ewondo' --clm_steps '' --mlm_steps 'Bafia,MKPAMAN_AMVOE_Ewondo' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/30/20 16:33:04 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/30/20 16:33:04 - 0:00:00 - ============ langs: Bafia, MKPAMAN_AMVOE_Ewondo\n",
      "INFO - 05/30/20 16:33:04 - 0:00:00 - ============ Monolingual data (Bafia)\n",
      "INFO - 05/30/20 16:33:04 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.Bafia.pth ...\n",
      "INFO - 05/30/20 16:33:04 - 0:00:00 - 489888 words (9128 unique) in 7919 sentences. 12558 unknown words (116 unique) covering 2.56% of the data.\n",
      "INFO - 05/30/20 16:33:04 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/30/20 16:33:04 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.Bafia.pth ...\n",
      "INFO - 05/30/20 16:33:04 - 0:00:00 - 489888 words (9128 unique) in 7919 sentences. 12558 unknown words (116 unique) covering 2.56% of the data.\n",
      "INFO - 05/30/20 16:33:04 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/30/20 16:33:04 - 0:00:00 - ============ Monolingual data (MKPAMAN_AMVOE_Ewondo)\n",
      "INFO - 05/30/20 16:33:04 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/30/20 16:33:04 - 0:00:00 - 489888 words (9128 unique) in 7919 sentences. 12558 unknown words (116 unique) covering 2.56% of the data.\n",
      "INFO - 05/30/20 16:33:04 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/30/20 16:33:04 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/30/20 16:33:04 - 0:00:00 - 489888 words (9128 unique) in 7919 sentences. 12558 unknown words (116 unique) covering 2.56% of the data.\n",
      "INFO - 05/30/20 16:33:04 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/30/20 16:33:04 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/30/20 16:33:04 - 0:00:00 - Monolingual data   - valid -        Bafia:      7919\n",
      "INFO - 05/30/20 16:33:04 - 0:00:00 - Monolingual data   -  test -        Bafia:      7919\n",
      "INFO - 05/30/20 16:33:04 - 0:00:00 - Monolingual data   - valid - MKPAMAN_AMVOE_Ewondo:      7919\n",
      "INFO - 05/30/20 16:33:04 - 0:00:00 - Monolingual data   -  test - MKPAMAN_AMVOE_Ewondo:      7919\n",
      "\n",
      "INFO - 05/30/20 16:33:04 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(9128, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=9128, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/30/20 16:33:04 - 0:00:01 - Number of parameters (model): 85461928\n",
      "INFO - 05/30/20 16:33:10 - 0:00:06 - Found 0 memories.\n",
      "INFO - 05/30/20 16:33:10 - 0:00:06 - Found 6 FFN.\n",
      "INFO - 05/30/20 16:33:10 - 0:00:06 - Found 102 parameters in model.\n",
      "INFO - 05/30/20 16:33:10 - 0:00:06 - Optimizers: model\n",
      "WARNING - 05/30/20 16:33:10 - 0:00:06 - Reloading checkpoint from /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaEwondo/maml/checkpoint.pth ...\n",
      "WARNING - 05/30/20 16:33:11 - 0:00:07 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/30/20 16:33:11 - 0:00:07 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/30/20 16:33:11 - 0:00:07 - Checkpoint reloaded. Resuming at epoch 40 / iteration 5320 ...\n",
      "INFO - 05/30/20 16:33:14 - 0:00:10 - epoch -> 40.000000\n",
      "INFO - 05/30/20 16:33:14 - 0:00:10 - valid_Bafia_mlm_ppl -> 7010.615781\n",
      "INFO - 05/30/20 16:33:14 - 0:00:10 - valid_Bafia_mlm_acc -> 9.067358\n",
      "INFO - 05/30/20 16:33:14 - 0:00:10 - valid_MKPAMAN_AMVOE_Ewondo_mlm_ppl -> 5565.363713\n",
      "INFO - 05/30/20 16:33:14 - 0:00:10 - valid_MKPAMAN_AMVOE_Ewondo_mlm_acc -> 8.808290\n",
      "INFO - 05/30/20 16:33:14 - 0:00:10 - valid_mlm_ppl -> 6287.989747\n",
      "INFO - 05/30/20 16:33:14 - 0:00:10 - valid_mlm_acc -> 8.937824\n",
      "INFO - 05/30/20 16:33:14 - 0:00:10 - test_Bafia_mlm_ppl -> 7010.615781\n",
      "INFO - 05/30/20 16:33:14 - 0:00:10 - test_Bafia_mlm_acc -> 9.067358\n",
      "INFO - 05/30/20 16:33:14 - 0:00:10 - test_MKPAMAN_AMVOE_Ewondo_mlm_ppl -> 5565.363713\n",
      "INFO - 05/30/20 16:33:14 - 0:00:10 - test_MKPAMAN_AMVOE_Ewondo_mlm_acc -> 8.808290\n",
      "INFO - 05/30/20 16:33:14 - 0:00:10 - test_mlm_ppl -> 6287.989747\n",
      "INFO - 05/30/20 16:33:14 - 0:00:10 - test_mlm_acc -> 8.937824\n",
      "INFO - 05/30/20 16:33:14 - 0:00:10 - __log__:{\"epoch\": 40, \"valid_Bafia_mlm_ppl\": 7010.615781160661, \"valid_Bafia_mlm_acc\": 9.067357512953368, \"valid_MKPAMAN_AMVOE_Ewondo_mlm_ppl\": 5565.363713237672, \"valid_MKPAMAN_AMVOE_Ewondo_mlm_acc\": 8.808290155440414, \"valid_mlm_ppl\": 6287.989747199166, \"valid_mlm_acc\": 8.937823834196891, \"test_Bafia_mlm_ppl\": 7010.615781160661, \"test_Bafia_mlm_acc\": 9.067357512953368, \"test_MKPAMAN_AMVOE_Ewondo_mlm_ppl\": 5565.363713237672, \"test_MKPAMAN_AMVOE_Ewondo_mlm_acc\": 8.808290155440414, \"test_mlm_ppl\": 6287.989747199166, \"test_mlm_acc\": 8.937823834196891}\n",
      "=====================\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.Bafia.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.Bafia.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth\n",
      "=====================\n",
      "=====================\n",
      "Limbum to Bafia\n",
      "Limbum to MKPAMAN_AMVOE_Ewondo\n",
      "=====================\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/30/20 16:33:15 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/30/20 16:33:15 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_BafiaEwondo --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo' --lgs 'Bafia-MKPAMAN_AMVOE_Ewondo' --clm_steps '' --mlm_steps 'Bafia,MKPAMAN_AMVOE_Ewondo' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaEwondo/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_BafiaEwondo\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Bafia', 1: 'MKPAMAN_AMVOE_Ewondo'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Bafia': 0, 'MKPAMAN_AMVOE_Ewondo': 1}\n",
      "                                     langs: ['Bafia', 'MKPAMAN_AMVOE_Ewondo']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Bafia-MKPAMAN_AMVOE_Ewondo']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Bafia', None), ('MKPAMAN_AMVOE_Ewondo', None)]\n",
      "                                     mono_dataset: {'Bafia': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/train.Bafia.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.Bafia.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.Bafia.pth'}, 'MKPAMAN_AMVOE_Ewondo': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/train.MKPAMAN_AMVOE_Ewondo.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': False, 'valid': False, 'test': False}\n",
      "                                     remove_long_sentences_test: False\n",
      "                                     remove_long_sentences_train: False\n",
      "                                     remove_long_sentences_valid: False\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/30/20 16:33:15 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaEwondo/maml\n",
      "                                     \n",
      "INFO - 05/30/20 16:33:15 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_BafiaEwondo --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo' --lgs 'Bafia-MKPAMAN_AMVOE_Ewondo' --clm_steps '' --mlm_steps 'Bafia,MKPAMAN_AMVOE_Ewondo' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/30/20 16:33:15 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/30/20 16:33:15 - 0:00:00 - ============ langs: Bafia, MKPAMAN_AMVOE_Ewondo\n",
      "INFO - 05/30/20 16:33:15 - 0:00:00 - ============ Monolingual data (Bafia)\n",
      "INFO - 05/30/20 16:33:15 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.Bafia.pth ...\n",
      "INFO - 05/30/20 16:33:15 - 0:00:00 - 489888 words (9128 unique) in 7919 sentences. 12558 unknown words (116 unique) covering 2.56% of the data.\n",
      "INFO - 05/30/20 16:33:15 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/30/20 16:33:15 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.Bafia.pth ...\n",
      "INFO - 05/30/20 16:33:15 - 0:00:00 - 489888 words (9128 unique) in 7919 sentences. 12558 unknown words (116 unique) covering 2.56% of the data.\n",
      "INFO - 05/30/20 16:33:15 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/30/20 16:33:15 - 0:00:00 - ============ Monolingual data (MKPAMAN_AMVOE_Ewondo)\n",
      "INFO - 05/30/20 16:33:15 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/30/20 16:33:15 - 0:00:00 - 489888 words (9128 unique) in 7919 sentences. 12558 unknown words (116 unique) covering 2.56% of the data.\n",
      "INFO - 05/30/20 16:33:15 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/30/20 16:33:15 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/30/20 16:33:15 - 0:00:00 - 489888 words (9128 unique) in 7919 sentences. 12558 unknown words (116 unique) covering 2.56% of the data.\n",
      "INFO - 05/30/20 16:33:15 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/30/20 16:33:15 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/30/20 16:33:15 - 0:00:00 - Monolingual data   - valid -        Bafia:      7919\n",
      "INFO - 05/30/20 16:33:15 - 0:00:00 - Monolingual data   -  test -        Bafia:      7919\n",
      "INFO - 05/30/20 16:33:15 - 0:00:00 - Monolingual data   - valid - MKPAMAN_AMVOE_Ewondo:      7919\n",
      "INFO - 05/30/20 16:33:15 - 0:00:00 - Monolingual data   -  test - MKPAMAN_AMVOE_Ewondo:      7919\n",
      "\n",
      "INFO - 05/30/20 16:33:16 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(9128, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=9128, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/30/20 16:33:16 - 0:00:01 - Number of parameters (model): 85461928\n",
      "INFO - 05/30/20 16:33:21 - 0:00:06 - Found 0 memories.\n",
      "INFO - 05/30/20 16:33:21 - 0:00:06 - Found 6 FFN.\n",
      "INFO - 05/30/20 16:33:21 - 0:00:06 - Found 102 parameters in model.\n",
      "INFO - 05/30/20 16:33:21 - 0:00:07 - Optimizers: model\n",
      "WARNING - 05/30/20 16:33:21 - 0:00:07 - Reloading checkpoint from /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaEwondo/maml/checkpoint.pth ...\n",
      "WARNING - 05/30/20 16:33:22 - 0:00:07 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/30/20 16:33:22 - 0:00:07 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/30/20 16:33:22 - 0:00:07 - Checkpoint reloaded. Resuming at epoch 40 / iteration 5320 ...\n",
      "INFO - 05/30/20 16:33:24 - 0:00:10 - epoch -> 40.000000\n",
      "INFO - 05/30/20 16:33:24 - 0:00:10 - valid_Bafia_mlm_ppl -> 7010.615781\n",
      "INFO - 05/30/20 16:33:24 - 0:00:10 - valid_Bafia_mlm_acc -> 9.067358\n",
      "INFO - 05/30/20 16:33:24 - 0:00:10 - valid_MKPAMAN_AMVOE_Ewondo_mlm_ppl -> 5565.363713\n",
      "INFO - 05/30/20 16:33:24 - 0:00:10 - valid_MKPAMAN_AMVOE_Ewondo_mlm_acc -> 8.808290\n",
      "INFO - 05/30/20 16:33:24 - 0:00:10 - valid_mlm_ppl -> 6287.989747\n",
      "INFO - 05/30/20 16:33:24 - 0:00:10 - valid_mlm_acc -> 8.937824\n",
      "INFO - 05/30/20 16:33:24 - 0:00:10 - test_Bafia_mlm_ppl -> 7010.615781\n",
      "INFO - 05/30/20 16:33:24 - 0:00:10 - test_Bafia_mlm_acc -> 9.067358\n",
      "INFO - 05/30/20 16:33:24 - 0:00:10 - test_MKPAMAN_AMVOE_Ewondo_mlm_ppl -> 5565.363713\n",
      "INFO - 05/30/20 16:33:24 - 0:00:10 - test_MKPAMAN_AMVOE_Ewondo_mlm_acc -> 8.808290\n",
      "INFO - 05/30/20 16:33:24 - 0:00:10 - test_mlm_ppl -> 6287.989747\n",
      "INFO - 05/30/20 16:33:24 - 0:00:10 - test_mlm_acc -> 8.937824\n",
      "INFO - 05/30/20 16:33:24 - 0:00:10 - __log__:{\"epoch\": 40, \"valid_Bafia_mlm_ppl\": 7010.615781160661, \"valid_Bafia_mlm_acc\": 9.067357512953368, \"valid_MKPAMAN_AMVOE_Ewondo_mlm_ppl\": 5565.363713237672, \"valid_MKPAMAN_AMVOE_Ewondo_mlm_acc\": 8.808290155440414, \"valid_mlm_ppl\": 6287.989747199166, \"valid_mlm_acc\": 8.937823834196891, \"test_Bafia_mlm_ppl\": 7010.615781160661, \"test_Bafia_mlm_acc\": 9.067357512953368, \"test_MKPAMAN_AMVOE_Ewondo_mlm_ppl\": 5565.363713237672, \"test_MKPAMAN_AMVOE_Ewondo_mlm_acc\": 8.808290155440414, \"test_mlm_ppl\": 6287.989747199166, \"test_mlm_acc\": 8.937823834196891}\n",
      "=====================\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.Bafia.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.Bafia.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "%env dump_path=/home/jupyter/models/africa/cluster1\n",
    "%env exp_name=mlm_tlm_BafiaEwondo\n",
    "%env data_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo\n",
    "%env lgs=Bafia-MKPAMAN_AMVOE_Ewondo\n",
    "%env mlm_steps=Bafia,MKPAMAN_AMVOE_Ewondo\n",
    "%env tgt_pair=Bafia-MKPAMAN_AMVOE_Ewondo\n",
    "%env src_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo\n",
    "%env tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo\n",
    "\n",
    "# creation of the dummy files so that the experiment does not bug\n",
    "! touch /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/train.Bafia.pth\n",
    "! touch /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/train.MKPAMAN_AMVOE_Ewondo.pth\n",
    "\n",
    "###### Bafia_Ewondo vs Bulu\n",
    "! ../duplicate.sh $src_path $tgt_path Bulu $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : delete the rename file\n",
    "! ../delete.sh $tgt_path $tgt_pair\n",
    "\n",
    "###### Bafia_Ewondo vs Limbum\n",
    "! ../duplicate.sh $src_path $tgt_path Limbum $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : delete the rename file\n",
    "! ../delete.sh $tgt_path $tgt_pair\n",
    "\n",
    "###### Bafia_Ewondo vs Ghomala\n",
    "! ../duplicate.sh $src_path $tgt_path Limbum $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : delete the rename file\n",
    "! ../delete.sh $tgt_path $tgt_pair\n",
    "\n",
    "###### Bafia_Ewondo vs Ngiemboon\n",
    "! ../duplicate.sh $src_path $tgt_path Limbum $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : delete the rename file\n",
    "! ../delete.sh $tgt_path $tgt_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bulu_Ewondo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: dump_path=/home/jupyter/models/africa/cluster1\n",
      "env: exp_name=mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo\n",
      "env: data_path=/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo\n",
      "env: lgs=Bulu-MKPAMAN_AMVOE_Ewondo\n",
      "env: mlm_steps=Bulu,MKPAMAN_AMVOE_Ewondo\n",
      "env: tgt_pair=Bulu-MKPAMAN_AMVOE_Ewondo\n",
      "env: src_path=/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo\n",
      "env: tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo\n",
      "=====================\n",
      "Bafia to Bulu\n",
      "Bafia to MKPAMAN_AMVOE_Ewondo\n",
      "=====================\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/30/20 16:34:12 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/30/20 16:34:12 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo' --lgs 'Bulu-MKPAMAN_AMVOE_Ewondo' --clm_steps '' --mlm_steps 'Bulu,MKPAMAN_AMVOE_Ewondo' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster1/mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Bulu', 1: 'MKPAMAN_AMVOE_Ewondo'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Bulu': 0, 'MKPAMAN_AMVOE_Ewondo': 1}\n",
      "                                     langs: ['Bulu', 'MKPAMAN_AMVOE_Ewondo']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Bulu-MKPAMAN_AMVOE_Ewondo']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Bulu', None), ('MKPAMAN_AMVOE_Ewondo', None)]\n",
      "                                     mono_dataset: {'Bulu': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/train.Bulu.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.Bulu.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.Bulu.pth'}, 'MKPAMAN_AMVOE_Ewondo': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/train.MKPAMAN_AMVOE_Ewondo.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': False, 'valid': False, 'test': False}\n",
      "                                     remove_long_sentences_test: False\n",
      "                                     remove_long_sentences_train: False\n",
      "                                     remove_long_sentences_valid: False\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/30/20 16:34:12 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster1/mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo/maml\n",
      "                                     \n",
      "INFO - 05/30/20 16:34:12 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo' --lgs 'Bulu-MKPAMAN_AMVOE_Ewondo' --clm_steps '' --mlm_steps 'Bulu,MKPAMAN_AMVOE_Ewondo' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/30/20 16:34:12 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/30/20 16:34:12 - 0:00:00 - ============ langs: Bulu, MKPAMAN_AMVOE_Ewondo\n",
      "INFO - 05/30/20 16:34:12 - 0:00:00 - ============ Monolingual data (Bulu)\n",
      "INFO - 05/30/20 16:34:12 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.Bulu.pth ...\n",
      "INFO - 05/30/20 16:34:12 - 0:00:00 - 621128 words (9061 unique) in 7950 sentences. 214574 unknown words (147 unique) covering 34.55% of the data.\n",
      "INFO - 05/30/20 16:34:12 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/30/20 16:34:12 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.Bulu.pth ...\n",
      "INFO - 05/30/20 16:34:12 - 0:00:00 - 621128 words (9061 unique) in 7950 sentences. 214574 unknown words (147 unique) covering 34.55% of the data.\n",
      "INFO - 05/30/20 16:34:12 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/30/20 16:34:12 - 0:00:00 - ============ Monolingual data (MKPAMAN_AMVOE_Ewondo)\n",
      "INFO - 05/30/20 16:34:12 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/30/20 16:34:12 - 0:00:00 - 621128 words (9061 unique) in 7950 sentences. 214574 unknown words (147 unique) covering 34.55% of the data.\n",
      "INFO - 05/30/20 16:34:12 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/30/20 16:34:12 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/30/20 16:34:12 - 0:00:00 - 621128 words (9061 unique) in 7950 sentences. 214574 unknown words (147 unique) covering 34.55% of the data.\n",
      "INFO - 05/30/20 16:34:12 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/30/20 16:34:12 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/30/20 16:34:12 - 0:00:00 - Monolingual data   - valid -         Bulu:      7950\n",
      "INFO - 05/30/20 16:34:12 - 0:00:00 - Monolingual data   -  test -         Bulu:      7950\n",
      "INFO - 05/30/20 16:34:12 - 0:00:00 - Monolingual data   - valid - MKPAMAN_AMVOE_Ewondo:      7950\n",
      "INFO - 05/30/20 16:34:12 - 0:00:00 - Monolingual data   -  test - MKPAMAN_AMVOE_Ewondo:      7950\n",
      "\n",
      "INFO - 05/30/20 16:34:12 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(9061, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=9061, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/30/20 16:34:12 - 0:00:01 - Number of parameters (model): 85393253\n",
      "INFO - 05/30/20 16:34:18 - 0:00:06 - Found 0 memories.\n",
      "INFO - 05/30/20 16:34:18 - 0:00:06 - Found 6 FFN.\n",
      "INFO - 05/30/20 16:34:18 - 0:00:06 - Found 102 parameters in model.\n",
      "INFO - 05/30/20 16:34:18 - 0:00:06 - Optimizers: model\n",
      "WARNING - 05/30/20 16:34:18 - 0:00:06 - Reloading checkpoint from /home/jupyter/models/africa/cluster1/mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo/maml/checkpoint.pth ...\n",
      "WARNING - 05/30/20 16:34:19 - 0:00:07 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/30/20 16:34:19 - 0:00:07 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/30/20 16:34:19 - 0:00:07 - Checkpoint reloaded. Resuming at epoch 32 / iteration 8480 ...\n",
      "INFO - 05/30/20 16:34:21 - 0:00:10 - epoch -> 32.000000\n",
      "INFO - 05/30/20 16:34:21 - 0:00:10 - valid_Bulu_mlm_ppl -> 15212.015517\n",
      "INFO - 05/30/20 16:34:21 - 0:00:10 - valid_Bulu_mlm_acc -> 11.139896\n",
      "INFO - 05/30/20 16:34:21 - 0:00:10 - valid_MKPAMAN_AMVOE_Ewondo_mlm_ppl -> 23927.731071\n",
      "INFO - 05/30/20 16:34:21 - 0:00:10 - valid_MKPAMAN_AMVOE_Ewondo_mlm_acc -> 9.844560\n",
      "INFO - 05/30/20 16:34:21 - 0:00:10 - valid_mlm_ppl -> 19569.873294\n",
      "INFO - 05/30/20 16:34:21 - 0:00:10 - valid_mlm_acc -> 10.492228\n",
      "INFO - 05/30/20 16:34:21 - 0:00:10 - test_Bulu_mlm_ppl -> 15212.015517\n",
      "INFO - 05/30/20 16:34:21 - 0:00:10 - test_Bulu_mlm_acc -> 11.139896\n",
      "INFO - 05/30/20 16:34:21 - 0:00:10 - test_MKPAMAN_AMVOE_Ewondo_mlm_ppl -> 23927.731071\n",
      "INFO - 05/30/20 16:34:21 - 0:00:10 - test_MKPAMAN_AMVOE_Ewondo_mlm_acc -> 9.844560\n",
      "INFO - 05/30/20 16:34:21 - 0:00:10 - test_mlm_ppl -> 19569.873294\n",
      "INFO - 05/30/20 16:34:21 - 0:00:10 - test_mlm_acc -> 10.492228\n",
      "INFO - 05/30/20 16:34:21 - 0:00:10 - __log__:{\"epoch\": 32, \"valid_Bulu_mlm_ppl\": 15212.01551742137, \"valid_Bulu_mlm_acc\": 11.139896373056995, \"valid_MKPAMAN_AMVOE_Ewondo_mlm_ppl\": 23927.731071248774, \"valid_MKPAMAN_AMVOE_Ewondo_mlm_acc\": 9.844559585492227, \"valid_mlm_ppl\": 19569.873294335073, \"valid_mlm_acc\": 10.492227979274611, \"test_Bulu_mlm_ppl\": 15212.01551742137, \"test_Bulu_mlm_acc\": 11.139896373056995, \"test_MKPAMAN_AMVOE_Ewondo_mlm_ppl\": 23927.731071248774, \"test_MKPAMAN_AMVOE_Ewondo_mlm_acc\": 9.844559585492227, \"test_mlm_ppl\": 19569.873294335073, \"test_mlm_acc\": 10.492227979274611}\n",
      "=====================\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.Bulu.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.Bulu.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth\n",
      "=====================\n",
      "=====================\n",
      "Limbum to Bulu\n",
      "Limbum to MKPAMAN_AMVOE_Ewondo\n",
      "=====================\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/30/20 16:34:23 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/30/20 16:34:23 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo' --lgs 'Bulu-MKPAMAN_AMVOE_Ewondo' --clm_steps '' --mlm_steps 'Bulu,MKPAMAN_AMVOE_Ewondo' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster1/mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Bulu', 1: 'MKPAMAN_AMVOE_Ewondo'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Bulu': 0, 'MKPAMAN_AMVOE_Ewondo': 1}\n",
      "                                     langs: ['Bulu', 'MKPAMAN_AMVOE_Ewondo']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Bulu-MKPAMAN_AMVOE_Ewondo']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Bulu', None), ('MKPAMAN_AMVOE_Ewondo', None)]\n",
      "                                     mono_dataset: {'Bulu': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/train.Bulu.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.Bulu.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.Bulu.pth'}, 'MKPAMAN_AMVOE_Ewondo': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/train.MKPAMAN_AMVOE_Ewondo.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': False, 'valid': False, 'test': False}\n",
      "                                     remove_long_sentences_test: False\n",
      "                                     remove_long_sentences_train: False\n",
      "                                     remove_long_sentences_valid: False\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/30/20 16:34:23 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster1/mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo/maml\n",
      "                                     \n",
      "INFO - 05/30/20 16:34:23 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo' --lgs 'Bulu-MKPAMAN_AMVOE_Ewondo' --clm_steps '' --mlm_steps 'Bulu,MKPAMAN_AMVOE_Ewondo' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/30/20 16:34:23 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/30/20 16:34:23 - 0:00:00 - ============ langs: Bulu, MKPAMAN_AMVOE_Ewondo\n",
      "INFO - 05/30/20 16:34:23 - 0:00:00 - ============ Monolingual data (Bulu)\n",
      "INFO - 05/30/20 16:34:23 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.Bulu.pth ...\n",
      "INFO - 05/30/20 16:34:23 - 0:00:00 - 532918 words (9061 unique) in 7919 sentences. 30235 unknown words (121 unique) covering 5.67% of the data.\n",
      "INFO - 05/30/20 16:34:23 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/30/20 16:34:23 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.Bulu.pth ...\n",
      "INFO - 05/30/20 16:34:23 - 0:00:00 - 532918 words (9061 unique) in 7919 sentences. 30235 unknown words (121 unique) covering 5.67% of the data.\n",
      "INFO - 05/30/20 16:34:23 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/30/20 16:34:23 - 0:00:00 - ============ Monolingual data (MKPAMAN_AMVOE_Ewondo)\n",
      "INFO - 05/30/20 16:34:23 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/30/20 16:34:23 - 0:00:00 - 532918 words (9061 unique) in 7919 sentences. 30235 unknown words (121 unique) covering 5.67% of the data.\n",
      "INFO - 05/30/20 16:34:23 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/30/20 16:34:23 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/30/20 16:34:23 - 0:00:00 - 532918 words (9061 unique) in 7919 sentences. 30235 unknown words (121 unique) covering 5.67% of the data.\n",
      "INFO - 05/30/20 16:34:23 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/30/20 16:34:23 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/30/20 16:34:23 - 0:00:00 - Monolingual data   - valid -         Bulu:      7919\n",
      "INFO - 05/30/20 16:34:23 - 0:00:00 - Monolingual data   -  test -         Bulu:      7919\n",
      "INFO - 05/30/20 16:34:23 - 0:00:00 - Monolingual data   - valid - MKPAMAN_AMVOE_Ewondo:      7919\n",
      "INFO - 05/30/20 16:34:23 - 0:00:00 - Monolingual data   -  test - MKPAMAN_AMVOE_Ewondo:      7919\n",
      "\n",
      "INFO - 05/30/20 16:34:23 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(9061, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=9061, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/30/20 16:34:23 - 0:00:01 - Number of parameters (model): 85393253\n",
      "INFO - 05/30/20 16:34:30 - 0:00:07 - Found 0 memories.\n",
      "INFO - 05/30/20 16:34:30 - 0:00:07 - Found 6 FFN.\n",
      "INFO - 05/30/20 16:34:30 - 0:00:07 - Found 102 parameters in model.\n",
      "INFO - 05/30/20 16:34:30 - 0:00:07 - Optimizers: model\n",
      "WARNING - 05/30/20 16:34:30 - 0:00:07 - Reloading checkpoint from /home/jupyter/models/africa/cluster1/mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo/maml/checkpoint.pth ...\n",
      "WARNING - 05/30/20 16:34:30 - 0:00:08 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/30/20 16:34:30 - 0:00:08 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/30/20 16:34:30 - 0:00:08 - Checkpoint reloaded. Resuming at epoch 32 / iteration 8480 ...\n",
      "INFO - 05/30/20 16:34:32 - 0:00:10 - epoch -> 32.000000\n",
      "INFO - 05/30/20 16:34:32 - 0:00:10 - valid_Bulu_mlm_ppl -> 8988.423852\n",
      "INFO - 05/30/20 16:34:32 - 0:00:10 - valid_Bulu_mlm_acc -> 5.958549\n",
      "INFO - 05/30/20 16:34:32 - 0:00:10 - valid_MKPAMAN_AMVOE_Ewondo_mlm_ppl -> 18431.467178\n",
      "INFO - 05/30/20 16:34:32 - 0:00:10 - valid_MKPAMAN_AMVOE_Ewondo_mlm_acc -> 4.663212\n",
      "INFO - 05/30/20 16:34:32 - 0:00:10 - valid_mlm_ppl -> 13709.945515\n",
      "INFO - 05/30/20 16:34:32 - 0:00:10 - valid_mlm_acc -> 5.310881\n",
      "INFO - 05/30/20 16:34:32 - 0:00:10 - test_Bulu_mlm_ppl -> 8988.423852\n",
      "INFO - 05/30/20 16:34:32 - 0:00:10 - test_Bulu_mlm_acc -> 5.958549\n",
      "INFO - 05/30/20 16:34:32 - 0:00:10 - test_MKPAMAN_AMVOE_Ewondo_mlm_ppl -> 18431.467178\n",
      "INFO - 05/30/20 16:34:32 - 0:00:10 - test_MKPAMAN_AMVOE_Ewondo_mlm_acc -> 4.663212\n",
      "INFO - 05/30/20 16:34:32 - 0:00:10 - test_mlm_ppl -> 13709.945515\n",
      "INFO - 05/30/20 16:34:32 - 0:00:10 - test_mlm_acc -> 5.310881\n",
      "INFO - 05/30/20 16:34:32 - 0:00:10 - __log__:{\"epoch\": 32, \"valid_Bulu_mlm_ppl\": 8988.423852294944, \"valid_Bulu_mlm_acc\": 5.958549222797927, \"valid_MKPAMAN_AMVOE_Ewondo_mlm_ppl\": 18431.46717810602, \"valid_MKPAMAN_AMVOE_Ewondo_mlm_acc\": 4.66321243523316, \"valid_mlm_ppl\": 13709.945515200481, \"valid_mlm_acc\": 5.310880829015543, \"test_Bulu_mlm_ppl\": 8988.423852294944, \"test_Bulu_mlm_acc\": 5.958549222797927, \"test_MKPAMAN_AMVOE_Ewondo_mlm_ppl\": 18431.46717810602, \"test_MKPAMAN_AMVOE_Ewondo_mlm_acc\": 4.66321243523316, \"test_mlm_ppl\": 13709.945515200481, \"test_mlm_acc\": 5.310880829015543}\n",
      "=====================\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.Bulu.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.Bulu.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth\n",
      "=====================\n",
      "=====================\n",
      "Limbum to Bulu\n",
      "Limbum to MKPAMAN_AMVOE_Ewondo\n",
      "=====================\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/30/20 16:34:33 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/30/20 16:34:33 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo' --lgs 'Bulu-MKPAMAN_AMVOE_Ewondo' --clm_steps '' --mlm_steps 'Bulu,MKPAMAN_AMVOE_Ewondo' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster1/mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Bulu', 1: 'MKPAMAN_AMVOE_Ewondo'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Bulu': 0, 'MKPAMAN_AMVOE_Ewondo': 1}\n",
      "                                     langs: ['Bulu', 'MKPAMAN_AMVOE_Ewondo']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Bulu-MKPAMAN_AMVOE_Ewondo']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Bulu', None), ('MKPAMAN_AMVOE_Ewondo', None)]\n",
      "                                     mono_dataset: {'Bulu': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/train.Bulu.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.Bulu.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.Bulu.pth'}, 'MKPAMAN_AMVOE_Ewondo': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/train.MKPAMAN_AMVOE_Ewondo.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': False, 'valid': False, 'test': False}\n",
      "                                     remove_long_sentences_test: False\n",
      "                                     remove_long_sentences_train: False\n",
      "                                     remove_long_sentences_valid: False\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/30/20 16:34:33 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster1/mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo/maml\n",
      "                                     \n",
      "INFO - 05/30/20 16:34:33 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo' --lgs 'Bulu-MKPAMAN_AMVOE_Ewondo' --clm_steps '' --mlm_steps 'Bulu,MKPAMAN_AMVOE_Ewondo' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/30/20 16:34:33 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/30/20 16:34:33 - 0:00:00 - ============ langs: Bulu, MKPAMAN_AMVOE_Ewondo\n",
      "INFO - 05/30/20 16:34:33 - 0:00:00 - ============ Monolingual data (Bulu)\n",
      "INFO - 05/30/20 16:34:33 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.Bulu.pth ...\n",
      "INFO - 05/30/20 16:34:33 - 0:00:00 - 532918 words (9061 unique) in 7919 sentences. 30235 unknown words (121 unique) covering 5.67% of the data.\n",
      "INFO - 05/30/20 16:34:33 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/30/20 16:34:33 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.Bulu.pth ...\n",
      "INFO - 05/30/20 16:34:33 - 0:00:00 - 532918 words (9061 unique) in 7919 sentences. 30235 unknown words (121 unique) covering 5.67% of the data.\n",
      "INFO - 05/30/20 16:34:33 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/30/20 16:34:33 - 0:00:00 - ============ Monolingual data (MKPAMAN_AMVOE_Ewondo)\n",
      "INFO - 05/30/20 16:34:33 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/30/20 16:34:33 - 0:00:00 - 532918 words (9061 unique) in 7919 sentences. 30235 unknown words (121 unique) covering 5.67% of the data.\n",
      "INFO - 05/30/20 16:34:33 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/30/20 16:34:33 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/30/20 16:34:33 - 0:00:00 - 532918 words (9061 unique) in 7919 sentences. 30235 unknown words (121 unique) covering 5.67% of the data.\n",
      "INFO - 05/30/20 16:34:33 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/30/20 16:34:33 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/30/20 16:34:33 - 0:00:00 - Monolingual data   - valid -         Bulu:      7919\n",
      "INFO - 05/30/20 16:34:33 - 0:00:00 - Monolingual data   -  test -         Bulu:      7919\n",
      "INFO - 05/30/20 16:34:33 - 0:00:00 - Monolingual data   - valid - MKPAMAN_AMVOE_Ewondo:      7919\n",
      "INFO - 05/30/20 16:34:33 - 0:00:00 - Monolingual data   -  test - MKPAMAN_AMVOE_Ewondo:      7919\n",
      "\n",
      "INFO - 05/30/20 16:34:34 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(9061, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=9061, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/30/20 16:34:34 - 0:00:01 - Number of parameters (model): 85393253\n",
      "INFO - 05/30/20 16:34:40 - 0:00:07 - Found 0 memories.\n",
      "INFO - 05/30/20 16:34:40 - 0:00:07 - Found 6 FFN.\n",
      "INFO - 05/30/20 16:34:40 - 0:00:07 - Found 102 parameters in model.\n",
      "INFO - 05/30/20 16:34:40 - 0:00:07 - Optimizers: model\n",
      "WARNING - 05/30/20 16:34:40 - 0:00:07 - Reloading checkpoint from /home/jupyter/models/africa/cluster1/mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo/maml/checkpoint.pth ...\n",
      "WARNING - 05/30/20 16:34:41 - 0:00:07 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/30/20 16:34:41 - 0:00:07 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/30/20 16:34:41 - 0:00:07 - Checkpoint reloaded. Resuming at epoch 32 / iteration 8480 ...\n",
      "INFO - 05/30/20 16:34:43 - 0:00:10 - epoch -> 32.000000\n",
      "INFO - 05/30/20 16:34:43 - 0:00:10 - valid_Bulu_mlm_ppl -> 8988.423852\n",
      "INFO - 05/30/20 16:34:43 - 0:00:10 - valid_Bulu_mlm_acc -> 5.958549\n",
      "INFO - 05/30/20 16:34:43 - 0:00:10 - valid_MKPAMAN_AMVOE_Ewondo_mlm_ppl -> 18431.467178\n",
      "INFO - 05/30/20 16:34:43 - 0:00:10 - valid_MKPAMAN_AMVOE_Ewondo_mlm_acc -> 4.663212\n",
      "INFO - 05/30/20 16:34:43 - 0:00:10 - valid_mlm_ppl -> 13709.945515\n",
      "INFO - 05/30/20 16:34:43 - 0:00:10 - valid_mlm_acc -> 5.310881\n",
      "INFO - 05/30/20 16:34:43 - 0:00:10 - test_Bulu_mlm_ppl -> 8988.423852\n",
      "INFO - 05/30/20 16:34:43 - 0:00:10 - test_Bulu_mlm_acc -> 5.958549\n",
      "INFO - 05/30/20 16:34:43 - 0:00:10 - test_MKPAMAN_AMVOE_Ewondo_mlm_ppl -> 18431.467178\n",
      "INFO - 05/30/20 16:34:43 - 0:00:10 - test_MKPAMAN_AMVOE_Ewondo_mlm_acc -> 4.663212\n",
      "INFO - 05/30/20 16:34:43 - 0:00:10 - test_mlm_ppl -> 13709.945515\n",
      "INFO - 05/30/20 16:34:43 - 0:00:10 - test_mlm_acc -> 5.310881\n",
      "INFO - 05/30/20 16:34:43 - 0:00:10 - __log__:{\"epoch\": 32, \"valid_Bulu_mlm_ppl\": 8988.423852294944, \"valid_Bulu_mlm_acc\": 5.958549222797927, \"valid_MKPAMAN_AMVOE_Ewondo_mlm_ppl\": 18431.46717810602, \"valid_MKPAMAN_AMVOE_Ewondo_mlm_acc\": 4.66321243523316, \"valid_mlm_ppl\": 13709.945515200481, \"valid_mlm_acc\": 5.310880829015543, \"test_Bulu_mlm_ppl\": 8988.423852294944, \"test_Bulu_mlm_acc\": 5.958549222797927, \"test_MKPAMAN_AMVOE_Ewondo_mlm_ppl\": 18431.46717810602, \"test_MKPAMAN_AMVOE_Ewondo_mlm_acc\": 4.66321243523316, \"test_mlm_ppl\": 13709.945515200481, \"test_mlm_acc\": 5.310880829015543}\n",
      "=====================\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.Bulu.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.Bulu.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth\n",
      "=====================\n",
      "=====================\n",
      "Limbum to Bulu\n",
      "Limbum to MKPAMAN_AMVOE_Ewondo\n",
      "=====================\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/30/20 16:34:44 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/30/20 16:34:44 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo' --lgs 'Bulu-MKPAMAN_AMVOE_Ewondo' --clm_steps '' --mlm_steps 'Bulu,MKPAMAN_AMVOE_Ewondo' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster1/mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Bulu', 1: 'MKPAMAN_AMVOE_Ewondo'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Bulu': 0, 'MKPAMAN_AMVOE_Ewondo': 1}\n",
      "                                     langs: ['Bulu', 'MKPAMAN_AMVOE_Ewondo']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Bulu-MKPAMAN_AMVOE_Ewondo']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Bulu', None), ('MKPAMAN_AMVOE_Ewondo', None)]\n",
      "                                     mono_dataset: {'Bulu': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/train.Bulu.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.Bulu.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.Bulu.pth'}, 'MKPAMAN_AMVOE_Ewondo': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/train.MKPAMAN_AMVOE_Ewondo.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': False, 'valid': False, 'test': False}\n",
      "                                     remove_long_sentences_test: False\n",
      "                                     remove_long_sentences_train: False\n",
      "                                     remove_long_sentences_valid: False\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/30/20 16:34:44 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster1/mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo/maml\n",
      "                                     \n",
      "INFO - 05/30/20 16:34:44 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo' --lgs 'Bulu-MKPAMAN_AMVOE_Ewondo' --clm_steps '' --mlm_steps 'Bulu,MKPAMAN_AMVOE_Ewondo' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/30/20 16:34:44 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/30/20 16:34:44 - 0:00:00 - ============ langs: Bulu, MKPAMAN_AMVOE_Ewondo\n",
      "INFO - 05/30/20 16:34:44 - 0:00:00 - ============ Monolingual data (Bulu)\n",
      "INFO - 05/30/20 16:34:44 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.Bulu.pth ...\n",
      "INFO - 05/30/20 16:34:44 - 0:00:00 - 532918 words (9061 unique) in 7919 sentences. 30235 unknown words (121 unique) covering 5.67% of the data.\n",
      "INFO - 05/30/20 16:34:44 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/30/20 16:34:44 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.Bulu.pth ...\n",
      "INFO - 05/30/20 16:34:44 - 0:00:00 - 532918 words (9061 unique) in 7919 sentences. 30235 unknown words (121 unique) covering 5.67% of the data.\n",
      "INFO - 05/30/20 16:34:44 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/30/20 16:34:44 - 0:00:00 - ============ Monolingual data (MKPAMAN_AMVOE_Ewondo)\n",
      "INFO - 05/30/20 16:34:44 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/30/20 16:34:44 - 0:00:00 - 532918 words (9061 unique) in 7919 sentences. 30235 unknown words (121 unique) covering 5.67% of the data.\n",
      "INFO - 05/30/20 16:34:44 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/30/20 16:34:44 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/30/20 16:34:44 - 0:00:00 - 532918 words (9061 unique) in 7919 sentences. 30235 unknown words (121 unique) covering 5.67% of the data.\n",
      "INFO - 05/30/20 16:34:44 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/30/20 16:34:44 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/30/20 16:34:44 - 0:00:00 - Monolingual data   - valid -         Bulu:      7919\n",
      "INFO - 05/30/20 16:34:44 - 0:00:00 - Monolingual data   -  test -         Bulu:      7919\n",
      "INFO - 05/30/20 16:34:44 - 0:00:00 - Monolingual data   - valid - MKPAMAN_AMVOE_Ewondo:      7919\n",
      "INFO - 05/30/20 16:34:44 - 0:00:00 - Monolingual data   -  test - MKPAMAN_AMVOE_Ewondo:      7919\n",
      "\n",
      "INFO - 05/30/20 16:34:45 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(9061, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=9061, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/30/20 16:34:45 - 0:00:01 - Number of parameters (model): 85393253\n",
      "INFO - 05/30/20 16:34:51 - 0:00:07 - Found 0 memories.\n",
      "INFO - 05/30/20 16:34:51 - 0:00:07 - Found 6 FFN.\n",
      "INFO - 05/30/20 16:34:51 - 0:00:07 - Found 102 parameters in model.\n",
      "INFO - 05/30/20 16:34:51 - 0:00:07 - Optimizers: model\n",
      "WARNING - 05/30/20 16:34:51 - 0:00:07 - Reloading checkpoint from /home/jupyter/models/africa/cluster1/mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo/maml/checkpoint.pth ...\n",
      "WARNING - 05/30/20 16:34:52 - 0:00:08 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/30/20 16:34:52 - 0:00:08 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/30/20 16:34:52 - 0:00:08 - Checkpoint reloaded. Resuming at epoch 32 / iteration 8480 ...\n",
      "INFO - 05/30/20 16:34:55 - 0:00:10 - epoch -> 32.000000\n",
      "INFO - 05/30/20 16:34:55 - 0:00:10 - valid_Bulu_mlm_ppl -> 8988.423852\n",
      "INFO - 05/30/20 16:34:55 - 0:00:10 - valid_Bulu_mlm_acc -> 5.958549\n",
      "INFO - 05/30/20 16:34:55 - 0:00:10 - valid_MKPAMAN_AMVOE_Ewondo_mlm_ppl -> 18431.467178\n",
      "INFO - 05/30/20 16:34:55 - 0:00:10 - valid_MKPAMAN_AMVOE_Ewondo_mlm_acc -> 4.663212\n",
      "INFO - 05/30/20 16:34:55 - 0:00:10 - valid_mlm_ppl -> 13709.945515\n",
      "INFO - 05/30/20 16:34:55 - 0:00:10 - valid_mlm_acc -> 5.310881\n",
      "INFO - 05/30/20 16:34:55 - 0:00:10 - test_Bulu_mlm_ppl -> 8988.423852\n",
      "INFO - 05/30/20 16:34:55 - 0:00:10 - test_Bulu_mlm_acc -> 5.958549\n",
      "INFO - 05/30/20 16:34:55 - 0:00:10 - test_MKPAMAN_AMVOE_Ewondo_mlm_ppl -> 18431.467178\n",
      "INFO - 05/30/20 16:34:55 - 0:00:10 - test_MKPAMAN_AMVOE_Ewondo_mlm_acc -> 4.663212\n",
      "INFO - 05/30/20 16:34:55 - 0:00:10 - test_mlm_ppl -> 13709.945515\n",
      "INFO - 05/30/20 16:34:55 - 0:00:10 - test_mlm_acc -> 5.310881\n",
      "INFO - 05/30/20 16:34:55 - 0:00:10 - __log__:{\"epoch\": 32, \"valid_Bulu_mlm_ppl\": 8988.423852294944, \"valid_Bulu_mlm_acc\": 5.958549222797927, \"valid_MKPAMAN_AMVOE_Ewondo_mlm_ppl\": 18431.46717810602, \"valid_MKPAMAN_AMVOE_Ewondo_mlm_acc\": 4.66321243523316, \"valid_mlm_ppl\": 13709.945515200481, \"valid_mlm_acc\": 5.310880829015543, \"test_Bulu_mlm_ppl\": 8988.423852294944, \"test_Bulu_mlm_acc\": 5.958549222797927, \"test_MKPAMAN_AMVOE_Ewondo_mlm_ppl\": 18431.46717810602, \"test_MKPAMAN_AMVOE_Ewondo_mlm_acc\": 4.66321243523316, \"test_mlm_ppl\": 13709.945515200481, \"test_mlm_acc\": 5.310880829015543}\n",
      "=====================\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.Bulu.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.Bulu.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "%env dump_path=/home/jupyter/models/africa/cluster1\n",
    "%env exp_name=mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo\n",
    "%env data_path=/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo\n",
    "%env lgs=Bulu-MKPAMAN_AMVOE_Ewondo\n",
    "%env mlm_steps=Bulu,MKPAMAN_AMVOE_Ewondo\n",
    "%env tgt_pair=Bulu-MKPAMAN_AMVOE_Ewondo\n",
    "%env src_path=/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo\n",
    "%env tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo\n",
    "\n",
    "# creation of the dummy files so that the experiment does not bug\n",
    "! touch /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/train.Bulu.pth\n",
    "! touch /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/train.MKPAMAN_AMVOE_Ewondo.pth\n",
    "\n",
    "###### Bulu_Ewondo vs Bafia\n",
    "! ../duplicate.sh $src_path $tgt_path Bafia $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : delete the rename file\n",
    "! ../delete.sh $tgt_path $tgt_pair\n",
    "\n",
    "###### Bulu_Ewondo vs Limbum\n",
    "! ../duplicate.sh $src_path $tgt_path Limbum $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : delete the rename file\n",
    "! ../delete.sh $tgt_path $tgt_pair\n",
    "\n",
    "###### Bulu_Ewondo vs Ghomala\n",
    "! ../duplicate.sh $src_path $tgt_path Limbum $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : delete the rename file\n",
    "! ../delete.sh $tgt_path $tgt_pair\n",
    "\n",
    "###### Bulu_Ewondo vs Ngiemboon\n",
    "! ../duplicate.sh $src_path $tgt_path Limbum $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : delete the rename file\n",
    "! ../delete.sh $tgt_path $tgt_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ghomala_Limbum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: dump_path=/home/jupyter/models/africa/cluster3\n",
      "env: exp_name=mlm_tlm_GhomalaLimbum\n",
      "env: data_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum\n",
      "env: lgs=Ghomala-Limbum\n",
      "env: mlm_steps=Ghomala,Limbum\n",
      "env: tgt_pair=Ghomala-Limbum\n",
      "env: src_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum\n",
      "env: tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum\n",
      "=====================\n",
      "Ngiemboon to Ghomala\n",
      "Ngiemboon to Limbum\n",
      "=====================\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/30/20 16:35:47 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/30/20 16:35:47 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaLimbum --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum' --lgs 'Ghomala-Limbum' --clm_steps '' --mlm_steps 'Ghomala,Limbum' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_GhomalaLimbum\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Ghomala', 1: 'Limbum'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Ghomala': 0, 'Limbum': 1}\n",
      "                                     langs: ['Ghomala', 'Limbum']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Ghomala-Limbum']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Ghomala', None), ('Limbum', None)]\n",
      "                                     mono_dataset: {'Ghomala': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/train.Ghomala.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Ghomala.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Ghomala.pth'}, 'Limbum': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/train.Limbum.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Limbum.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Limbum.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': False, 'valid': False, 'test': False}\n",
      "                                     remove_long_sentences_test: False\n",
      "                                     remove_long_sentences_train: False\n",
      "                                     remove_long_sentences_valid: False\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/30/20 16:35:47 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml\n",
      "                                     \n",
      "INFO - 05/30/20 16:35:47 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaLimbum --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum' --lgs 'Ghomala-Limbum' --clm_steps '' --mlm_steps 'Ghomala,Limbum' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/30/20 16:35:47 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/30/20 16:35:47 - 0:00:00 - ============ langs: Ghomala, Limbum\n",
      "INFO - 05/30/20 16:35:47 - 0:00:00 - ============ Monolingual data (Ghomala)\n",
      "INFO - 05/30/20 16:35:47 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Ghomala.pth ...\n",
      "INFO - 05/30/20 16:35:47 - 0:00:00 - 807431 words (5057 unique) in 7929 sentences. 325627 unknown words (322 unique) covering 40.33% of the data.\n",
      "INFO - 05/30/20 16:35:47 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/30/20 16:35:47 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Ghomala.pth ...\n",
      "INFO - 05/30/20 16:35:47 - 0:00:00 - 807431 words (5057 unique) in 7929 sentences. 325627 unknown words (322 unique) covering 40.33% of the data.\n",
      "INFO - 05/30/20 16:35:47 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/30/20 16:35:47 - 0:00:00 - ============ Monolingual data (Limbum)\n",
      "INFO - 05/30/20 16:35:47 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Limbum.pth ...\n",
      "INFO - 05/30/20 16:35:47 - 0:00:00 - 807431 words (5057 unique) in 7929 sentences. 325627 unknown words (322 unique) covering 40.33% of the data.\n",
      "INFO - 05/30/20 16:35:47 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/30/20 16:35:47 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Limbum.pth ...\n",
      "INFO - 05/30/20 16:35:47 - 0:00:00 - 807431 words (5057 unique) in 7929 sentences. 325627 unknown words (322 unique) covering 40.33% of the data.\n",
      "INFO - 05/30/20 16:35:47 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/30/20 16:35:47 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/30/20 16:35:47 - 0:00:00 - Monolingual data   - valid -      Ghomala:      7929\n",
      "INFO - 05/30/20 16:35:47 - 0:00:00 - Monolingual data   -  test -      Ghomala:      7929\n",
      "INFO - 05/30/20 16:35:47 - 0:00:00 - Monolingual data   - valid -       Limbum:      7929\n",
      "INFO - 05/30/20 16:35:47 - 0:00:00 - Monolingual data   -  test -       Limbum:      7929\n",
      "\n",
      "INFO - 05/30/20 16:35:47 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(5057, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=5057, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/30/20 16:35:47 - 0:00:01 - Number of parameters (model): 81289153\n",
      "INFO - 05/30/20 16:35:53 - 0:00:06 - Found 0 memories.\n",
      "INFO - 05/30/20 16:35:53 - 0:00:06 - Found 6 FFN.\n",
      "INFO - 05/30/20 16:35:53 - 0:00:06 - Found 102 parameters in model.\n",
      "INFO - 05/30/20 16:35:53 - 0:00:06 - Optimizers: model\n",
      "WARNING - 05/30/20 16:35:53 - 0:00:06 - Reloading checkpoint from /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml/checkpoint.pth ...\n",
      "WARNING - 05/30/20 16:35:54 - 0:00:07 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/30/20 16:35:54 - 0:00:07 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/30/20 16:35:54 - 0:00:07 - Checkpoint reloaded. Resuming at epoch 52 / iteration 13728 ...\n",
      "INFO - 05/30/20 16:35:56 - 0:00:10 - epoch -> 52.000000\n",
      "INFO - 05/30/20 16:35:56 - 0:00:10 - valid_Ghomala_mlm_ppl -> 39110.947736\n",
      "INFO - 05/30/20 16:35:56 - 0:00:10 - valid_Ghomala_mlm_acc -> 3.108808\n",
      "INFO - 05/30/20 16:35:56 - 0:00:10 - valid_Limbum_mlm_ppl -> 36375.860328\n",
      "INFO - 05/30/20 16:35:56 - 0:00:10 - valid_Limbum_mlm_acc -> 4.145078\n",
      "INFO - 05/30/20 16:35:56 - 0:00:10 - valid_mlm_ppl -> 37743.404032\n",
      "INFO - 05/30/20 16:35:56 - 0:00:10 - valid_mlm_acc -> 3.626943\n",
      "INFO - 05/30/20 16:35:56 - 0:00:10 - test_Ghomala_mlm_ppl -> 39110.947736\n",
      "INFO - 05/30/20 16:35:56 - 0:00:10 - test_Ghomala_mlm_acc -> 3.108808\n",
      "INFO - 05/30/20 16:35:56 - 0:00:10 - test_Limbum_mlm_ppl -> 36375.860328\n",
      "INFO - 05/30/20 16:35:56 - 0:00:10 - test_Limbum_mlm_acc -> 4.145078\n",
      "INFO - 05/30/20 16:35:56 - 0:00:10 - test_mlm_ppl -> 37743.404032\n",
      "INFO - 05/30/20 16:35:56 - 0:00:10 - test_mlm_acc -> 3.626943\n",
      "INFO - 05/30/20 16:35:56 - 0:00:10 - __log__:{\"epoch\": 52, \"valid_Ghomala_mlm_ppl\": 39110.9477355718, \"valid_Ghomala_mlm_acc\": 3.1088082901554404, \"valid_Limbum_mlm_ppl\": 36375.86032779106, \"valid_Limbum_mlm_acc\": 4.1450777202072535, \"valid_mlm_ppl\": 37743.40403168143, \"valid_mlm_acc\": 3.6269430051813467, \"test_Ghomala_mlm_ppl\": 39110.9477355718, \"test_Ghomala_mlm_acc\": 3.1088082901554404, \"test_Limbum_mlm_ppl\": 36375.86032779106, \"test_Limbum_mlm_acc\": 4.1450777202072535, \"test_mlm_ppl\": 37743.40403168143, \"test_mlm_acc\": 3.6269430051813467}\n",
      "=====================\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Ghomala.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Ghomala.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Limbum.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Limbum.pth\n",
      "=====================\n",
      "=====================\n",
      "Bafia to Ghomala\n",
      "Bafia to Limbum\n",
      "=====================\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/30/20 16:35:58 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/30/20 16:35:58 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaLimbum --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum' --lgs 'Ghomala-Limbum' --clm_steps '' --mlm_steps 'Ghomala,Limbum' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_GhomalaLimbum\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Ghomala', 1: 'Limbum'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Ghomala': 0, 'Limbum': 1}\n",
      "                                     langs: ['Ghomala', 'Limbum']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Ghomala-Limbum']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Ghomala', None), ('Limbum', None)]\n",
      "                                     mono_dataset: {'Ghomala': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/train.Ghomala.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Ghomala.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Ghomala.pth'}, 'Limbum': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/train.Limbum.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Limbum.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Limbum.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': False, 'valid': False, 'test': False}\n",
      "                                     remove_long_sentences_test: False\n",
      "                                     remove_long_sentences_train: False\n",
      "                                     remove_long_sentences_valid: False\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/30/20 16:35:58 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml\n",
      "                                     \n",
      "INFO - 05/30/20 16:35:58 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaLimbum --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum' --lgs 'Ghomala-Limbum' --clm_steps '' --mlm_steps 'Ghomala,Limbum' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/30/20 16:35:58 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/30/20 16:35:58 - 0:00:00 - ============ langs: Ghomala, Limbum\n",
      "INFO - 05/30/20 16:35:58 - 0:00:00 - ============ Monolingual data (Ghomala)\n",
      "INFO - 05/30/20 16:35:58 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Ghomala.pth ...\n",
      "INFO - 05/30/20 16:35:58 - 0:00:00 - 552895 words (5057 unique) in 7950 sentences. 171055 unknown words (374 unique) covering 30.94% of the data.\n",
      "INFO - 05/30/20 16:35:58 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/30/20 16:35:58 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Ghomala.pth ...\n",
      "INFO - 05/30/20 16:35:58 - 0:00:00 - 552895 words (5057 unique) in 7950 sentences. 171055 unknown words (374 unique) covering 30.94% of the data.\n",
      "INFO - 05/30/20 16:35:58 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/30/20 16:35:58 - 0:00:00 - ============ Monolingual data (Limbum)\n",
      "INFO - 05/30/20 16:35:58 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Limbum.pth ...\n",
      "INFO - 05/30/20 16:35:58 - 0:00:00 - 552895 words (5057 unique) in 7950 sentences. 171055 unknown words (374 unique) covering 30.94% of the data.\n",
      "INFO - 05/30/20 16:35:58 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/30/20 16:35:58 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Limbum.pth ...\n",
      "INFO - 05/30/20 16:35:58 - 0:00:00 - 552895 words (5057 unique) in 7950 sentences. 171055 unknown words (374 unique) covering 30.94% of the data.\n",
      "INFO - 05/30/20 16:35:58 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/30/20 16:35:58 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/30/20 16:35:58 - 0:00:00 - Monolingual data   - valid -      Ghomala:      7950\n",
      "INFO - 05/30/20 16:35:58 - 0:00:00 - Monolingual data   -  test -      Ghomala:      7950\n",
      "INFO - 05/30/20 16:35:58 - 0:00:00 - Monolingual data   - valid -       Limbum:      7950\n",
      "INFO - 05/30/20 16:35:58 - 0:00:00 - Monolingual data   -  test -       Limbum:      7950\n",
      "\n",
      "INFO - 05/30/20 16:35:58 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(5057, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=5057, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/30/20 16:35:58 - 0:00:01 - Number of parameters (model): 81289153\n",
      "INFO - 05/30/20 16:36:04 - 0:00:07 - Found 0 memories.\n",
      "INFO - 05/30/20 16:36:04 - 0:00:07 - Found 6 FFN.\n",
      "INFO - 05/30/20 16:36:04 - 0:00:07 - Found 102 parameters in model.\n",
      "INFO - 05/30/20 16:36:04 - 0:00:07 - Optimizers: model\n",
      "WARNING - 05/30/20 16:36:04 - 0:00:07 - Reloading checkpoint from /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml/checkpoint.pth ...\n",
      "WARNING - 05/30/20 16:36:05 - 0:00:07 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/30/20 16:36:05 - 0:00:07 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/30/20 16:36:05 - 0:00:07 - Checkpoint reloaded. Resuming at epoch 52 / iteration 13728 ...\n",
      "INFO - 05/30/20 16:36:07 - 0:00:10 - epoch -> 52.000000\n",
      "INFO - 05/30/20 16:36:07 - 0:00:10 - valid_Ghomala_mlm_ppl -> 29464.973934\n",
      "INFO - 05/30/20 16:36:07 - 0:00:10 - valid_Ghomala_mlm_acc -> 3.626943\n",
      "INFO - 05/30/20 16:36:07 - 0:00:10 - valid_Limbum_mlm_ppl -> 31800.529833\n",
      "INFO - 05/30/20 16:36:07 - 0:00:10 - valid_Limbum_mlm_acc -> 4.404145\n",
      "INFO - 05/30/20 16:36:07 - 0:00:10 - valid_mlm_ppl -> 30632.751883\n",
      "INFO - 05/30/20 16:36:07 - 0:00:10 - valid_mlm_acc -> 4.015544\n",
      "INFO - 05/30/20 16:36:07 - 0:00:10 - test_Ghomala_mlm_ppl -> 29464.973934\n",
      "INFO - 05/30/20 16:36:07 - 0:00:10 - test_Ghomala_mlm_acc -> 3.626943\n",
      "INFO - 05/30/20 16:36:07 - 0:00:10 - test_Limbum_mlm_ppl -> 31800.529833\n",
      "INFO - 05/30/20 16:36:07 - 0:00:10 - test_Limbum_mlm_acc -> 4.404145\n",
      "INFO - 05/30/20 16:36:07 - 0:00:10 - test_mlm_ppl -> 30632.751883\n",
      "INFO - 05/30/20 16:36:07 - 0:00:10 - test_mlm_acc -> 4.015544\n",
      "INFO - 05/30/20 16:36:07 - 0:00:10 - __log__:{\"epoch\": 52, \"valid_Ghomala_mlm_ppl\": 29464.973934128007, \"valid_Ghomala_mlm_acc\": 3.626943005181347, \"valid_Limbum_mlm_ppl\": 31800.52983279711, \"valid_Limbum_mlm_acc\": 4.404145077720207, \"valid_mlm_ppl\": 30632.75188346256, \"valid_mlm_acc\": 4.015544041450777, \"test_Ghomala_mlm_ppl\": 29464.973934128007, \"test_Ghomala_mlm_acc\": 3.626943005181347, \"test_Limbum_mlm_ppl\": 31800.52983279711, \"test_Limbum_mlm_acc\": 4.404145077720207, \"test_mlm_ppl\": 30632.75188346256, \"test_mlm_acc\": 4.015544041450777}\n",
      "=====================\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Ghomala.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Ghomala.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Limbum.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Limbum.pth\n",
      "=====================\n",
      "=====================\n",
      "Bulu to Ghomala\n",
      "Bulu to Limbum\n",
      "=====================\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/30/20 16:36:08 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/30/20 16:36:08 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaLimbum --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum' --lgs 'Ghomala-Limbum' --clm_steps '' --mlm_steps 'Ghomala,Limbum' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_GhomalaLimbum\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Ghomala', 1: 'Limbum'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Ghomala': 0, 'Limbum': 1}\n",
      "                                     langs: ['Ghomala', 'Limbum']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Ghomala-Limbum']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Ghomala', None), ('Limbum', None)]\n",
      "                                     mono_dataset: {'Ghomala': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/train.Ghomala.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Ghomala.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Ghomala.pth'}, 'Limbum': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/train.Limbum.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Limbum.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Limbum.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': False, 'valid': False, 'test': False}\n",
      "                                     remove_long_sentences_test: False\n",
      "                                     remove_long_sentences_train: False\n",
      "                                     remove_long_sentences_valid: False\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/30/20 16:36:08 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml\n",
      "                                     \n",
      "INFO - 05/30/20 16:36:08 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaLimbum --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum' --lgs 'Ghomala-Limbum' --clm_steps '' --mlm_steps 'Ghomala,Limbum' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/30/20 16:36:08 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/30/20 16:36:08 - 0:00:00 - ============ langs: Ghomala, Limbum\n",
      "INFO - 05/30/20 16:36:08 - 0:00:00 - ============ Monolingual data (Ghomala)\n",
      "INFO - 05/30/20 16:36:08 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Ghomala.pth ...\n",
      "INFO - 05/30/20 16:36:08 - 0:00:00 - 379166 words (5057 unique) in 7946 sentences. 60291 unknown words (386 unique) covering 15.90% of the data.\n",
      "INFO - 05/30/20 16:36:08 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/30/20 16:36:08 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Ghomala.pth ...\n",
      "INFO - 05/30/20 16:36:08 - 0:00:00 - 379166 words (5057 unique) in 7946 sentences. 60291 unknown words (386 unique) covering 15.90% of the data.\n",
      "INFO - 05/30/20 16:36:08 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/30/20 16:36:08 - 0:00:00 - ============ Monolingual data (Limbum)\n",
      "INFO - 05/30/20 16:36:08 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Limbum.pth ...\n",
      "INFO - 05/30/20 16:36:08 - 0:00:00 - 379166 words (5057 unique) in 7946 sentences. 60291 unknown words (386 unique) covering 15.90% of the data.\n",
      "INFO - 05/30/20 16:36:08 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/30/20 16:36:08 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Limbum.pth ...\n",
      "INFO - 05/30/20 16:36:08 - 0:00:00 - 379166 words (5057 unique) in 7946 sentences. 60291 unknown words (386 unique) covering 15.90% of the data.\n",
      "INFO - 05/30/20 16:36:09 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/30/20 16:36:09 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/30/20 16:36:09 - 0:00:00 - Monolingual data   - valid -      Ghomala:      7946\n",
      "INFO - 05/30/20 16:36:09 - 0:00:00 - Monolingual data   -  test -      Ghomala:      7946\n",
      "INFO - 05/30/20 16:36:09 - 0:00:00 - Monolingual data   - valid -       Limbum:      7946\n",
      "INFO - 05/30/20 16:36:09 - 0:00:00 - Monolingual data   -  test -       Limbum:      7946\n",
      "\n",
      "INFO - 05/30/20 16:36:09 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(5057, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=5057, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/30/20 16:36:09 - 0:00:01 - Number of parameters (model): 81289153\n",
      "INFO - 05/30/20 16:36:15 - 0:00:06 - Found 0 memories.\n",
      "INFO - 05/30/20 16:36:15 - 0:00:06 - Found 6 FFN.\n",
      "INFO - 05/30/20 16:36:15 - 0:00:06 - Found 102 parameters in model.\n",
      "INFO - 05/30/20 16:36:15 - 0:00:06 - Optimizers: model\n",
      "WARNING - 05/30/20 16:36:15 - 0:00:06 - Reloading checkpoint from /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml/checkpoint.pth ...\n",
      "WARNING - 05/30/20 16:36:16 - 0:00:07 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/30/20 16:36:16 - 0:00:07 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/30/20 16:36:16 - 0:00:07 - Checkpoint reloaded. Resuming at epoch 52 / iteration 13728 ...\n",
      "INFO - 05/30/20 16:36:18 - 0:00:10 - epoch -> 52.000000\n",
      "INFO - 05/30/20 16:36:18 - 0:00:10 - valid_Ghomala_mlm_ppl -> 10406.330025\n",
      "INFO - 05/30/20 16:36:18 - 0:00:10 - valid_Ghomala_mlm_acc -> 12.694301\n",
      "INFO - 05/30/20 16:36:18 - 0:00:10 - valid_Limbum_mlm_ppl -> 12645.012128\n",
      "INFO - 05/30/20 16:36:18 - 0:00:10 - valid_Limbum_mlm_acc -> 12.694301\n",
      "INFO - 05/30/20 16:36:18 - 0:00:10 - valid_mlm_ppl -> 11525.671076\n",
      "INFO - 05/30/20 16:36:18 - 0:00:10 - valid_mlm_acc -> 12.694301\n",
      "INFO - 05/30/20 16:36:18 - 0:00:10 - test_Ghomala_mlm_ppl -> 10406.330025\n",
      "INFO - 05/30/20 16:36:18 - 0:00:10 - test_Ghomala_mlm_acc -> 12.694301\n",
      "INFO - 05/30/20 16:36:18 - 0:00:10 - test_Limbum_mlm_ppl -> 12645.012128\n",
      "INFO - 05/30/20 16:36:18 - 0:00:10 - test_Limbum_mlm_acc -> 12.694301\n",
      "INFO - 05/30/20 16:36:18 - 0:00:10 - test_mlm_ppl -> 11525.671076\n",
      "INFO - 05/30/20 16:36:18 - 0:00:10 - test_mlm_acc -> 12.694301\n",
      "INFO - 05/30/20 16:36:18 - 0:00:10 - __log__:{\"epoch\": 52, \"valid_Ghomala_mlm_ppl\": 10406.330024977324, \"valid_Ghomala_mlm_acc\": 12.694300518134716, \"valid_Limbum_mlm_ppl\": 12645.012128007043, \"valid_Limbum_mlm_acc\": 12.694300518134716, \"valid_mlm_ppl\": 11525.671076492185, \"valid_mlm_acc\": 12.694300518134716, \"test_Ghomala_mlm_ppl\": 10406.330024977324, \"test_Ghomala_mlm_acc\": 12.694300518134716, \"test_Limbum_mlm_ppl\": 12645.012128007043, \"test_Limbum_mlm_acc\": 12.694300518134716, \"test_mlm_ppl\": 11525.671076492185, \"test_mlm_acc\": 12.694300518134716}\n",
      "=====================\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Ghomala.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Ghomala.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Limbum.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Limbum.pth\n",
      "=====================\n",
      "=====================\n",
      "MKPAMAN_AMVOE_Ewondo to Ghomala\n",
      "MKPAMAN_AMVOE_Ewondo to Limbum\n",
      "=====================\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/30/20 16:36:19 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/30/20 16:36:19 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaLimbum --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum' --lgs 'Ghomala-Limbum' --clm_steps '' --mlm_steps 'Ghomala,Limbum' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_GhomalaLimbum\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Ghomala', 1: 'Limbum'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Ghomala': 0, 'Limbum': 1}\n",
      "                                     langs: ['Ghomala', 'Limbum']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Ghomala-Limbum']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Ghomala', None), ('Limbum', None)]\n",
      "                                     mono_dataset: {'Ghomala': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/train.Ghomala.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Ghomala.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Ghomala.pth'}, 'Limbum': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/train.Limbum.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Limbum.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Limbum.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': False, 'valid': False, 'test': False}\n",
      "                                     remove_long_sentences_test: False\n",
      "                                     remove_long_sentences_train: False\n",
      "                                     remove_long_sentences_valid: False\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/30/20 16:36:19 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml\n",
      "                                     \n",
      "INFO - 05/30/20 16:36:19 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaLimbum --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum' --lgs 'Ghomala-Limbum' --clm_steps '' --mlm_steps 'Ghomala,Limbum' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/30/20 16:36:19 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/30/20 16:36:19 - 0:00:00 - ============ langs: Ghomala, Limbum\n",
      "INFO - 05/30/20 16:36:19 - 0:00:00 - ============ Monolingual data (Ghomala)\n",
      "INFO - 05/30/20 16:36:19 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Ghomala.pth ...\n",
      "INFO - 05/30/20 16:36:19 - 0:00:00 - 351765 words (5057 unique) in 7944 sentences. 62708 unknown words (397 unique) covering 17.83% of the data.\n",
      "INFO - 05/30/20 16:36:19 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/30/20 16:36:19 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Ghomala.pth ...\n",
      "INFO - 05/30/20 16:36:19 - 0:00:00 - 351765 words (5057 unique) in 7944 sentences. 62708 unknown words (397 unique) covering 17.83% of the data.\n",
      "INFO - 05/30/20 16:36:19 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/30/20 16:36:19 - 0:00:00 - ============ Monolingual data (Limbum)\n",
      "INFO - 05/30/20 16:36:19 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Limbum.pth ...\n",
      "INFO - 05/30/20 16:36:19 - 0:00:00 - 351765 words (5057 unique) in 7944 sentences. 62708 unknown words (397 unique) covering 17.83% of the data.\n",
      "INFO - 05/30/20 16:36:19 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/30/20 16:36:19 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Limbum.pth ...\n",
      "INFO - 05/30/20 16:36:19 - 0:00:00 - 351765 words (5057 unique) in 7944 sentences. 62708 unknown words (397 unique) covering 17.83% of the data.\n",
      "INFO - 05/30/20 16:36:19 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/30/20 16:36:19 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/30/20 16:36:19 - 0:00:00 - Monolingual data   - valid -      Ghomala:      7944\n",
      "INFO - 05/30/20 16:36:19 - 0:00:00 - Monolingual data   -  test -      Ghomala:      7944\n",
      "INFO - 05/30/20 16:36:19 - 0:00:00 - Monolingual data   - valid -       Limbum:      7944\n",
      "INFO - 05/30/20 16:36:19 - 0:00:00 - Monolingual data   -  test -       Limbum:      7944\n",
      "\n",
      "INFO - 05/30/20 16:36:20 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(5057, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=5057, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/30/20 16:36:20 - 0:00:01 - Number of parameters (model): 81289153\n",
      "INFO - 05/30/20 16:36:26 - 0:00:06 - Found 0 memories.\n",
      "INFO - 05/30/20 16:36:26 - 0:00:06 - Found 6 FFN.\n",
      "INFO - 05/30/20 16:36:26 - 0:00:06 - Found 102 parameters in model.\n",
      "INFO - 05/30/20 16:36:26 - 0:00:06 - Optimizers: model\n",
      "WARNING - 05/30/20 16:36:26 - 0:00:06 - Reloading checkpoint from /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml/checkpoint.pth ...\n",
      "WARNING - 05/30/20 16:36:26 - 0:00:07 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/30/20 16:36:26 - 0:00:07 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/30/20 16:36:26 - 0:00:07 - Checkpoint reloaded. Resuming at epoch 52 / iteration 13728 ...\n",
      "INFO - 05/30/20 16:36:29 - 0:00:10 - epoch -> 52.000000\n",
      "INFO - 05/30/20 16:36:29 - 0:00:10 - valid_Ghomala_mlm_ppl -> 8813.300203\n",
      "INFO - 05/30/20 16:36:29 - 0:00:10 - valid_Ghomala_mlm_acc -> 11.917098\n",
      "INFO - 05/30/20 16:36:29 - 0:00:10 - valid_Limbum_mlm_ppl -> 10185.249968\n",
      "INFO - 05/30/20 16:36:29 - 0:00:10 - valid_Limbum_mlm_acc -> 12.435233\n",
      "INFO - 05/30/20 16:36:29 - 0:00:10 - valid_mlm_ppl -> 9499.275086\n",
      "INFO - 05/30/20 16:36:29 - 0:00:10 - valid_mlm_acc -> 12.176166\n",
      "INFO - 05/30/20 16:36:29 - 0:00:10 - test_Ghomala_mlm_ppl -> 8813.300203\n",
      "INFO - 05/30/20 16:36:29 - 0:00:10 - test_Ghomala_mlm_acc -> 11.917098\n",
      "INFO - 05/30/20 16:36:29 - 0:00:10 - test_Limbum_mlm_ppl -> 10185.249968\n",
      "INFO - 05/30/20 16:36:29 - 0:00:10 - test_Limbum_mlm_acc -> 12.435233\n",
      "INFO - 05/30/20 16:36:29 - 0:00:10 - test_mlm_ppl -> 9499.275086\n",
      "INFO - 05/30/20 16:36:29 - 0:00:10 - test_mlm_acc -> 12.176166\n",
      "INFO - 05/30/20 16:36:29 - 0:00:10 - __log__:{\"epoch\": 52, \"valid_Ghomala_mlm_ppl\": 8813.300203123761, \"valid_Ghomala_mlm_acc\": 11.917098445595855, \"valid_Limbum_mlm_ppl\": 10185.249968261569, \"valid_Limbum_mlm_acc\": 12.435233160621761, \"valid_mlm_ppl\": 9499.275085692665, \"valid_mlm_acc\": 12.176165803108809, \"test_Ghomala_mlm_ppl\": 8813.300203123761, \"test_Ghomala_mlm_acc\": 11.917098445595855, \"test_Limbum_mlm_ppl\": 10185.249968261569, \"test_Limbum_mlm_acc\": 12.435233160621761, \"test_mlm_ppl\": 9499.275085692665, \"test_mlm_acc\": 12.176165803108809}\n",
      "=====================\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Ghomala.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Ghomala.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Limbum.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Limbum.pth\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "%env dump_path=/home/jupyter/models/africa/cluster3\n",
    "%env exp_name=mlm_tlm_GhomalaLimbum\n",
    "%env data_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum\n",
    "%env lgs=Ghomala-Limbum\n",
    "%env mlm_steps=Ghomala,Limbum\n",
    "%env tgt_pair=Ghomala-Limbum\n",
    "%env src_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum\n",
    "%env tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum\n",
    "\n",
    "# creation of the dummy files so that the experiment does not bug\n",
    "! touch /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/train.Ghomala.pth\n",
    "! touch /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/train.Limbum.pth\n",
    "\n",
    "###### Ghomala_Limbum vs Ngiemboon\n",
    "! ../duplicate.sh $src_path $tgt_path Ngiemboon $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : delete the rename file\n",
    "! ../delete.sh $tgt_path $tgt_pair\n",
    "\n",
    "###### Ghomala_Limbum vs Bafia\n",
    "! ../duplicate.sh $src_path $tgt_path Bafia $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : delete the rename file\n",
    "! ../delete.sh $tgt_path $tgt_pair\n",
    "\n",
    "###### Ghomala_Limbum vs Bulu\n",
    "! ../duplicate.sh $src_path $tgt_path Bulu $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : delete the rename file\n",
    "! ../delete.sh $tgt_path $tgt_pair\n",
    "\n",
    "###### Ghomala_Limbum vs Ewondo\n",
    "! ../duplicate.sh $src_path $tgt_path MKPAMAN_AMVOE_Ewondo $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : delete the rename file\n",
    "! ../delete.sh $tgt_path $tgt_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ghomala_Ngiemboon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: dump_path=/home/jupyter/models/africa/cluster3\n",
      "env: exp_name=mlm_tlm_GhomalaNgiemboon\n",
      "env: data_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon\n",
      "env: lgs=Ghomala-Ngiemboon\n",
      "env: mlm_steps=Ghomala,Ngiemboon\n",
      "env: tgt_pair=Ghomala-Ngiemboon\n",
      "env: src_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon\n",
      "env: tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon\n",
      "=====================\n",
      "Limbum to Ghomala\n",
      "Limbum to Ngiemboon\n",
      "=====================\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/30/20 16:37:46 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/30/20 16:37:46 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon' --lgs 'Ghomala-Ngiemboon' --clm_steps '' --mlm_steps 'Ghomala,Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_GhomalaNgiemboon\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Ghomala', 1: 'Ngiemboon'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Ghomala': 0, 'Ngiemboon': 1}\n",
      "                                     langs: ['Ghomala', 'Ngiemboon']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Ghomala-Ngiemboon']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Ghomala', None), ('Ngiemboon', None)]\n",
      "                                     mono_dataset: {'Ghomala': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/train.Ghomala.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ghomala.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ghomala.pth'}, 'Ngiemboon': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/train.Ngiemboon.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ngiemboon.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ngiemboon.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': False, 'valid': False, 'test': False}\n",
      "                                     remove_long_sentences_test: False\n",
      "                                     remove_long_sentences_train: False\n",
      "                                     remove_long_sentences_valid: False\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/30/20 16:37:46 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml\n",
      "                                     \n",
      "INFO - 05/30/20 16:37:46 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon' --lgs 'Ghomala-Ngiemboon' --clm_steps '' --mlm_steps 'Ghomala,Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/30/20 16:37:46 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/30/20 16:37:46 - 0:00:00 - ============ langs: Ghomala, Ngiemboon\n",
      "INFO - 05/30/20 16:37:46 - 0:00:00 - ============ Monolingual data (Ghomala)\n",
      "INFO - 05/30/20 16:37:46 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ghomala.pth ...\n",
      "INFO - 05/30/20 16:37:46 - 0:00:00 - 578283 words (6715 unique) in 7919 sentences. 2848 unknown words (46 unique) covering 0.49% of the data.\n",
      "INFO - 05/30/20 16:37:46 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/30/20 16:37:46 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ghomala.pth ...\n",
      "INFO - 05/30/20 16:37:46 - 0:00:00 - 578283 words (6715 unique) in 7919 sentences. 2848 unknown words (46 unique) covering 0.49% of the data.\n",
      "INFO - 05/30/20 16:37:46 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/30/20 16:37:46 - 0:00:00 - ============ Monolingual data (Ngiemboon)\n",
      "INFO - 05/30/20 16:37:46 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ngiemboon.pth ...\n",
      "INFO - 05/30/20 16:37:46 - 0:00:00 - 578283 words (6715 unique) in 7919 sentences. 2848 unknown words (46 unique) covering 0.49% of the data.\n",
      "INFO - 05/30/20 16:37:46 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/30/20 16:37:46 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ngiemboon.pth ...\n",
      "INFO - 05/30/20 16:37:46 - 0:00:00 - 578283 words (6715 unique) in 7919 sentences. 2848 unknown words (46 unique) covering 0.49% of the data.\n",
      "INFO - 05/30/20 16:37:46 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/30/20 16:37:46 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/30/20 16:37:46 - 0:00:00 - Monolingual data   - valid -      Ghomala:      7919\n",
      "INFO - 05/30/20 16:37:46 - 0:00:00 - Monolingual data   -  test -      Ghomala:      7919\n",
      "INFO - 05/30/20 16:37:46 - 0:00:00 - Monolingual data   - valid -    Ngiemboon:      7919\n",
      "INFO - 05/30/20 16:37:46 - 0:00:00 - Monolingual data   -  test -    Ngiemboon:      7919\n",
      "\n",
      "INFO - 05/30/20 16:37:47 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(6715, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=6715, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/30/20 16:37:47 - 0:00:01 - Number of parameters (model): 82988603\n",
      "INFO - 05/30/20 16:37:53 - 0:00:06 - Found 0 memories.\n",
      "INFO - 05/30/20 16:37:53 - 0:00:06 - Found 6 FFN.\n",
      "INFO - 05/30/20 16:37:53 - 0:00:06 - Found 102 parameters in model.\n",
      "INFO - 05/30/20 16:37:53 - 0:00:06 - Optimizers: model\n",
      "WARNING - 05/30/20 16:37:53 - 0:00:06 - Reloading checkpoint from /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml/checkpoint.pth ...\n",
      "WARNING - 05/30/20 16:37:53 - 0:00:07 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/30/20 16:37:53 - 0:00:07 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/30/20 16:37:53 - 0:00:07 - Checkpoint reloaded. Resuming at epoch 40 / iteration 10600 ...\n",
      "INFO - 05/30/20 16:37:55 - 0:00:09 - epoch -> 40.000000\n",
      "INFO - 05/30/20 16:37:55 - 0:00:09 - valid_Ghomala_mlm_ppl -> 3893.065548\n",
      "INFO - 05/30/20 16:37:55 - 0:00:09 - valid_Ghomala_mlm_acc -> 5.181347\n",
      "INFO - 05/30/20 16:37:55 - 0:00:09 - valid_Ngiemboon_mlm_ppl -> 6174.332449\n",
      "INFO - 05/30/20 16:37:55 - 0:00:09 - valid_Ngiemboon_mlm_acc -> 3.886010\n",
      "INFO - 05/30/20 16:37:55 - 0:00:09 - valid_mlm_ppl -> 5033.698998\n",
      "INFO - 05/30/20 16:37:55 - 0:00:09 - valid_mlm_acc -> 4.533679\n",
      "INFO - 05/30/20 16:37:55 - 0:00:09 - test_Ghomala_mlm_ppl -> 3893.065548\n",
      "INFO - 05/30/20 16:37:55 - 0:00:09 - test_Ghomala_mlm_acc -> 5.181347\n",
      "INFO - 05/30/20 16:37:55 - 0:00:09 - test_Ngiemboon_mlm_ppl -> 6174.332449\n",
      "INFO - 05/30/20 16:37:55 - 0:00:09 - test_Ngiemboon_mlm_acc -> 3.886010\n",
      "INFO - 05/30/20 16:37:55 - 0:00:09 - test_mlm_ppl -> 5033.698998\n",
      "INFO - 05/30/20 16:37:55 - 0:00:09 - test_mlm_acc -> 4.533679\n",
      "INFO - 05/30/20 16:37:55 - 0:00:09 - __log__:{\"epoch\": 40, \"valid_Ghomala_mlm_ppl\": 3893.065547790979, \"valid_Ghomala_mlm_acc\": 5.181347150259067, \"valid_Ngiemboon_mlm_ppl\": 6174.332449153792, \"valid_Ngiemboon_mlm_acc\": 3.8860103626943006, \"valid_mlm_ppl\": 5033.698998472386, \"valid_mlm_acc\": 4.533678756476684, \"test_Ghomala_mlm_ppl\": 3893.065547790979, \"test_Ghomala_mlm_acc\": 5.181347150259067, \"test_Ngiemboon_mlm_ppl\": 6174.332449153792, \"test_Ngiemboon_mlm_acc\": 3.8860103626943006, \"test_mlm_ppl\": 5033.698998472386, \"test_mlm_acc\": 4.533678756476684}\n",
      "=====================\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ghomala.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ghomala.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ngiemboon.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ngiemboon.pth\n",
      "=====================\n",
      "=====================\n",
      "Bafia to Ghomala\n",
      "Bafia to Ngiemboon\n",
      "=====================\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/30/20 16:37:56 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/30/20 16:37:56 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon' --lgs 'Ghomala-Ngiemboon' --clm_steps '' --mlm_steps 'Ghomala,Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_GhomalaNgiemboon\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Ghomala', 1: 'Ngiemboon'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Ghomala': 0, 'Ngiemboon': 1}\n",
      "                                     langs: ['Ghomala', 'Ngiemboon']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Ghomala-Ngiemboon']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Ghomala', None), ('Ngiemboon', None)]\n",
      "                                     mono_dataset: {'Ghomala': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/train.Ghomala.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ghomala.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ghomala.pth'}, 'Ngiemboon': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/train.Ngiemboon.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ngiemboon.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ngiemboon.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': False, 'valid': False, 'test': False}\n",
      "                                     remove_long_sentences_test: False\n",
      "                                     remove_long_sentences_train: False\n",
      "                                     remove_long_sentences_valid: False\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/30/20 16:37:56 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml\n",
      "                                     \n",
      "INFO - 05/30/20 16:37:56 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon' --lgs 'Ghomala-Ngiemboon' --clm_steps '' --mlm_steps 'Ghomala,Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/30/20 16:37:56 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/30/20 16:37:56 - 0:00:00 - ============ langs: Ghomala, Ngiemboon\n",
      "INFO - 05/30/20 16:37:56 - 0:00:00 - ============ Monolingual data (Ghomala)\n",
      "INFO - 05/30/20 16:37:56 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ghomala.pth ...\n",
      "INFO - 05/30/20 16:37:56 - 0:00:00 - 613668 words (6715 unique) in 7950 sentences. 81452 unknown words (70 unique) covering 13.27% of the data.\n",
      "INFO - 05/30/20 16:37:56 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/30/20 16:37:56 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ghomala.pth ...\n",
      "INFO - 05/30/20 16:37:56 - 0:00:00 - 613668 words (6715 unique) in 7950 sentences. 81452 unknown words (70 unique) covering 13.27% of the data.\n",
      "INFO - 05/30/20 16:37:56 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/30/20 16:37:56 - 0:00:00 - ============ Monolingual data (Ngiemboon)\n",
      "INFO - 05/30/20 16:37:56 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ngiemboon.pth ...\n",
      "INFO - 05/30/20 16:37:56 - 0:00:00 - 613668 words (6715 unique) in 7950 sentences. 81452 unknown words (70 unique) covering 13.27% of the data.\n",
      "INFO - 05/30/20 16:37:56 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/30/20 16:37:56 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ngiemboon.pth ...\n",
      "INFO - 05/30/20 16:37:56 - 0:00:00 - 613668 words (6715 unique) in 7950 sentences. 81452 unknown words (70 unique) covering 13.27% of the data.\n",
      "INFO - 05/30/20 16:37:56 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/30/20 16:37:56 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/30/20 16:37:56 - 0:00:00 - Monolingual data   - valid -      Ghomala:      7950\n",
      "INFO - 05/30/20 16:37:56 - 0:00:00 - Monolingual data   -  test -      Ghomala:      7950\n",
      "INFO - 05/30/20 16:37:56 - 0:00:00 - Monolingual data   - valid -    Ngiemboon:      7950\n",
      "INFO - 05/30/20 16:37:56 - 0:00:00 - Monolingual data   -  test -    Ngiemboon:      7950\n",
      "\n",
      "INFO - 05/30/20 16:37:57 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(6715, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=6715, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/30/20 16:37:57 - 0:00:01 - Number of parameters (model): 82988603\n",
      "INFO - 05/30/20 16:38:03 - 0:00:07 - Found 0 memories.\n",
      "INFO - 05/30/20 16:38:03 - 0:00:07 - Found 6 FFN.\n",
      "INFO - 05/30/20 16:38:03 - 0:00:07 - Found 102 parameters in model.\n",
      "INFO - 05/30/20 16:38:03 - 0:00:07 - Optimizers: model\n",
      "WARNING - 05/30/20 16:38:03 - 0:00:07 - Reloading checkpoint from /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml/checkpoint.pth ...\n",
      "WARNING - 05/30/20 16:38:04 - 0:00:07 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/30/20 16:38:04 - 0:00:07 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/30/20 16:38:04 - 0:00:07 - Checkpoint reloaded. Resuming at epoch 40 / iteration 10600 ...\n",
      "INFO - 05/30/20 16:38:07 - 0:00:10 - epoch -> 40.000000\n",
      "INFO - 05/30/20 16:38:07 - 0:00:10 - valid_Ghomala_mlm_ppl -> 4181.634359\n",
      "INFO - 05/30/20 16:38:07 - 0:00:10 - valid_Ghomala_mlm_acc -> 12.176166\n",
      "INFO - 05/30/20 16:38:07 - 0:00:10 - valid_Ngiemboon_mlm_ppl -> 8395.345568\n",
      "INFO - 05/30/20 16:38:07 - 0:00:10 - valid_Ngiemboon_mlm_acc -> 9.844560\n",
      "INFO - 05/30/20 16:38:07 - 0:00:10 - valid_mlm_ppl -> 6288.489964\n",
      "INFO - 05/30/20 16:38:07 - 0:00:10 - valid_mlm_acc -> 11.010363\n",
      "INFO - 05/30/20 16:38:07 - 0:00:10 - test_Ghomala_mlm_ppl -> 4181.634359\n",
      "INFO - 05/30/20 16:38:07 - 0:00:10 - test_Ghomala_mlm_acc -> 12.176166\n",
      "INFO - 05/30/20 16:38:07 - 0:00:10 - test_Ngiemboon_mlm_ppl -> 8395.345568\n",
      "INFO - 05/30/20 16:38:07 - 0:00:10 - test_Ngiemboon_mlm_acc -> 9.844560\n",
      "INFO - 05/30/20 16:38:07 - 0:00:10 - test_mlm_ppl -> 6288.489964\n",
      "INFO - 05/30/20 16:38:07 - 0:00:10 - test_mlm_acc -> 11.010363\n",
      "INFO - 05/30/20 16:38:07 - 0:00:10 - __log__:{\"epoch\": 40, \"valid_Ghomala_mlm_ppl\": 4181.634359397054, \"valid_Ghomala_mlm_acc\": 12.176165803108809, \"valid_Ngiemboon_mlm_ppl\": 8395.345567604116, \"valid_Ngiemboon_mlm_acc\": 9.844559585492227, \"valid_mlm_ppl\": 6288.489963500585, \"valid_mlm_acc\": 11.010362694300518, \"test_Ghomala_mlm_ppl\": 4181.634359397054, \"test_Ghomala_mlm_acc\": 12.176165803108809, \"test_Ngiemboon_mlm_ppl\": 8395.345567604116, \"test_Ngiemboon_mlm_acc\": 9.844559585492227, \"test_mlm_ppl\": 6288.489963500585, \"test_mlm_acc\": 11.010362694300518}\n",
      "=====================\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ghomala.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ghomala.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ngiemboon.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ngiemboon.pth\n",
      "=====================\n",
      "=====================\n",
      "Bulu to Ghomala\n",
      "Bulu to Ngiemboon\n",
      "=====================\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/30/20 16:38:08 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/30/20 16:38:08 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon' --lgs 'Ghomala-Ngiemboon' --clm_steps '' --mlm_steps 'Ghomala,Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_GhomalaNgiemboon\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Ghomala', 1: 'Ngiemboon'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Ghomala': 0, 'Ngiemboon': 1}\n",
      "                                     langs: ['Ghomala', 'Ngiemboon']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Ghomala-Ngiemboon']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Ghomala', None), ('Ngiemboon', None)]\n",
      "                                     mono_dataset: {'Ghomala': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/train.Ghomala.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ghomala.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ghomala.pth'}, 'Ngiemboon': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/train.Ngiemboon.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ngiemboon.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ngiemboon.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': False, 'valid': False, 'test': False}\n",
      "                                     remove_long_sentences_test: False\n",
      "                                     remove_long_sentences_train: False\n",
      "                                     remove_long_sentences_valid: False\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/30/20 16:38:08 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml\n",
      "                                     \n",
      "INFO - 05/30/20 16:38:08 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon' --lgs 'Ghomala-Ngiemboon' --clm_steps '' --mlm_steps 'Ghomala,Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/30/20 16:38:08 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/30/20 16:38:08 - 0:00:00 - ============ langs: Ghomala, Ngiemboon\n",
      "INFO - 05/30/20 16:38:08 - 0:00:00 - ============ Monolingual data (Ghomala)\n",
      "INFO - 05/30/20 16:38:08 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ghomala.pth ...\n",
      "INFO - 05/30/20 16:38:08 - 0:00:00 - 430245 words (6715 unique) in 7946 sentences. 15757 unknown words (86 unique) covering 3.66% of the data.\n",
      "INFO - 05/30/20 16:38:08 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/30/20 16:38:08 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ghomala.pth ...\n",
      "INFO - 05/30/20 16:38:08 - 0:00:00 - 430245 words (6715 unique) in 7946 sentences. 15757 unknown words (86 unique) covering 3.66% of the data.\n",
      "INFO - 05/30/20 16:38:08 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/30/20 16:38:08 - 0:00:00 - ============ Monolingual data (Ngiemboon)\n",
      "INFO - 05/30/20 16:38:08 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ngiemboon.pth ...\n",
      "INFO - 05/30/20 16:38:08 - 0:00:00 - 430245 words (6715 unique) in 7946 sentences. 15757 unknown words (86 unique) covering 3.66% of the data.\n",
      "INFO - 05/30/20 16:38:08 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/30/20 16:38:08 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ngiemboon.pth ...\n",
      "INFO - 05/30/20 16:38:08 - 0:00:00 - 430245 words (6715 unique) in 7946 sentences. 15757 unknown words (86 unique) covering 3.66% of the data.\n",
      "INFO - 05/30/20 16:38:08 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/30/20 16:38:08 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/30/20 16:38:08 - 0:00:00 - Monolingual data   - valid -      Ghomala:      7946\n",
      "INFO - 05/30/20 16:38:08 - 0:00:00 - Monolingual data   -  test -      Ghomala:      7946\n",
      "INFO - 05/30/20 16:38:08 - 0:00:00 - Monolingual data   - valid -    Ngiemboon:      7946\n",
      "INFO - 05/30/20 16:38:08 - 0:00:00 - Monolingual data   -  test -    Ngiemboon:      7946\n",
      "\n",
      "INFO - 05/30/20 16:38:09 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(6715, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=6715, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/30/20 16:38:09 - 0:00:01 - Number of parameters (model): 82988603\n",
      "INFO - 05/30/20 16:38:14 - 0:00:06 - Found 0 memories.\n",
      "INFO - 05/30/20 16:38:14 - 0:00:06 - Found 6 FFN.\n",
      "INFO - 05/30/20 16:38:14 - 0:00:06 - Found 102 parameters in model.\n",
      "INFO - 05/30/20 16:38:14 - 0:00:06 - Optimizers: model\n",
      "WARNING - 05/30/20 16:38:14 - 0:00:06 - Reloading checkpoint from /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml/checkpoint.pth ...\n",
      "WARNING - 05/30/20 16:38:15 - 0:00:07 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/30/20 16:38:15 - 0:00:07 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/30/20 16:38:15 - 0:00:07 - Checkpoint reloaded. Resuming at epoch 40 / iteration 10600 ...\n",
      "INFO - 05/30/20 16:38:17 - 0:00:09 - epoch -> 40.000000\n",
      "INFO - 05/30/20 16:38:17 - 0:00:09 - valid_Ghomala_mlm_ppl -> 2653.361936\n",
      "INFO - 05/30/20 16:38:17 - 0:00:09 - valid_Ghomala_mlm_acc -> 8.031088\n",
      "INFO - 05/30/20 16:38:17 - 0:00:09 - valid_Ngiemboon_mlm_ppl -> 4331.512144\n",
      "INFO - 05/30/20 16:38:17 - 0:00:09 - valid_Ngiemboon_mlm_acc -> 8.031088\n",
      "INFO - 05/30/20 16:38:17 - 0:00:09 - valid_mlm_ppl -> 3492.437040\n",
      "INFO - 05/30/20 16:38:17 - 0:00:09 - valid_mlm_acc -> 8.031088\n",
      "INFO - 05/30/20 16:38:17 - 0:00:09 - test_Ghomala_mlm_ppl -> 2653.361936\n",
      "INFO - 05/30/20 16:38:17 - 0:00:09 - test_Ghomala_mlm_acc -> 8.031088\n",
      "INFO - 05/30/20 16:38:17 - 0:00:09 - test_Ngiemboon_mlm_ppl -> 4331.512144\n",
      "INFO - 05/30/20 16:38:17 - 0:00:09 - test_Ngiemboon_mlm_acc -> 8.031088\n",
      "INFO - 05/30/20 16:38:17 - 0:00:09 - test_mlm_ppl -> 3492.437040\n",
      "INFO - 05/30/20 16:38:17 - 0:00:09 - test_mlm_acc -> 8.031088\n",
      "INFO - 05/30/20 16:38:17 - 0:00:09 - __log__:{\"epoch\": 40, \"valid_Ghomala_mlm_ppl\": 2653.3619356981044, \"valid_Ghomala_mlm_acc\": 8.031088082901555, \"valid_Ngiemboon_mlm_ppl\": 4331.51214429967, \"valid_Ngiemboon_mlm_acc\": 8.031088082901555, \"valid_mlm_ppl\": 3492.437039998887, \"valid_mlm_acc\": 8.031088082901555, \"test_Ghomala_mlm_ppl\": 2653.3619356981044, \"test_Ghomala_mlm_acc\": 8.031088082901555, \"test_Ngiemboon_mlm_ppl\": 4331.51214429967, \"test_Ngiemboon_mlm_acc\": 8.031088082901555, \"test_mlm_ppl\": 3492.437039998887, \"test_mlm_acc\": 8.031088082901555}\n",
      "=====================\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ghomala.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ghomala.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ngiemboon.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ngiemboon.pth\n",
      "=====================\n",
      "=====================\n",
      "MKPAMAN_AMVOE_Ewondo to Ghomala\n",
      "MKPAMAN_AMVOE_Ewondo to Ngiemboon\n",
      "=====================\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/30/20 16:38:18 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/30/20 16:38:18 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon' --lgs 'Ghomala-Ngiemboon' --clm_steps '' --mlm_steps 'Ghomala,Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_GhomalaNgiemboon\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Ghomala', 1: 'Ngiemboon'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Ghomala': 0, 'Ngiemboon': 1}\n",
      "                                     langs: ['Ghomala', 'Ngiemboon']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Ghomala-Ngiemboon']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Ghomala', None), ('Ngiemboon', None)]\n",
      "                                     mono_dataset: {'Ghomala': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/train.Ghomala.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ghomala.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ghomala.pth'}, 'Ngiemboon': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/train.Ngiemboon.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ngiemboon.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ngiemboon.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': False, 'valid': False, 'test': False}\n",
      "                                     remove_long_sentences_test: False\n",
      "                                     remove_long_sentences_train: False\n",
      "                                     remove_long_sentences_valid: False\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/30/20 16:38:18 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml\n",
      "                                     \n",
      "INFO - 05/30/20 16:38:18 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon' --lgs 'Ghomala-Ngiemboon' --clm_steps '' --mlm_steps 'Ghomala,Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/30/20 16:38:18 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/30/20 16:38:18 - 0:00:00 - ============ langs: Ghomala, Ngiemboon\n",
      "INFO - 05/30/20 16:38:18 - 0:00:00 - ============ Monolingual data (Ghomala)\n",
      "INFO - 05/30/20 16:38:18 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ghomala.pth ...\n",
      "INFO - 05/30/20 16:38:18 - 0:00:00 - 385319 words (6715 unique) in 7944 sentences. 11543 unknown words (72 unique) covering 3.00% of the data.\n",
      "INFO - 05/30/20 16:38:18 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/30/20 16:38:18 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ghomala.pth ...\n",
      "INFO - 05/30/20 16:38:18 - 0:00:00 - 385319 words (6715 unique) in 7944 sentences. 11543 unknown words (72 unique) covering 3.00% of the data.\n",
      "INFO - 05/30/20 16:38:18 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/30/20 16:38:18 - 0:00:00 - ============ Monolingual data (Ngiemboon)\n",
      "INFO - 05/30/20 16:38:18 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ngiemboon.pth ...\n",
      "INFO - 05/30/20 16:38:18 - 0:00:00 - 385319 words (6715 unique) in 7944 sentences. 11543 unknown words (72 unique) covering 3.00% of the data.\n",
      "INFO - 05/30/20 16:38:18 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/30/20 16:38:18 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ngiemboon.pth ...\n",
      "INFO - 05/30/20 16:38:18 - 0:00:00 - 385319 words (6715 unique) in 7944 sentences. 11543 unknown words (72 unique) covering 3.00% of the data.\n",
      "INFO - 05/30/20 16:38:18 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/30/20 16:38:18 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/30/20 16:38:18 - 0:00:00 - Monolingual data   - valid -      Ghomala:      7944\n",
      "INFO - 05/30/20 16:38:18 - 0:00:00 - Monolingual data   -  test -      Ghomala:      7944\n",
      "INFO - 05/30/20 16:38:18 - 0:00:00 - Monolingual data   - valid -    Ngiemboon:      7944\n",
      "INFO - 05/30/20 16:38:18 - 0:00:00 - Monolingual data   -  test -    Ngiemboon:      7944\n",
      "\n",
      "INFO - 05/30/20 16:38:19 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(6715, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=6715, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/30/20 16:38:19 - 0:00:01 - Number of parameters (model): 82988603\n",
      "INFO - 05/30/20 16:38:25 - 0:00:07 - Found 0 memories.\n",
      "INFO - 05/30/20 16:38:25 - 0:00:07 - Found 6 FFN.\n",
      "INFO - 05/30/20 16:38:25 - 0:00:07 - Found 102 parameters in model.\n",
      "INFO - 05/30/20 16:38:25 - 0:00:07 - Optimizers: model\n",
      "WARNING - 05/30/20 16:38:25 - 0:00:07 - Reloading checkpoint from /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml/checkpoint.pth ...\n",
      "WARNING - 05/30/20 16:38:26 - 0:00:08 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/30/20 16:38:26 - 0:00:08 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/30/20 16:38:26 - 0:00:08 - Checkpoint reloaded. Resuming at epoch 40 / iteration 10600 ...\n",
      "INFO - 05/30/20 16:38:28 - 0:00:10 - epoch -> 40.000000\n",
      "INFO - 05/30/20 16:38:28 - 0:00:10 - valid_Ghomala_mlm_ppl -> 1441.643544\n",
      "INFO - 05/30/20 16:38:28 - 0:00:10 - valid_Ghomala_mlm_acc -> 14.766839\n",
      "INFO - 05/30/20 16:38:28 - 0:00:10 - valid_Ngiemboon_mlm_ppl -> 1695.519009\n",
      "INFO - 05/30/20 16:38:28 - 0:00:10 - valid_Ngiemboon_mlm_acc -> 12.694301\n",
      "INFO - 05/30/20 16:38:28 - 0:00:10 - valid_mlm_ppl -> 1568.581276\n",
      "INFO - 05/30/20 16:38:28 - 0:00:10 - valid_mlm_acc -> 13.730570\n",
      "INFO - 05/30/20 16:38:28 - 0:00:10 - test_Ghomala_mlm_ppl -> 1441.643544\n",
      "INFO - 05/30/20 16:38:28 - 0:00:10 - test_Ghomala_mlm_acc -> 14.766839\n",
      "INFO - 05/30/20 16:38:28 - 0:00:10 - test_Ngiemboon_mlm_ppl -> 1695.519009\n",
      "INFO - 05/30/20 16:38:28 - 0:00:10 - test_Ngiemboon_mlm_acc -> 12.694301\n",
      "INFO - 05/30/20 16:38:28 - 0:00:10 - test_mlm_ppl -> 1568.581276\n",
      "INFO - 05/30/20 16:38:28 - 0:00:10 - test_mlm_acc -> 13.730570\n",
      "INFO - 05/30/20 16:38:28 - 0:00:10 - __log__:{\"epoch\": 40, \"valid_Ghomala_mlm_ppl\": 1441.6435440981058, \"valid_Ghomala_mlm_acc\": 14.766839378238341, \"valid_Ngiemboon_mlm_ppl\": 1695.5190085937065, \"valid_Ngiemboon_mlm_acc\": 12.694300518134716, \"valid_mlm_ppl\": 1568.5812763459062, \"valid_mlm_acc\": 13.730569948186528, \"test_Ghomala_mlm_ppl\": 1441.6435440981058, \"test_Ghomala_mlm_acc\": 14.766839378238341, \"test_Ngiemboon_mlm_ppl\": 1695.5190085937065, \"test_Ngiemboon_mlm_acc\": 12.694300518134716, \"test_mlm_ppl\": 1568.5812763459062, \"test_mlm_acc\": 13.730569948186528}\n",
      "=====================\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ghomala.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ghomala.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ngiemboon.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ngiemboon.pth\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "%env dump_path=/home/jupyter/models/africa/cluster3\n",
    "%env exp_name=mlm_tlm_GhomalaNgiemboon\n",
    "%env data_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon\n",
    "%env lgs=Ghomala-Ngiemboon\n",
    "%env mlm_steps=Ghomala,Ngiemboon\n",
    "%env tgt_pair=Ghomala-Ngiemboon\n",
    "%env src_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon\n",
    "%env tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon\n",
    "\n",
    "# creation of the dummy files so that the experiment does not bug\n",
    "! touch /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/train.Ghomala.pth\n",
    "! touch /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/train.Ngiemboon.pth\n",
    "\n",
    "###### Ghomala_Ngiemboon vs Limbum\n",
    "! ../duplicate.sh $src_path $tgt_path Limbum $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : delete the rename file\n",
    "! ../delete.sh $tgt_path $tgt_pair\n",
    "\n",
    "###### Ghomala_Ngiemboon vs Bafia\n",
    "! ../duplicate.sh $src_path $tgt_path Bafia $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : delete the rename file\n",
    "! ../delete.sh $tgt_path $tgt_pair\n",
    "\n",
    "###### Ghomala_Ngiemboon vs Bulu\n",
    "! ../duplicate.sh $src_path $tgt_path Bulu $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : delete the rename file\n",
    "! ../delete.sh $tgt_path $tgt_pair\n",
    "\n",
    "###### Ghomala_Ngiemboon vs Ewondo\n",
    "! ../duplicate.sh $src_path $tgt_path MKPAMAN_AMVOE_Ewondo $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : delete the rename file\n",
    "! ../delete.sh $tgt_path $tgt_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limbum_Ngiemboon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: dump_path=/home/jupyter/models/africa/cluster3\n",
      "env: exp_name=mlm_tlm_LimbumNgiemboon\n",
      "env: data_path=/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon\n",
      "env: lgs=Limbum-Ngiemboon\n",
      "env: mlm_steps=Limbum,Ngiemboon\n",
      "env: tgt_pair=Limbum-Ngiemboon\n",
      "env: src_path=/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon\n",
      "env: tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon\n",
      "=====================\n",
      "Ghomala to Limbum\n",
      "Ghomala to Ngiemboon\n",
      "=====================\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/30/20 16:39:21 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/30/20 16:39:21 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_LimbumNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon' --lgs 'Limbum-Ngiemboon' --clm_steps '' --mlm_steps 'Limbum,Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_LimbumNgiemboon\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Limbum', 1: 'Ngiemboon'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Limbum': 0, 'Ngiemboon': 1}\n",
      "                                     langs: ['Limbum', 'Ngiemboon']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Limbum-Ngiemboon']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Limbum', None), ('Ngiemboon', None)]\n",
      "                                     mono_dataset: {'Limbum': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/train.Limbum.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Limbum.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Limbum.pth'}, 'Ngiemboon': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/train.Ngiemboon.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Ngiemboon.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Ngiemboon.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': False, 'valid': False, 'test': False}\n",
      "                                     remove_long_sentences_test: False\n",
      "                                     remove_long_sentences_train: False\n",
      "                                     remove_long_sentences_valid: False\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/30/20 16:39:21 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml\n",
      "                                     \n",
      "INFO - 05/30/20 16:39:21 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_LimbumNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon' --lgs 'Limbum-Ngiemboon' --clm_steps '' --mlm_steps 'Limbum,Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/30/20 16:39:21 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/30/20 16:39:21 - 0:00:00 - ============ langs: Limbum, Ngiemboon\n",
      "INFO - 05/30/20 16:39:21 - 0:00:00 - ============ Monolingual data (Limbum)\n",
      "INFO - 05/30/20 16:39:21 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Limbum.pth ...\n",
      "INFO - 05/30/20 16:39:21 - 0:00:00 - 500482 words (6667 unique) in 7942 sentences. 159492 unknown words (57 unique) covering 31.87% of the data.\n",
      "INFO - 05/30/20 16:39:21 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/30/20 16:39:21 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Limbum.pth ...\n",
      "INFO - 05/30/20 16:39:21 - 0:00:00 - 500482 words (6667 unique) in 7942 sentences. 159492 unknown words (57 unique) covering 31.87% of the data.\n",
      "INFO - 05/30/20 16:39:21 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/30/20 16:39:21 - 0:00:00 - ============ Monolingual data (Ngiemboon)\n",
      "INFO - 05/30/20 16:39:21 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Ngiemboon.pth ...\n",
      "INFO - 05/30/20 16:39:21 - 0:00:00 - 500482 words (6667 unique) in 7942 sentences. 159492 unknown words (57 unique) covering 31.87% of the data.\n",
      "INFO - 05/30/20 16:39:21 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/30/20 16:39:21 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Ngiemboon.pth ...\n",
      "INFO - 05/30/20 16:39:21 - 0:00:00 - 500482 words (6667 unique) in 7942 sentences. 159492 unknown words (57 unique) covering 31.87% of the data.\n",
      "INFO - 05/30/20 16:39:21 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/30/20 16:39:21 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/30/20 16:39:21 - 0:00:00 - Monolingual data   - valid -       Limbum:      7942\n",
      "INFO - 05/30/20 16:39:21 - 0:00:00 - Monolingual data   -  test -       Limbum:      7942\n",
      "INFO - 05/30/20 16:39:21 - 0:00:00 - Monolingual data   - valid -    Ngiemboon:      7942\n",
      "INFO - 05/30/20 16:39:21 - 0:00:00 - Monolingual data   -  test -    Ngiemboon:      7942\n",
      "\n",
      "INFO - 05/30/20 16:39:22 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(6667, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=6667, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/30/20 16:39:22 - 0:00:01 - Number of parameters (model): 82939403\n",
      "INFO - 05/30/20 16:39:27 - 0:00:06 - Found 0 memories.\n",
      "INFO - 05/30/20 16:39:27 - 0:00:06 - Found 6 FFN.\n",
      "INFO - 05/30/20 16:39:27 - 0:00:06 - Found 102 parameters in model.\n",
      "INFO - 05/30/20 16:39:27 - 0:00:06 - Optimizers: model\n",
      "WARNING - 05/30/20 16:39:27 - 0:00:06 - Reloading checkpoint from /home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml/checkpoint.pth ...\n",
      "WARNING - 05/30/20 16:39:28 - 0:00:07 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/30/20 16:39:28 - 0:00:07 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/30/20 16:39:28 - 0:00:07 - Checkpoint reloaded. Resuming at epoch 60 / iteration 15840 ...\n",
      "INFO - 05/30/20 16:39:30 - 0:00:09 - epoch -> 60.000000\n",
      "INFO - 05/30/20 16:39:30 - 0:00:09 - valid_Limbum_mlm_ppl -> 17083.489067\n",
      "INFO - 05/30/20 16:39:30 - 0:00:09 - valid_Limbum_mlm_acc -> 12.953368\n",
      "INFO - 05/30/20 16:39:30 - 0:00:09 - valid_Ngiemboon_mlm_ppl -> 12017.180215\n",
      "INFO - 05/30/20 16:39:30 - 0:00:09 - valid_Ngiemboon_mlm_acc -> 13.989637\n",
      "INFO - 05/30/20 16:39:30 - 0:00:09 - valid_mlm_ppl -> 14550.334641\n",
      "INFO - 05/30/20 16:39:30 - 0:00:09 - valid_mlm_acc -> 13.471503\n",
      "INFO - 05/30/20 16:39:30 - 0:00:09 - test_Limbum_mlm_ppl -> 17083.489067\n",
      "INFO - 05/30/20 16:39:30 - 0:00:09 - test_Limbum_mlm_acc -> 12.953368\n",
      "INFO - 05/30/20 16:39:30 - 0:00:09 - test_Ngiemboon_mlm_ppl -> 12017.180215\n",
      "INFO - 05/30/20 16:39:30 - 0:00:09 - test_Ngiemboon_mlm_acc -> 13.989637\n",
      "INFO - 05/30/20 16:39:30 - 0:00:09 - test_mlm_ppl -> 14550.334641\n",
      "INFO - 05/30/20 16:39:30 - 0:00:09 - test_mlm_acc -> 13.471503\n",
      "INFO - 05/30/20 16:39:30 - 0:00:09 - __log__:{\"epoch\": 60, \"valid_Limbum_mlm_ppl\": 17083.489067193823, \"valid_Limbum_mlm_acc\": 12.953367875647668, \"valid_Ngiemboon_mlm_ppl\": 12017.180214905266, \"valid_Ngiemboon_mlm_acc\": 13.989637305699482, \"valid_mlm_ppl\": 14550.334641049543, \"valid_mlm_acc\": 13.471502590673575, \"test_Limbum_mlm_ppl\": 17083.489067193823, \"test_Limbum_mlm_acc\": 12.953367875647668, \"test_Ngiemboon_mlm_ppl\": 12017.180214905266, \"test_Ngiemboon_mlm_acc\": 13.989637305699482, \"test_mlm_ppl\": 14550.334641049543, \"test_mlm_acc\": 13.471502590673575}\n",
      "=====================\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Limbum.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Limbum.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Ngiemboon.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Ngiemboon.pth\n",
      "=====================\n",
      "=====================\n",
      "Bafia to Limbum\n",
      "Bafia to Ngiemboon\n",
      "=====================\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/30/20 16:39:31 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/30/20 16:39:31 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_LimbumNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon' --lgs 'Limbum-Ngiemboon' --clm_steps '' --mlm_steps 'Limbum,Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_LimbumNgiemboon\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Limbum', 1: 'Ngiemboon'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Limbum': 0, 'Ngiemboon': 1}\n",
      "                                     langs: ['Limbum', 'Ngiemboon']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Limbum-Ngiemboon']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Limbum', None), ('Ngiemboon', None)]\n",
      "                                     mono_dataset: {'Limbum': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/train.Limbum.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Limbum.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Limbum.pth'}, 'Ngiemboon': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/train.Ngiemboon.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Ngiemboon.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Ngiemboon.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': False, 'valid': False, 'test': False}\n",
      "                                     remove_long_sentences_test: False\n",
      "                                     remove_long_sentences_train: False\n",
      "                                     remove_long_sentences_valid: False\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/30/20 16:39:31 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml\n",
      "                                     \n",
      "INFO - 05/30/20 16:39:31 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_LimbumNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon' --lgs 'Limbum-Ngiemboon' --clm_steps '' --mlm_steps 'Limbum,Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/30/20 16:39:31 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/30/20 16:39:31 - 0:00:00 - ============ langs: Limbum, Ngiemboon\n",
      "INFO - 05/30/20 16:39:31 - 0:00:00 - ============ Monolingual data (Limbum)\n",
      "INFO - 05/30/20 16:39:31 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Limbum.pth ...\n",
      "INFO - 05/30/20 16:39:31 - 0:00:00 - 625171 words (6667 unique) in 7950 sentences. 206249 unknown words (52 unique) covering 32.99% of the data.\n",
      "INFO - 05/30/20 16:39:31 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/30/20 16:39:31 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Limbum.pth ...\n",
      "INFO - 05/30/20 16:39:31 - 0:00:00 - 625171 words (6667 unique) in 7950 sentences. 206249 unknown words (52 unique) covering 32.99% of the data.\n",
      "INFO - 05/30/20 16:39:31 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/30/20 16:39:31 - 0:00:00 - ============ Monolingual data (Ngiemboon)\n",
      "INFO - 05/30/20 16:39:31 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Ngiemboon.pth ...\n",
      "INFO - 05/30/20 16:39:31 - 0:00:00 - 625171 words (6667 unique) in 7950 sentences. 206249 unknown words (52 unique) covering 32.99% of the data.\n",
      "INFO - 05/30/20 16:39:31 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/30/20 16:39:31 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Ngiemboon.pth ...\n",
      "INFO - 05/30/20 16:39:31 - 0:00:00 - 625171 words (6667 unique) in 7950 sentences. 206249 unknown words (52 unique) covering 32.99% of the data.\n",
      "INFO - 05/30/20 16:39:31 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/30/20 16:39:31 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/30/20 16:39:31 - 0:00:00 - Monolingual data   - valid -       Limbum:      7950\n",
      "INFO - 05/30/20 16:39:31 - 0:00:00 - Monolingual data   -  test -       Limbum:      7950\n",
      "INFO - 05/30/20 16:39:31 - 0:00:00 - Monolingual data   - valid -    Ngiemboon:      7950\n",
      "INFO - 05/30/20 16:39:31 - 0:00:00 - Monolingual data   -  test -    Ngiemboon:      7950\n",
      "\n",
      "INFO - 05/30/20 16:39:32 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(6667, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=6667, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/30/20 16:39:32 - 0:00:01 - Number of parameters (model): 82939403\n",
      "INFO - 05/30/20 16:39:38 - 0:00:07 - Found 0 memories.\n",
      "INFO - 05/30/20 16:39:38 - 0:00:07 - Found 6 FFN.\n",
      "INFO - 05/30/20 16:39:38 - 0:00:07 - Found 102 parameters in model.\n",
      "INFO - 05/30/20 16:39:38 - 0:00:07 - Optimizers: model\n",
      "WARNING - 05/30/20 16:39:38 - 0:00:07 - Reloading checkpoint from /home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml/checkpoint.pth ...\n",
      "WARNING - 05/30/20 16:39:39 - 0:00:08 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/30/20 16:39:39 - 0:00:08 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/30/20 16:39:39 - 0:00:08 - Checkpoint reloaded. Resuming at epoch 60 / iteration 15840 ...\n",
      "INFO - 05/30/20 16:39:41 - 0:00:10 - epoch -> 60.000000\n",
      "INFO - 05/30/20 16:39:41 - 0:00:10 - valid_Limbum_mlm_ppl -> 32284.276526\n",
      "INFO - 05/30/20 16:39:41 - 0:00:10 - valid_Limbum_mlm_acc -> 11.398964\n",
      "INFO - 05/30/20 16:39:41 - 0:00:10 - valid_Ngiemboon_mlm_ppl -> 31114.458484\n",
      "INFO - 05/30/20 16:39:41 - 0:00:10 - valid_Ngiemboon_mlm_acc -> 11.398964\n",
      "INFO - 05/30/20 16:39:41 - 0:00:10 - valid_mlm_ppl -> 31699.367505\n",
      "INFO - 05/30/20 16:39:41 - 0:00:10 - valid_mlm_acc -> 11.398964\n",
      "INFO - 05/30/20 16:39:41 - 0:00:10 - test_Limbum_mlm_ppl -> 32284.276526\n",
      "INFO - 05/30/20 16:39:41 - 0:00:10 - test_Limbum_mlm_acc -> 11.398964\n",
      "INFO - 05/30/20 16:39:41 - 0:00:10 - test_Ngiemboon_mlm_ppl -> 31114.458484\n",
      "INFO - 05/30/20 16:39:41 - 0:00:10 - test_Ngiemboon_mlm_acc -> 11.398964\n",
      "INFO - 05/30/20 16:39:41 - 0:00:10 - test_mlm_ppl -> 31699.367505\n",
      "INFO - 05/30/20 16:39:41 - 0:00:10 - test_mlm_acc -> 11.398964\n",
      "INFO - 05/30/20 16:39:41 - 0:00:10 - __log__:{\"epoch\": 60, \"valid_Limbum_mlm_ppl\": 32284.27652594165, \"valid_Limbum_mlm_acc\": 11.398963730569948, \"valid_Ngiemboon_mlm_ppl\": 31114.45848352915, \"valid_Ngiemboon_mlm_acc\": 11.398963730569948, \"valid_mlm_ppl\": 31699.367504735397, \"valid_mlm_acc\": 11.398963730569948, \"test_Limbum_mlm_ppl\": 32284.27652594165, \"test_Limbum_mlm_acc\": 11.398963730569948, \"test_Ngiemboon_mlm_ppl\": 31114.45848352915, \"test_Ngiemboon_mlm_acc\": 11.398963730569948, \"test_mlm_ppl\": 31699.367504735397, \"test_mlm_acc\": 11.398963730569948}\n",
      "=====================\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Limbum.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Limbum.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Ngiemboon.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Ngiemboon.pth\n",
      "=====================\n",
      "=====================\n",
      "Bulu to Limbum\n",
      "Bulu to Ngiemboon\n",
      "=====================\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/30/20 16:39:43 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/30/20 16:39:43 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_LimbumNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon' --lgs 'Limbum-Ngiemboon' --clm_steps '' --mlm_steps 'Limbum,Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_LimbumNgiemboon\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Limbum', 1: 'Ngiemboon'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Limbum': 0, 'Ngiemboon': 1}\n",
      "                                     langs: ['Limbum', 'Ngiemboon']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Limbum-Ngiemboon']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Limbum', None), ('Ngiemboon', None)]\n",
      "                                     mono_dataset: {'Limbum': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/train.Limbum.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Limbum.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Limbum.pth'}, 'Ngiemboon': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/train.Ngiemboon.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Ngiemboon.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Ngiemboon.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': False, 'valid': False, 'test': False}\n",
      "                                     remove_long_sentences_test: False\n",
      "                                     remove_long_sentences_train: False\n",
      "                                     remove_long_sentences_valid: False\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/30/20 16:39:43 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml\n",
      "                                     \n",
      "INFO - 05/30/20 16:39:43 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_LimbumNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon' --lgs 'Limbum-Ngiemboon' --clm_steps '' --mlm_steps 'Limbum,Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/30/20 16:39:43 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/30/20 16:39:43 - 0:00:00 - ============ langs: Limbum, Ngiemboon\n",
      "INFO - 05/30/20 16:39:43 - 0:00:00 - ============ Monolingual data (Limbum)\n",
      "INFO - 05/30/20 16:39:43 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Limbum.pth ...\n",
      "INFO - 05/30/20 16:39:43 - 0:00:00 - 400009 words (6667 unique) in 7946 sentences. 15365 unknown words (65 unique) covering 3.84% of the data.\n",
      "INFO - 05/30/20 16:39:43 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/30/20 16:39:43 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Limbum.pth ...\n",
      "INFO - 05/30/20 16:39:43 - 0:00:00 - 400009 words (6667 unique) in 7946 sentences. 15365 unknown words (65 unique) covering 3.84% of the data.\n",
      "INFO - 05/30/20 16:39:43 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/30/20 16:39:43 - 0:00:00 - ============ Monolingual data (Ngiemboon)\n",
      "INFO - 05/30/20 16:39:43 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Ngiemboon.pth ...\n",
      "INFO - 05/30/20 16:39:43 - 0:00:00 - 400009 words (6667 unique) in 7946 sentences. 15365 unknown words (65 unique) covering 3.84% of the data.\n",
      "INFO - 05/30/20 16:39:43 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/30/20 16:39:43 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Ngiemboon.pth ...\n",
      "INFO - 05/30/20 16:39:43 - 0:00:00 - 400009 words (6667 unique) in 7946 sentences. 15365 unknown words (65 unique) covering 3.84% of the data.\n",
      "INFO - 05/30/20 16:39:43 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/30/20 16:39:43 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/30/20 16:39:43 - 0:00:00 - Monolingual data   - valid -       Limbum:      7946\n",
      "INFO - 05/30/20 16:39:43 - 0:00:00 - Monolingual data   -  test -       Limbum:      7946\n",
      "INFO - 05/30/20 16:39:43 - 0:00:00 - Monolingual data   - valid -    Ngiemboon:      7946\n",
      "INFO - 05/30/20 16:39:43 - 0:00:00 - Monolingual data   -  test -    Ngiemboon:      7946\n",
      "\n",
      "INFO - 05/30/20 16:39:43 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(6667, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=6667, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/30/20 16:39:43 - 0:00:01 - Number of parameters (model): 82939403\n",
      "INFO - 05/30/20 16:39:49 - 0:00:06 - Found 0 memories.\n",
      "INFO - 05/30/20 16:39:49 - 0:00:06 - Found 6 FFN.\n",
      "INFO - 05/30/20 16:39:49 - 0:00:06 - Found 102 parameters in model.\n",
      "INFO - 05/30/20 16:39:49 - 0:00:06 - Optimizers: model\n",
      "WARNING - 05/30/20 16:39:49 - 0:00:06 - Reloading checkpoint from /home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml/checkpoint.pth ...\n",
      "WARNING - 05/30/20 16:39:50 - 0:00:07 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/30/20 16:39:50 - 0:00:07 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/30/20 16:39:50 - 0:00:07 - Checkpoint reloaded. Resuming at epoch 60 / iteration 15840 ...\n",
      "INFO - 05/30/20 16:39:52 - 0:00:10 - epoch -> 60.000000\n",
      "INFO - 05/30/20 16:39:52 - 0:00:10 - valid_Limbum_mlm_ppl -> 8025.559232\n",
      "INFO - 05/30/20 16:39:52 - 0:00:10 - valid_Limbum_mlm_acc -> 9.326425\n",
      "INFO - 05/30/20 16:39:52 - 0:00:10 - valid_Ngiemboon_mlm_ppl -> 6003.756952\n",
      "INFO - 05/30/20 16:39:52 - 0:00:10 - valid_Ngiemboon_mlm_acc -> 10.880829\n",
      "INFO - 05/30/20 16:39:52 - 0:00:10 - valid_mlm_ppl -> 7014.658092\n",
      "INFO - 05/30/20 16:39:52 - 0:00:10 - valid_mlm_acc -> 10.103627\n",
      "INFO - 05/30/20 16:39:52 - 0:00:10 - test_Limbum_mlm_ppl -> 8025.559232\n",
      "INFO - 05/30/20 16:39:52 - 0:00:10 - test_Limbum_mlm_acc -> 9.326425\n",
      "INFO - 05/30/20 16:39:52 - 0:00:10 - test_Ngiemboon_mlm_ppl -> 6003.756952\n",
      "INFO - 05/30/20 16:39:52 - 0:00:10 - test_Ngiemboon_mlm_acc -> 10.880829\n",
      "INFO - 05/30/20 16:39:52 - 0:00:10 - test_mlm_ppl -> 7014.658092\n",
      "INFO - 05/30/20 16:39:52 - 0:00:10 - test_mlm_acc -> 10.103627\n",
      "INFO - 05/30/20 16:39:52 - 0:00:10 - __log__:{\"epoch\": 60, \"valid_Limbum_mlm_ppl\": 8025.559232173076, \"valid_Limbum_mlm_acc\": 9.32642487046632, \"valid_Ngiemboon_mlm_ppl\": 6003.756952108556, \"valid_Ngiemboon_mlm_acc\": 10.880829015544041, \"valid_mlm_ppl\": 7014.658092140817, \"valid_mlm_acc\": 10.103626943005182, \"test_Limbum_mlm_ppl\": 8025.559232173076, \"test_Limbum_mlm_acc\": 9.32642487046632, \"test_Ngiemboon_mlm_ppl\": 6003.756952108556, \"test_Ngiemboon_mlm_acc\": 10.880829015544041, \"test_mlm_ppl\": 7014.658092140817, \"test_mlm_acc\": 10.103626943005182}\n",
      "=====================\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Limbum.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Limbum.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Ngiemboon.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Ngiemboon.pth\n",
      "=====================\n",
      "=====================\n",
      "MKPAMAN_AMVOE_Ewondo to Limbum\n",
      "MKPAMAN_AMVOE_Ewondo to Ngiemboon\n",
      "=====================\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/30/20 16:39:54 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/30/20 16:39:54 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_LimbumNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon' --lgs 'Limbum-Ngiemboon' --clm_steps '' --mlm_steps 'Limbum,Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_LimbumNgiemboon\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Limbum', 1: 'Ngiemboon'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Limbum': 0, 'Ngiemboon': 1}\n",
      "                                     langs: ['Limbum', 'Ngiemboon']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Limbum-Ngiemboon']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Limbum', None), ('Ngiemboon', None)]\n",
      "                                     mono_dataset: {'Limbum': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/train.Limbum.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Limbum.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Limbum.pth'}, 'Ngiemboon': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/train.Ngiemboon.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Ngiemboon.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Ngiemboon.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': False, 'valid': False, 'test': False}\n",
      "                                     remove_long_sentences_test: False\n",
      "                                     remove_long_sentences_train: False\n",
      "                                     remove_long_sentences_valid: False\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/30/20 16:39:54 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml\n",
      "                                     \n",
      "INFO - 05/30/20 16:39:54 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_LimbumNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon' --lgs 'Limbum-Ngiemboon' --clm_steps '' --mlm_steps 'Limbum,Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/30/20 16:39:54 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/30/20 16:39:54 - 0:00:00 - ============ langs: Limbum, Ngiemboon\n",
      "INFO - 05/30/20 16:39:54 - 0:00:00 - ============ Monolingual data (Limbum)\n",
      "INFO - 05/30/20 16:39:54 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Limbum.pth ...\n",
      "INFO - 05/30/20 16:39:54 - 0:00:00 - 384402 words (6667 unique) in 7944 sentences. 3374 unknown words (56 unique) covering 0.88% of the data.\n",
      "INFO - 05/30/20 16:39:54 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/30/20 16:39:54 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Limbum.pth ...\n",
      "INFO - 05/30/20 16:39:54 - 0:00:00 - 384402 words (6667 unique) in 7944 sentences. 3374 unknown words (56 unique) covering 0.88% of the data.\n",
      "INFO - 05/30/20 16:39:54 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/30/20 16:39:54 - 0:00:00 - ============ Monolingual data (Ngiemboon)\n",
      "INFO - 05/30/20 16:39:54 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Ngiemboon.pth ...\n",
      "INFO - 05/30/20 16:39:54 - 0:00:00 - 384402 words (6667 unique) in 7944 sentences. 3374 unknown words (56 unique) covering 0.88% of the data.\n",
      "INFO - 05/30/20 16:39:54 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/30/20 16:39:54 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Ngiemboon.pth ...\n",
      "INFO - 05/30/20 16:39:54 - 0:00:00 - 384402 words (6667 unique) in 7944 sentences. 3374 unknown words (56 unique) covering 0.88% of the data.\n",
      "INFO - 05/30/20 16:39:54 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/30/20 16:39:54 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/30/20 16:39:54 - 0:00:00 - Monolingual data   - valid -       Limbum:      7944\n",
      "INFO - 05/30/20 16:39:54 - 0:00:00 - Monolingual data   -  test -       Limbum:      7944\n",
      "INFO - 05/30/20 16:39:54 - 0:00:00 - Monolingual data   - valid -    Ngiemboon:      7944\n",
      "INFO - 05/30/20 16:39:54 - 0:00:00 - Monolingual data   -  test -    Ngiemboon:      7944\n",
      "\n",
      "INFO - 05/30/20 16:39:54 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(6667, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=6667, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/30/20 16:39:54 - 0:00:01 - Number of parameters (model): 82939403\n",
      "INFO - 05/30/20 16:39:58 - 0:00:04 - Found 0 memories.\n",
      "INFO - 05/30/20 16:39:58 - 0:00:04 - Found 6 FFN.\n",
      "INFO - 05/30/20 16:39:58 - 0:00:04 - Found 102 parameters in model.\n",
      "INFO - 05/30/20 16:39:58 - 0:00:05 - Optimizers: model\n",
      "WARNING - 05/30/20 16:39:58 - 0:00:05 - Reloading checkpoint from /home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml/checkpoint.pth ...\n",
      "WARNING - 05/30/20 16:39:59 - 0:00:05 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/30/20 16:39:59 - 0:00:05 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/30/20 16:39:59 - 0:00:05 - Checkpoint reloaded. Resuming at epoch 60 / iteration 15840 ...\n",
      "INFO - 05/30/20 16:40:00 - 0:00:07 - epoch -> 60.000000\n",
      "INFO - 05/30/20 16:40:00 - 0:00:07 - valid_Limbum_mlm_ppl -> 7568.201924\n",
      "INFO - 05/30/20 16:40:00 - 0:00:07 - valid_Limbum_mlm_acc -> 6.476684\n",
      "INFO - 05/30/20 16:40:00 - 0:00:07 - valid_Ngiemboon_mlm_ppl -> 4472.056722\n",
      "INFO - 05/30/20 16:40:00 - 0:00:07 - valid_Ngiemboon_mlm_acc -> 10.103627\n",
      "INFO - 05/30/20 16:40:00 - 0:00:07 - valid_mlm_ppl -> 6020.129323\n",
      "INFO - 05/30/20 16:40:00 - 0:00:07 - valid_mlm_acc -> 8.290155\n",
      "INFO - 05/30/20 16:40:00 - 0:00:07 - test_Limbum_mlm_ppl -> 7568.201924\n",
      "INFO - 05/30/20 16:40:00 - 0:00:07 - test_Limbum_mlm_acc -> 6.476684\n",
      "INFO - 05/30/20 16:40:00 - 0:00:07 - test_Ngiemboon_mlm_ppl -> 4472.056722\n",
      "INFO - 05/30/20 16:40:00 - 0:00:07 - test_Ngiemboon_mlm_acc -> 10.103627\n",
      "INFO - 05/30/20 16:40:00 - 0:00:07 - test_mlm_ppl -> 6020.129323\n",
      "INFO - 05/30/20 16:40:00 - 0:00:07 - test_mlm_acc -> 8.290155\n",
      "INFO - 05/30/20 16:40:00 - 0:00:07 - __log__:{\"epoch\": 60, \"valid_Limbum_mlm_ppl\": 7568.2019243477325, \"valid_Limbum_mlm_acc\": 6.476683937823834, \"valid_Ngiemboon_mlm_ppl\": 4472.056721701962, \"valid_Ngiemboon_mlm_acc\": 10.103626943005182, \"valid_mlm_ppl\": 6020.129323024847, \"valid_mlm_acc\": 8.290155440414509, \"test_Limbum_mlm_ppl\": 7568.2019243477325, \"test_Limbum_mlm_acc\": 6.476683937823834, \"test_Ngiemboon_mlm_ppl\": 4472.056721701962, \"test_Ngiemboon_mlm_acc\": 10.103626943005182, \"test_mlm_ppl\": 6020.129323024847, \"test_mlm_acc\": 8.290155440414509}\n",
      "=====================\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Limbum.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Limbum.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Ngiemboon.pth\n",
      "delete /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Ngiemboon.pth\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "%env dump_path=/home/jupyter/models/africa/cluster3\n",
    "%env exp_name=mlm_tlm_LimbumNgiemboon\n",
    "%env data_path=/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon\n",
    "%env lgs=Limbum-Ngiemboon\n",
    "%env mlm_steps=Limbum,Ngiemboon\n",
    "%env tgt_pair=Limbum-Ngiemboon\n",
    "%env src_path=/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon\n",
    "%env tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon\n",
    "\n",
    "# creation of the dummy files so that the experiment does not bug\n",
    "! touch /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/train.Limbum.pth\n",
    "! touch /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/train.Ngiemboon.pth\n",
    "\n",
    "###### Limbum_Ngiemboon vs Ghomala\n",
    "! ../duplicate.sh $src_path $tgt_path Ghomala $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : delete the rename file\n",
    "! ../delete.sh $tgt_path $tgt_pair\n",
    "\n",
    "###### Limbum_Ngiemboon vs Bafia\n",
    "! ../duplicate.sh $src_path $tgt_path Bafia $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : delete the rename file\n",
    "! ../delete.sh $tgt_path $tgt_pair\n",
    "\n",
    "###### Limbum_Ngiemboon vs Bulu\n",
    "! ../duplicate.sh $src_path $tgt_path Bulu $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : delete the rename file\n",
    "! ../delete.sh $tgt_path $tgt_pair\n",
    "\n",
    "###### Limbum_Ngiemboon vs Ewondo\n",
    "! ../duplicate.sh $src_path $tgt_path MKPAMAN_AMVOE_Ewondo $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : delete the rename file\n",
    "! ../delete.sh $tgt_path $tgt_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
