{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(bookmark:HOME) -> /home/jupyter/meta_XLM/XLM\n",
      "/home/jupyter/meta_XLM/XLM\n"
     ]
    }
   ],
   "source": [
    "%bookmark HOME \"/home/jupyter/meta_XLM/XLM\" \n",
    "%cd -b HOME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: csv_path=/home/jupyter\n",
      "env: output_dir=/home/jupyter/data/evaluation_hypothesis\n",
      "env: data_type=mono\n"
     ]
    }
   ],
   "source": [
    "%env csv_path=/home/jupyter\n",
    "%env output_dir=/home/jupyter/data/evaluation_hypothesis\n",
    "%env data_type=mono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: languages=Bafia,Bafia\n",
      "Bafia\n",
      "======= Read 7950 totals samples\n",
      "======= Delete 0 samples\n",
      "======= Save 7950 samples\n",
      "env: languages=Bulu,Bulu\n",
      "Bulu\n",
      "======= Read 31297 totals samples\n",
      "======= Delete 2402 samples\n",
      "======= Save 28895 samples\n",
      "env: languages=MKPAMAN_AMVOE_Ewondo,MKPAMAN_AMVOE_Ewondo\n",
      "MKPAMAN_AMVOE_Ewondo\n",
      "======= Read 7950 totals samples\n",
      "======= Delete 6 samples\n",
      "======= Save 7944 samples\n",
      "env: languages=Ghomala,Ghomala\n",
      "Ghomala\n",
      "======= Read 7950 totals samples\n",
      "======= Delete 8 samples\n",
      "======= Save 7942 samples\n",
      "env: languages=Limbum,Limbum\n",
      "Limbum\n",
      "======= Read 7950 totals samples\n",
      "======= Delete 31 samples\n",
      "======= Save 7919 samples\n",
      "env: languages=Ngiemboon,Ngiemboon\n",
      "Ngiemboon\n",
      "======= Read 7950 totals samples\n",
      "======= Delete 21 samples\n",
      "======= Save 7929 samples\n"
     ]
    }
   ],
   "source": [
    "%env languages=Bafia,Bafia\n",
    "! python ../bible.py --csv_path $csv_path --output_dir $output_dir --data_type $data_type --languages $languages\n",
    "\n",
    "%env languages=Bulu,Bulu\n",
    "! python ../bible.py --csv_path $csv_path --output_dir $output_dir --data_type $data_type --languages $languages\n",
    "\n",
    "%env languages=MKPAMAN_AMVOE_Ewondo,MKPAMAN_AMVOE_Ewondo\n",
    "! python ../bible.py --csv_path $csv_path --output_dir $output_dir --data_type $data_type --languages $languages\n",
    "\n",
    "%env languages=Ghomala,Ghomala\n",
    "! python ../bible.py --csv_path $csv_path --output_dir $output_dir --data_type $data_type --languages $languages\n",
    "\n",
    "%env languages=Limbum,Limbum\n",
    "! python ../bible.py --csv_path $csv_path --output_dir $output_dir --data_type $data_type --languages $languages\n",
    "\n",
    "%env languages=Ngiemboon,Ngiemboon\n",
    "! python ../bible.py --csv_path $csv_path --output_dir $output_dir --data_type $data_type --languages $languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select below each language pair trained for the purpose of MLM+TLM, and for this pair we use its code and vocabulary to process the other pairs in order to start the evaluation; and in this evaluation we rename the evaluated pair data files into the data files of the evaluating pairs.\n",
    "That's long enough!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: src_path=/home/jupyter/data/evaluation_hypothesis\n",
      "env: threads_for_tokenizer=16\n",
      "env: n_samples=-1\n",
      "env: test_size=10\n",
      "env: val_size=10\n",
      "env: TOKENIZE=tools/tokenizer_our.sh\n",
      "env: LOWER_REMOVE_ACCENT=tools/lowercase_and_remove_accent.py\n",
      "env: FASTBPE=tools/fastBPE/fast\n"
     ]
    }
   ],
   "source": [
    "%env src_path=/home/jupyter/data/evaluation_hypothesis\n",
    "%env threads_for_tokenizer=16 \n",
    "%env n_samples=-1\n",
    "%env test_size=10             \n",
    "%env val_size=10\n",
    "# tools paths\n",
    "%env TOKENIZE=tools/tokenizer_our.sh\n",
    "%env LOWER_REMOVE_ACCENT=tools/lowercase_and_remove_accent.py\n",
    "%env FASTBPE=tools/fastBPE/fast\n",
    "! chmod +x $FASTBPE\n",
    "! chmod +x tools/mosesdecoder/scripts/tokenizer/*.perl\n",
    "! chmod +x ../build_evaluate_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir /home/jupyter/models/africa/evaluation_hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bafia_Bulu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu\n",
      "env: CODE_VOCAB_PATH=/home/jupyter/models/africa/cluster1/data/Bafia_Bulu/processed\n",
      "*** Cleaning and tokenizing MKPAMAN_AMVOE_Ewondo data ... ***\n",
      "Tokenizer Version 1.1\n",
      "Language: MKPAMAN_AMVOE_Ewondo\n",
      "Number of threads: 16\n",
      "WARNING: No known abbreviations for language 'MKPAMAN_AMVOE_Ewondo', attempting fall-back to English version...\n",
      "*** Tokenized (+ lowercase + accent-removal) MKPAMAN_AMVOE_Ewondo data to /home/jupyter/data/evaluation_hypothesis/MKPAMAN_AMVOE_Ewondo.all ***\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Bulu/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/train.MKPAMAN_AMVOE_Ewondo ...\n",
      "Read 148418 words (6898 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/train.MKPAMAN_AMVOE_Ewondo ...\n",
      "Modified 148418 words from text file.\n",
      "INFO - 05/29/20 20:31:39 - 0:00:00 - Read 8683 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/train.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/29/20 20:31:39 - 0:00:00 - 223384 words (8683 unique) in 6357 sentences.\n",
      "INFO - 05/29/20 20:31:39 - 0:00:00 - 16530 unknown words (342 unique), covering 7.40% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Bulu/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.MKPAMAN_AMVOE_Ewondo ...\n",
      "Read 18748 words (2528 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.MKPAMAN_AMVOE_Ewondo ...\n",
      "Modified 18748 words from text file.\n",
      "INFO - 05/29/20 20:31:40 - 0:00:00 - Read 8683 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/29/20 20:31:40 - 0:00:00 - 28143 words (8683 unique) in 794 sentences.\n",
      "INFO - 05/29/20 20:31:40 - 0:00:00 - 2003 unknown words (183 unique), covering 7.12% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Bulu/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.MKPAMAN_AMVOE_Ewondo ...\n",
      "Read 18332 words (2563 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.MKPAMAN_AMVOE_Ewondo ...\n",
      "Modified 18332 words from text file.\n",
      "INFO - 05/29/20 20:31:40 - 0:00:00 - Read 8683 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/29/20 20:31:40 - 0:00:00 - 27446 words (8683 unique) in 794 sentences.\n",
      "INFO - 05/29/20 20:31:40 - 0:00:00 - 2017 unknown words (194 unique), covering 7.35% of the data.\n",
      "*** Cleaning and tokenizing Ghomala data ... ***\n",
      "Tokenizer Version 1.1\n",
      "Language: Ghomala\n",
      "Number of threads: 16\n",
      "WARNING: No known abbreviations for language 'Ghomala', attempting fall-back to English version...\n",
      "*** Tokenized (+ lowercase + accent-removal) Ghomala data to /home/jupyter/data/evaluation_hypothesis/Ghomala.all ***\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Bulu/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/train.Ghomala ...\n",
      "Read 207949 words (2354 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/train.Ghomala ...\n",
      "Modified 207949 words from text file.\n",
      "INFO - 05/29/20 20:31:42 - 0:00:00 - Read 8683 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/train.Ghomala.pth ...\n",
      "INFO - 05/29/20 20:31:42 - 0:00:00 - 380619 words (8683 unique) in 6355 sentences.\n",
      "INFO - 05/29/20 20:31:42 - 0:00:00 - 107235 unknown words (166 unique), covering 28.17% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Bulu/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Ghomala ...\n",
      "Read 25671 words (1126 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Ghomala ...\n",
      "Modified 25671 words from text file.\n",
      "INFO - 05/29/20 20:31:43 - 0:00:00 - Read 8683 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Ghomala.pth ...\n",
      "INFO - 05/29/20 20:31:43 - 0:00:00 - 46932 words (8683 unique) in 794 sentences.\n",
      "INFO - 05/29/20 20:31:43 - 0:00:00 - 13140 unknown words (91 unique), covering 28.00% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Bulu/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Ghomala ...\n",
      "Read 25433 words (1118 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Ghomala ...\n",
      "Modified 25433 words from text file.\n",
      "INFO - 05/29/20 20:31:44 - 0:00:00 - Read 8683 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Ghomala.pth ...\n",
      "INFO - 05/29/20 20:31:44 - 0:00:00 - 46777 words (8683 unique) in 794 sentences.\n",
      "INFO - 05/29/20 20:31:44 - 0:00:00 - 13157 unknown words (77 unique), covering 28.13% of the data.\n",
      "*** Cleaning and tokenizing Limbum data ... ***\n",
      "Tokenizer Version 1.1\n",
      "Language: Limbum\n",
      "Number of threads: 16\n",
      "WARNING: No known abbreviations for language 'Limbum', attempting fall-back to English version...\n",
      "*** Tokenized (+ lowercase + accent-removal) Limbum data to /home/jupyter/data/evaluation_hypothesis/Limbum.all ***\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Bulu/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/train.Limbum ...\n",
      "Read 257088 words (2692 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/train.Limbum ...\n",
      "Modified 257088 words from text file.\n",
      "INFO - 05/29/20 20:31:46 - 0:00:00 - Read 8683 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/train.Limbum.pth ...\n",
      "INFO - 05/29/20 20:31:46 - 0:00:00 - 390633 words (8683 unique) in 6336 sentences.\n",
      "INFO - 05/29/20 20:31:46 - 0:00:00 - 22560 unknown words (166 unique), covering 5.78% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Bulu/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Limbum ...\n",
      "Read 32162 words (1298 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Limbum ...\n",
      "Modified 32162 words from text file.\n",
      "INFO - 05/29/20 20:31:47 - 0:00:00 - Read 8683 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Limbum.pth ...\n",
      "INFO - 05/29/20 20:31:47 - 0:00:00 - 49216 words (8683 unique) in 792 sentences.\n",
      "INFO - 05/29/20 20:31:47 - 0:00:00 - 2934 unknown words (101 unique), covering 5.96% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Bulu/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Limbum ...\n",
      "Read 32138 words (1314 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Limbum ...\n",
      "Modified 32138 words from text file.\n",
      "INFO - 05/29/20 20:31:47 - 0:00:00 - Read 8683 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Limbum.pth ...\n",
      "INFO - 05/29/20 20:31:47 - 0:00:00 - 48967 words (8683 unique) in 792 sentences.\n",
      "INFO - 05/29/20 20:31:47 - 0:00:00 - 2812 unknown words (99 unique), covering 5.74% of the data.\n",
      "*** Cleaning and tokenizing Ngiemboon data ... ***\n",
      "Tokenizer Version 1.1\n",
      "Language: Ngiemboon\n",
      "Number of threads: 16\n",
      "WARNING: No known abbreviations for language 'Ngiemboon', attempting fall-back to English version...\n",
      "*** Tokenized (+ lowercase + accent-removal) Ngiemboon data to /home/jupyter/data/evaluation_hypothesis/Ngiemboon.all ***\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Bulu/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/train.Ngiemboon ...\n",
      "Read 205429 words (11313 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/train.Ngiemboon ...\n",
      "Modified 205429 words from text file.\n",
      "INFO - 05/29/20 20:31:49 - 0:00:00 - Read 8683 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/train.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 20:31:50 - 0:00:01 - 647333 words (8683 unique) in 6344 sentences.\n",
      "INFO - 05/29/20 20:31:50 - 0:00:01 - 189682 unknown words (163 unique), covering 29.30% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Bulu/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Ngiemboon ...\n",
      "Read 25650 words (3475 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Ngiemboon ...\n",
      "Modified 25650 words from text file.\n",
      "INFO - 05/29/20 20:31:50 - 0:00:00 - Read 8683 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 20:31:51 - 0:00:00 - 80874 words (8683 unique) in 793 sentences.\n",
      "INFO - 05/29/20 20:31:51 - 0:00:00 - 23668 unknown words (99 unique), covering 29.27% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Bulu/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Ngiemboon ...\n",
      "Read 25416 words (3526 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Ngiemboon ...\n",
      "Modified 25416 words from text file.\n",
      "INFO - 05/29/20 20:31:51 - 0:00:00 - Read 8683 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 20:31:51 - 0:00:00 - 80250 words (8683 unique) in 793 sentences.\n",
      "INFO - 05/29/20 20:31:51 - 0:00:00 - 23519 unknown words (92 unique), covering 29.31% of the data.\n"
     ]
    }
   ],
   "source": [
    "%env tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu\n",
    "%env CODE_VOCAB_PATH=/home/jupyter/models/africa/cluster1/data/Bafia_Bulu/processed\n",
    "! ../build_evaluate_data.sh MKPAMAN_AMVOE_Ewondo $n_samples\n",
    "! ../build_evaluate_data.sh Ghomala $n_samples\n",
    "! ../build_evaluate_data.sh Limbum $n_samples\n",
    "! ../build_evaluate_data.sh Ngiemboon $n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Bafia_Ewondo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo\n",
      "env: CODE_VOCAB_PATH=/home/jupyter/models/africa/cluster1/data/Bafia_Ewondo/processed\n",
      "*** Cleaning and tokenizing Bulu data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Bulu.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/train.Bulu ...\n",
      "Read 693166 words (11175 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/train.Bulu ...\n",
      "Modified 693166 words from text file.\n",
      "INFO - 05/29/20 20:35:33 - 0:00:00 - Read 9128 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/train.Bulu.pth ...\n",
      "INFO - 05/29/20 20:35:34 - 0:00:01 - 1020735 words (9128 unique) in 23118 sentences.\n",
      "INFO - 05/29/20 20:35:34 - 0:00:01 - 59064 unknown words (334 unique), covering 5.79% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Bulu ...\n",
      "Read 86467 words (4734 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Bulu ...\n",
      "Modified 86467 words from text file.\n",
      "INFO - 05/29/20 20:35:34 - 0:00:00 - Read 9128 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.Bulu.pth ...\n",
      "INFO - 05/29/20 20:35:34 - 0:00:00 - 127164 words (9128 unique) in 2889 sentences.\n",
      "INFO - 05/29/20 20:35:34 - 0:00:00 - 7355 unknown words (194 unique), covering 5.78% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Bulu ...\n",
      "Read 86828 words (4589 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Bulu ...\n",
      "Modified 86828 words from text file.\n",
      "INFO - 05/29/20 20:35:35 - 0:00:00 - Read 9128 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.Bulu.pth ...\n",
      "INFO - 05/29/20 20:35:35 - 0:00:00 - 127880 words (9128 unique) in 2889 sentences.\n",
      "INFO - 05/29/20 20:35:35 - 0:00:00 - 7359 unknown words (206 unique), covering 5.75% of the data.\n",
      "*** Cleaning and tokenizing Ghomala data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Ghomala.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/train.Ghomala ...\n",
      "Read 207949 words (2354 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/train.Ghomala ...\n",
      "Modified 207949 words from text file.\n",
      "INFO - 05/29/20 20:35:36 - 0:00:00 - Read 9128 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/train.Ghomala.pth ...\n",
      "INFO - 05/29/20 20:35:36 - 0:00:00 - 380299 words (9128 unique) in 6355 sentences.\n",
      "INFO - 05/29/20 20:35:36 - 0:00:00 - 92223 unknown words (116 unique), covering 24.25% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Ghomala ...\n",
      "Read 25671 words (1126 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Ghomala ...\n",
      "Modified 25671 words from text file.\n",
      "INFO - 05/29/20 20:35:37 - 0:00:00 - Read 9128 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.Ghomala.pth ...\n",
      "INFO - 05/29/20 20:35:37 - 0:00:00 - 46896 words (9128 unique) in 794 sentences.\n",
      "INFO - 05/29/20 20:35:37 - 0:00:00 - 11305 unknown words (71 unique), covering 24.11% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Ghomala ...\n",
      "Read 25433 words (1118 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Ghomala ...\n",
      "Modified 25433 words from text file.\n",
      "INFO - 05/29/20 20:35:38 - 0:00:00 - Read 9128 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.Ghomala.pth ...\n",
      "INFO - 05/29/20 20:35:38 - 0:00:00 - 46753 words (9128 unique) in 794 sentences.\n",
      "INFO - 05/29/20 20:35:38 - 0:00:00 - 11249 unknown words (60 unique), covering 24.06% of the data.\n",
      "*** Cleaning and tokenizing Limbum data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Limbum.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/train.Limbum ...\n",
      "Read 257088 words (2692 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/train.Limbum ...\n",
      "Modified 257088 words from text file.\n",
      "INFO - 05/29/20 20:35:39 - 0:00:00 - Read 9128 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/train.Limbum.pth ...\n",
      "INFO - 05/29/20 20:35:39 - 0:00:00 - 391807 words (9128 unique) in 6336 sentences.\n",
      "INFO - 05/29/20 20:35:39 - 0:00:00 - 9924 unknown words (111 unique), covering 2.53% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Limbum ...\n",
      "Read 32162 words (1298 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Limbum ...\n",
      "Modified 32162 words from text file.\n",
      "INFO - 05/29/20 20:35:40 - 0:00:00 - Read 9128 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.Limbum.pth ...\n",
      "INFO - 05/29/20 20:35:40 - 0:00:00 - 49193 words (9128 unique) in 792 sentences.\n",
      "INFO - 05/29/20 20:35:40 - 0:00:00 - 1317 unknown words (68 unique), covering 2.68% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Limbum ...\n",
      "Read 32138 words (1314 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Limbum ...\n",
      "Modified 32138 words from text file.\n",
      "INFO - 05/29/20 20:35:40 - 0:00:00 - Read 9128 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.Limbum.pth ...\n",
      "INFO - 05/29/20 20:35:41 - 0:00:00 - 48934 words (9128 unique) in 792 sentences.\n",
      "INFO - 05/29/20 20:35:41 - 0:00:00 - 1320 unknown words (69 unique), covering 2.70% of the data.\n",
      "*** Cleaning and tokenizing Ngiemboon data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Ngiemboon.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/train.Ngiemboon ...\n",
      "Read 205429 words (11313 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/train.Ngiemboon ...\n",
      "Modified 205429 words from text file.\n",
      "INFO - 05/29/20 20:35:41 - 0:00:00 - Read 9128 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/train.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 20:35:42 - 0:00:01 - 660190 words (9128 unique) in 6344 sentences.\n",
      "INFO - 05/29/20 20:35:42 - 0:00:01 - 172299 unknown words (121 unique), covering 26.10% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Ngiemboon ...\n",
      "Read 25650 words (3475 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Ngiemboon ...\n",
      "Modified 25650 words from text file.\n",
      "INFO - 05/29/20 20:35:43 - 0:00:00 - Read 9128 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 20:35:43 - 0:00:00 - 82319 words (9128 unique) in 793 sentences.\n",
      "INFO - 05/29/20 20:35:43 - 0:00:00 - 21496 unknown words (66 unique), covering 26.11% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Ngiemboon ...\n",
      "Read 25416 words (3526 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Ngiemboon ...\n",
      "Modified 25416 words from text file.\n",
      "INFO - 05/29/20 20:35:43 - 0:00:00 - Read 9128 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 20:35:44 - 0:00:00 - 81950 words (9128 unique) in 793 sentences.\n",
      "INFO - 05/29/20 20:35:44 - 0:00:00 - 21321 unknown words (65 unique), covering 26.02% of the data.\n"
     ]
    }
   ],
   "source": [
    "%env tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo\n",
    "%env CODE_VOCAB_PATH=/home/jupyter/models/africa/cluster1/data/Bafia_Ewondo/processed\n",
    "! ../build_evaluate_data.sh Bulu $n_samples\n",
    "! ../build_evaluate_data.sh Ghomala $n_samples\n",
    "! ../build_evaluate_data.sh Limbum $n_samples\n",
    "! ../build_evaluate_data.sh Ngiemboon $n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bulu_Ewondo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo\n",
      "env: CODE_VOCAB_PATH=/home/jupyter/models/africa/cluster1/data/Bulu_MKPAMAN_AMVOE_Ewondo/processed\n",
      "*** Cleaning and tokenizing Bafia data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Bafia.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bulu_MKPAMAN_AMVOE_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/train.Bafia ...\n",
      "Read 273337 words (4735 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/train.Bafia ...\n",
      "Modified 273337 words from text file.\n",
      "INFO - 05/29/20 20:37:44 - 0:00:00 - Read 9061 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/train.Bafia.pth ...\n",
      "INFO - 05/29/20 20:37:44 - 0:00:00 - 495902 words (9061 unique) in 6361 sentences.\n",
      "INFO - 05/29/20 20:37:44 - 0:00:00 - 171059 unknown words (141 unique), covering 34.49% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bulu_MKPAMAN_AMVOE_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Bafia ...\n",
      "Read 35077 words (1960 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Bafia ...\n",
      "Modified 35077 words from text file.\n",
      "INFO - 05/29/20 20:37:45 - 0:00:00 - Read 9061 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.Bafia.pth ...\n",
      "INFO - 05/29/20 20:37:45 - 0:00:00 - 63509 words (9061 unique) in 795 sentences.\n",
      "INFO - 05/29/20 20:37:45 - 0:00:00 - 21991 unknown words (77 unique), covering 34.63% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bulu_MKPAMAN_AMVOE_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Bafia ...\n",
      "Read 34130 words (1939 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Bafia ...\n",
      "Modified 34130 words from text file.\n",
      "INFO - 05/29/20 20:37:46 - 0:00:00 - Read 9061 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.Bafia.pth ...\n",
      "INFO - 05/29/20 20:37:46 - 0:00:00 - 61798 words (9061 unique) in 795 sentences.\n",
      "INFO - 05/29/20 20:37:46 - 0:00:00 - 21546 unknown words (78 unique), covering 34.87% of the data.\n",
      "*** Cleaning and tokenizing Ghomala data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Ghomala.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bulu_MKPAMAN_AMVOE_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/train.Ghomala ...\n",
      "Read 207949 words (2354 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/train.Ghomala ...\n",
      "Modified 207949 words from text file.\n",
      "INFO - 05/29/20 20:37:47 - 0:00:00 - Read 9061 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/train.Ghomala.pth ...\n",
      "INFO - 05/29/20 20:37:47 - 0:00:00 - 417298 words (9061 unique) in 6355 sentences.\n",
      "INFO - 05/29/20 20:37:47 - 0:00:00 - 135073 unknown words (120 unique), covering 32.37% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bulu_MKPAMAN_AMVOE_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Ghomala ...\n",
      "Read 25671 words (1126 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Ghomala ...\n",
      "Modified 25671 words from text file.\n",
      "INFO - 05/29/20 20:37:48 - 0:00:00 - Read 9061 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.Ghomala.pth ...\n",
      "INFO - 05/29/20 20:37:48 - 0:00:00 - 51401 words (9061 unique) in 794 sentences.\n",
      "INFO - 05/29/20 20:37:48 - 0:00:00 - 16583 unknown words (69 unique), covering 32.26% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bulu_MKPAMAN_AMVOE_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Ghomala ...\n",
      "Read 25433 words (1118 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Ghomala ...\n",
      "Modified 25433 words from text file.\n",
      "INFO - 05/29/20 20:37:49 - 0:00:00 - Read 9061 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.Ghomala.pth ...\n",
      "INFO - 05/29/20 20:37:49 - 0:00:00 - 51295 words (9061 unique) in 794 sentences.\n",
      "INFO - 05/29/20 20:37:49 - 0:00:00 - 16397 unknown words (63 unique), covering 31.97% of the data.\n",
      "*** Cleaning and tokenizing Limbum data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Limbum.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bulu_MKPAMAN_AMVOE_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/train.Limbum ...\n",
      "Read 257088 words (2692 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/train.Limbum ...\n",
      "Modified 257088 words from text file.\n",
      "INFO - 05/29/20 20:37:49 - 0:00:00 - Read 9061 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/train.Limbum.pth ...\n",
      "INFO - 05/29/20 20:37:50 - 0:00:00 - 426018 words (9061 unique) in 6336 sentences.\n",
      "INFO - 05/29/20 20:37:50 - 0:00:00 - 24130 unknown words (117 unique), covering 5.66% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bulu_MKPAMAN_AMVOE_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Limbum ...\n",
      "Read 32162 words (1298 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Limbum ...\n",
      "Modified 32162 words from text file.\n",
      "INFO - 05/29/20 20:37:50 - 0:00:00 - Read 9061 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.Limbum.pth ...\n",
      "INFO - 05/29/20 20:37:50 - 0:00:00 - 53665 words (9061 unique) in 792 sentences.\n",
      "INFO - 05/29/20 20:37:50 - 0:00:00 - 3026 unknown words (73 unique), covering 5.64% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bulu_MKPAMAN_AMVOE_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Limbum ...\n",
      "Read 32138 words (1314 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Limbum ...\n",
      "Modified 32138 words from text file.\n",
      "INFO - 05/29/20 20:37:51 - 0:00:00 - Read 9061 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.Limbum.pth ...\n",
      "INFO - 05/29/20 20:37:51 - 0:00:00 - 53291 words (9061 unique) in 792 sentences.\n",
      "INFO - 05/29/20 20:37:51 - 0:00:00 - 3080 unknown words (77 unique), covering 5.78% of the data.\n",
      "*** Cleaning and tokenizing Ngiemboon data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Ngiemboon.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bulu_MKPAMAN_AMVOE_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/train.Ngiemboon ...\n",
      "Read 205429 words (11313 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/train.Ngiemboon ...\n",
      "Modified 205429 words from text file.\n",
      "INFO - 05/29/20 20:37:52 - 0:00:00 - Read 9061 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/train.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 20:37:53 - 0:00:01 - 676751 words (9061 unique) in 6344 sentences.\n",
      "INFO - 05/29/20 20:37:53 - 0:00:01 - 210303 unknown words (155 unique), covering 31.08% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bulu_MKPAMAN_AMVOE_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Ngiemboon ...\n",
      "Read 25650 words (3475 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Ngiemboon ...\n",
      "Modified 25650 words from text file.\n",
      "INFO - 05/29/20 20:37:53 - 0:00:00 - Read 9061 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 20:37:53 - 0:00:00 - 84591 words (9061 unique) in 793 sentences.\n",
      "INFO - 05/29/20 20:37:53 - 0:00:00 - 26386 unknown words (78 unique), covering 31.19% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bulu_MKPAMAN_AMVOE_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Ngiemboon ...\n",
      "Read 25416 words (3526 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Ngiemboon ...\n",
      "Modified 25416 words from text file.\n",
      "INFO - 05/29/20 20:37:54 - 0:00:00 - Read 9061 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 20:37:54 - 0:00:00 - 83972 words (9061 unique) in 793 sentences.\n",
      "INFO - 05/29/20 20:37:54 - 0:00:00 - 26088 unknown words (80 unique), covering 31.07% of the data.\n"
     ]
    }
   ],
   "source": [
    "%env tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo\n",
    "%env CODE_VOCAB_PATH=/home/jupyter/models/africa/cluster1/data/Bulu_MKPAMAN_AMVOE_Ewondo/processed\n",
    "! ../build_evaluate_data.sh Bafia $n_samples\n",
    "! ../build_evaluate_data.sh Ghomala $n_samples\n",
    "! ../build_evaluate_data.sh Limbum $n_samples\n",
    "! ../build_evaluate_data.sh Ngiemboon $n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ghomala_Limbum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum\n",
      "env: CODE_VOCAB_PATH=/home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed\n",
      "*** Cleaning and tokenizing Bafia data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Bafia.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/train.Bafia ...\n",
      "Read 273337 words (4735 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/train.Bafia ...\n",
      "Modified 273337 words from text file.\n",
      "INFO - 05/29/20 20:38:58 - 0:00:00 - Read 5057 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/train.Bafia.pth ...\n",
      "INFO - 05/29/20 20:38:59 - 0:00:00 - 441375 words (5057 unique) in 6361 sentences.\n",
      "INFO - 05/29/20 20:38:59 - 0:00:00 - 136531 unknown words (363 unique), covering 30.93% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Bafia ...\n",
      "Read 35077 words (1960 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Bafia ...\n",
      "Modified 35077 words from text file.\n",
      "INFO - 05/29/20 20:38:59 - 0:00:00 - Read 5057 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Bafia.pth ...\n",
      "INFO - 05/29/20 20:38:59 - 0:00:00 - 56604 words (5057 unique) in 795 sentences.\n",
      "INFO - 05/29/20 20:38:59 - 0:00:00 - 17463 unknown words (227 unique), covering 30.85% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Bafia ...\n",
      "Read 34130 words (1939 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Bafia ...\n",
      "Modified 34130 words from text file.\n",
      "INFO - 05/29/20 20:39:00 - 0:00:00 - Read 5057 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Bafia.pth ...\n",
      "INFO - 05/29/20 20:39:00 - 0:00:00 - 54990 words (5057 unique) in 795 sentences.\n",
      "INFO - 05/29/20 20:39:00 - 0:00:00 - 17072 unknown words (232 unique), covering 31.05% of the data.\n",
      "*** Cleaning and tokenizing Bulu data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Bulu.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/train.Bulu ...\n",
      "Read 693166 words (11175 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/train.Bulu ...\n",
      "Modified 693166 words from text file.\n",
      "INFO - 05/29/20 20:39:01 - 0:00:00 - Read 5057 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/train.Bulu.pth ...\n",
      "INFO - 05/29/20 20:39:02 - 0:00:01 - 1182052 words (5057 unique) in 23118 sentences.\n",
      "INFO - 05/29/20 20:39:02 - 0:00:01 - 185502 unknown words (526 unique), covering 15.69% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Bulu ...\n",
      "Read 86467 words (4734 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Bulu ...\n",
      "Modified 86467 words from text file.\n",
      "INFO - 05/29/20 20:39:03 - 0:00:00 - Read 5057 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Bulu.pth ...\n",
      "INFO - 05/29/20 20:39:03 - 0:00:00 - 147105 words (5057 unique) in 2889 sentences.\n",
      "INFO - 05/29/20 20:39:03 - 0:00:00 - 22977 unknown words (361 unique), covering 15.62% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Bulu ...\n",
      "Read 86828 words (4589 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Bulu ...\n",
      "Modified 86828 words from text file.\n",
      "INFO - 05/29/20 20:39:03 - 0:00:00 - Read 5057 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Bulu.pth ...\n",
      "INFO - 05/29/20 20:39:04 - 0:00:00 - 147887 words (5057 unique) in 2889 sentences.\n",
      "INFO - 05/29/20 20:39:04 - 0:00:00 - 23332 unknown words (354 unique), covering 15.78% of the data.\n",
      "*** Cleaning and tokenizing MKPAMAN_AMVOE_Ewondo data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/MKPAMAN_AMVOE_Ewondo.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/train.MKPAMAN_AMVOE_Ewondo ...\n",
      "Read 148418 words (6898 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/train.MKPAMAN_AMVOE_Ewondo ...\n",
      "Modified 148418 words from text file.\n",
      "INFO - 05/29/20 20:39:04 - 0:00:00 - Read 5057 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/train.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/29/20 20:39:05 - 0:00:00 - 281691 words (5057 unique) in 6357 sentences.\n",
      "INFO - 05/29/20 20:39:05 - 0:00:00 - 50196 unknown words (378 unique), covering 17.82% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.MKPAMAN_AMVOE_Ewondo ...\n",
      "Read 18748 words (2528 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.MKPAMAN_AMVOE_Ewondo ...\n",
      "Modified 18748 words from text file.\n",
      "INFO - 05/29/20 20:39:05 - 0:00:00 - Read 5057 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/29/20 20:39:05 - 0:00:00 - 35442 words (5057 unique) in 794 sentences.\n",
      "INFO - 05/29/20 20:39:05 - 0:00:00 - 6333 unknown words (235 unique), covering 17.87% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.MKPAMAN_AMVOE_Ewondo ...\n",
      "Read 18332 words (2563 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.MKPAMAN_AMVOE_Ewondo ...\n",
      "Modified 18332 words from text file.\n",
      "INFO - 05/29/20 20:39:06 - 0:00:00 - Read 5057 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/29/20 20:39:06 - 0:00:00 - 34684 words (5057 unique) in 794 sentences.\n",
      "INFO - 05/29/20 20:39:06 - 0:00:00 - 6189 unknown words (251 unique), covering 17.84% of the data.\n",
      "*** Cleaning and tokenizing Ngiemboon data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Ngiemboon.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/train.Ngiemboon ...\n",
      "Read 205429 words (11313 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/train.Ngiemboon ...\n",
      "Modified 205429 words from text file.\n",
      "INFO - 05/29/20 20:39:07 - 0:00:00 - Read 5057 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/train.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 20:39:07 - 0:00:01 - 646648 words (5057 unique) in 6344 sentences.\n",
      "INFO - 05/29/20 20:39:07 - 0:00:01 - 260931 unknown words (314 unique), covering 40.35% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Ngiemboon ...\n",
      "Read 25650 words (3475 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Ngiemboon ...\n",
      "Modified 25650 words from text file.\n",
      "INFO - 05/29/20 20:39:08 - 0:00:00 - Read 5057 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 20:39:08 - 0:00:00 - 80756 words (5057 unique) in 793 sentences.\n",
      "INFO - 05/29/20 20:39:08 - 0:00:00 - 32486 unknown words (198 unique), covering 40.23% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Ngiemboon ...\n",
      "Read 25416 words (3526 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Ngiemboon ...\n",
      "Modified 25416 words from text file.\n",
      "INFO - 05/29/20 20:39:09 - 0:00:00 - Read 5057 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 20:39:09 - 0:00:00 - 80177 words (5057 unique) in 793 sentences.\n",
      "INFO - 05/29/20 20:39:09 - 0:00:00 - 32280 unknown words (186 unique), covering 40.26% of the data.\n"
     ]
    }
   ],
   "source": [
    "%env tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum\n",
    "%env CODE_VOCAB_PATH=/home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed\n",
    "! ../build_evaluate_data.sh Bafia $n_samples\n",
    "! ../build_evaluate_data.sh Bulu $n_samples\n",
    "! ../build_evaluate_data.sh MKPAMAN_AMVOE_Ewondo $n_samples\n",
    "! ../build_evaluate_data.sh Ngiemboon $n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ghomala_Ngiemboon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon\n",
      "env: CODE_VOCAB_PATH=/home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed\n",
      "*** Cleaning and tokenizing Bafia data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Bafia.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/train.Bafia ...\n",
      "Read 273337 words (4735 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/train.Bafia ...\n",
      "Modified 273337 words from text file.\n",
      "INFO - 05/29/20 20:40:46 - 0:00:00 - Read 6715 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/train.Bafia.pth ...\n",
      "INFO - 05/29/20 20:40:47 - 0:00:00 - 489802 words (6715 unique) in 6361 sentences.\n",
      "INFO - 05/29/20 20:40:47 - 0:00:00 - 64995 unknown words (66 unique), covering 13.27% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Bafia ...\n",
      "Read 35077 words (1960 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Bafia ...\n",
      "Modified 35077 words from text file.\n",
      "INFO - 05/29/20 20:40:47 - 0:00:00 - Read 6715 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Bafia.pth ...\n",
      "INFO - 05/29/20 20:40:47 - 0:00:00 - 62901 words (6715 unique) in 795 sentences.\n",
      "INFO - 05/29/20 20:40:47 - 0:00:00 - 8304 unknown words (41 unique), covering 13.20% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Bafia ...\n",
      "Read 34130 words (1939 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Bafia ...\n",
      "Modified 34130 words from text file.\n",
      "INFO - 05/29/20 20:40:48 - 0:00:00 - Read 6715 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Bafia.pth ...\n",
      "INFO - 05/29/20 20:40:48 - 0:00:00 - 61045 words (6715 unique) in 795 sentences.\n",
      "INFO - 05/29/20 20:40:48 - 0:00:00 - 8156 unknown words (40 unique), covering 13.36% of the data.\n",
      "*** Cleaning and tokenizing Bulu data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Bulu.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/train.Bulu ...\n",
      "Read 693166 words (11175 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/train.Bulu ...\n",
      "Modified 693166 words from text file.\n",
      "INFO - 05/29/20 20:40:49 - 0:00:00 - Read 6715 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/train.Bulu.pth ...\n",
      "INFO - 05/29/20 20:40:50 - 0:00:01 - 1344481 words (6715 unique) in 23118 sentences.\n",
      "INFO - 05/29/20 20:40:50 - 0:00:01 - 45629 unknown words (124 unique), covering 3.39% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Bulu ...\n",
      "Read 86467 words (4734 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Bulu ...\n",
      "Modified 86467 words from text file.\n",
      "INFO - 05/29/20 20:40:51 - 0:00:00 - Read 6715 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Bulu.pth ...\n",
      "INFO - 05/29/20 20:40:51 - 0:00:00 - 167498 words (6715 unique) in 2889 sentences.\n",
      "INFO - 05/29/20 20:40:51 - 0:00:00 - 5743 unknown words (89 unique), covering 3.43% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Bulu ...\n",
      "Read 86828 words (4589 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Bulu ...\n",
      "Modified 86828 words from text file.\n",
      "INFO - 05/29/20 20:40:52 - 0:00:00 - Read 6715 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Bulu.pth ...\n",
      "INFO - 05/29/20 20:40:52 - 0:00:00 - 168469 words (6715 unique) in 2889 sentences.\n",
      "INFO - 05/29/20 20:40:52 - 0:00:00 - 5684 unknown words (89 unique), covering 3.37% of the data.\n",
      "*** Cleaning and tokenizing MKPAMAN_AMVOE_Ewondo data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/MKPAMAN_AMVOE_Ewondo.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/train.MKPAMAN_AMVOE_Ewondo ...\n",
      "Read 148418 words (6898 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/train.MKPAMAN_AMVOE_Ewondo ...\n",
      "Modified 148418 words from text file.\n",
      "INFO - 05/29/20 20:40:53 - 0:00:00 - Read 6715 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/train.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/29/20 20:40:53 - 0:00:00 - 308620 words (6715 unique) in 6357 sentences.\n",
      "INFO - 05/29/20 20:40:53 - 0:00:00 - 9258 unknown words (72 unique), covering 3.00% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.MKPAMAN_AMVOE_Ewondo ...\n",
      "Read 18748 words (2528 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.MKPAMAN_AMVOE_Ewondo ...\n",
      "Modified 18748 words from text file.\n",
      "INFO - 05/29/20 20:40:53 - 0:00:00 - Read 6715 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/29/20 20:40:53 - 0:00:00 - 38743 words (6715 unique) in 794 sentences.\n",
      "INFO - 05/29/20 20:40:53 - 0:00:00 - 1156 unknown words (43 unique), covering 2.98% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.MKPAMAN_AMVOE_Ewondo ...\n",
      "Read 18332 words (2563 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.MKPAMAN_AMVOE_Ewondo ...\n",
      "Modified 18332 words from text file.\n",
      "INFO - 05/29/20 20:40:54 - 0:00:00 - Read 6715 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/29/20 20:40:54 - 0:00:00 - 38014 words (6715 unique) in 794 sentences.\n",
      "INFO - 05/29/20 20:40:54 - 0:00:00 - 1130 unknown words (43 unique), covering 2.97% of the data.\n",
      "*** Cleaning and tokenizing Limbum data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Limbum.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/train.Limbum ...\n",
      "Read 257088 words (2692 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/train.Limbum ...\n",
      "Modified 257088 words from text file.\n",
      "INFO - 05/29/20 20:40:55 - 0:00:00 - Read 6715 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/train.Limbum.pth ...\n",
      "INFO - 05/29/20 20:40:55 - 0:00:00 - 462404 words (6715 unique) in 6336 sentences.\n",
      "INFO - 05/29/20 20:40:55 - 0:00:00 - 2246 unknown words (46 unique), covering 0.49% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Limbum ...\n",
      "Read 32162 words (1298 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Limbum ...\n",
      "Modified 32162 words from text file.\n",
      "INFO - 05/29/20 20:40:56 - 0:00:00 - Read 6715 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Limbum.pth ...\n",
      "INFO - 05/29/20 20:40:56 - 0:00:00 - 58011 words (6715 unique) in 792 sentences.\n",
      "INFO - 05/29/20 20:40:56 - 0:00:00 - 302 unknown words (26 unique), covering 0.52% of the data.\n",
      "INFO - 05/29/20 20:40:56 - 0:00:00 - ede: 84\n",
      "INFO - 05/29/20 20:40:56 - 0:00:00 - nja@@: 57\n",
      "INFO - 05/29/20 20:40:56 - 0:00:00 - kwa@@: 47\n",
      "INFO - 05/29/20 20:40:56 - 0:00:00 - no@@: 20\n",
      "INFO - 05/29/20 20:40:56 - 0:00:00 - gw@@: 16\n",
      "INFO - 05/29/20 20:40:56 - 0:00:00 - eb@@: 15\n",
      "INFO - 05/29/20 20:40:56 - 0:00:00 - bee@@: 12\n",
      "INFO - 05/29/20 20:40:56 - 0:00:00 - fes@@: 12\n",
      "INFO - 05/29/20 20:40:56 - 0:00:00 - ham: 8\n",
      "INFO - 05/29/20 20:40:56 - 0:00:00 - babilo@@: 4\n",
      "INFO - 05/29/20 20:40:56 - 0:00:00 - esa@@: 4\n",
      "INFO - 05/29/20 20:40:56 - 0:00:00 - iska@@: 3\n",
      "INFO - 05/29/20 20:40:56 - 0:00:00 - bap@@: 3\n",
      "INFO - 05/29/20 20:40:56 - 0:00:00 - ange@@: 2\n",
      "INFO - 05/29/20 20:40:56 - 0:00:00 - kusi: 2\n",
      "INFO - 05/29/20 20:40:56 - 0:00:00 - timo@@: 2\n",
      "INFO - 05/29/20 20:40:56 - 0:00:00 - sadu@@: 2\n",
      "INFO - 05/29/20 20:40:56 - 0:00:00 - odi@@: 1\n",
      "INFO - 05/29/20 20:40:56 - 0:00:00 - nari@@: 1\n",
      "INFO - 05/29/20 20:40:56 - 0:00:00 - silv@@: 1\n",
      "INFO - 05/29/20 20:40:56 - 0:00:00 - elo@@: 1\n",
      "INFO - 05/29/20 20:40:56 - 0:00:00 - mak@@: 1\n",
      "INFO - 05/29/20 20:40:56 - 0:00:00 - ksa@@: 1\n",
      "INFO - 05/29/20 20:40:56 - 0:00:00 - safa@@: 1\n",
      "INFO - 05/29/20 20:40:56 - 0:00:00 - edo@@: 1\n",
      "INFO - 05/29/20 20:40:56 - 0:00:00 - sod@@: 1\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Limbum ...\n",
      "Read 32138 words (1314 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Limbum ...\n",
      "Modified 32138 words from text file.\n",
      "INFO - 05/29/20 20:40:57 - 0:00:00 - Read 6715 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Limbum.pth ...\n",
      "INFO - 05/29/20 20:40:57 - 0:00:00 - 57927 words (6715 unique) in 792 sentences.\n",
      "INFO - 05/29/20 20:40:57 - 0:00:00 - 300 unknown words (28 unique), covering 0.52% of the data.\n",
      "INFO - 05/29/20 20:40:57 - 0:00:00 - ede: 71\n",
      "INFO - 05/29/20 20:40:57 - 0:00:00 - nja@@: 53\n",
      "INFO - 05/29/20 20:40:57 - 0:00:00 - kwa@@: 40\n",
      "INFO - 05/29/20 20:40:57 - 0:00:00 - no@@: 19\n",
      "INFO - 05/29/20 20:40:57 - 0:00:00 - gw@@: 19\n",
      "INFO - 05/29/20 20:40:57 - 0:00:00 - eb@@: 15\n",
      "INFO - 05/29/20 20:40:57 - 0:00:00 - bee@@: 14\n",
      "INFO - 05/29/20 20:40:57 - 0:00:00 - fes@@: 14\n",
      "INFO - 05/29/20 20:40:57 - 0:00:00 - esa@@: 9\n",
      "INFO - 05/29/20 20:40:57 - 0:00:00 - ham: 8\n",
      "INFO - 05/29/20 20:40:57 - 0:00:00 - edo@@: 6\n",
      "INFO - 05/29/20 20:40:57 - 0:00:00 - bap@@: 5\n",
      "INFO - 05/29/20 20:40:57 - 0:00:00 - timo@@: 4\n",
      "INFO - 05/29/20 20:40:57 - 0:00:00 - iti: 2\n",
      "INFO - 05/29/20 20:40:57 - 0:00:00 - aro: 2\n",
      "INFO - 05/29/20 20:40:57 - 0:00:00 - ksa@@: 2\n",
      "INFO - 05/29/20 20:40:57 - 0:00:00 - mak@@: 2\n",
      "INFO - 05/29/20 20:40:57 - 0:00:00 - elo@@: 2\n",
      "INFO - 05/29/20 20:40:57 - 0:00:00 - ange@@: 2\n",
      "INFO - 05/29/20 20:40:57 - 0:00:00 - lgo@@: 2\n",
      "INFO - 05/29/20 20:40:57 - 0:00:00 - al@@: 2\n",
      "INFO - 05/29/20 20:40:57 - 0:00:00 - loni@@: 1\n",
      "INFO - 05/29/20 20:40:57 - 0:00:00 - odi@@: 1\n",
      "INFO - 05/29/20 20:40:57 - 0:00:00 - sod@@: 1\n",
      "INFO - 05/29/20 20:40:57 - 0:00:00 - nari@@: 1\n",
      "INFO - 05/29/20 20:40:57 - 0:00:00 - babilo@@: 1\n",
      "INFO - 05/29/20 20:40:57 - 0:00:00 - kusi: 1\n",
      "INFO - 05/29/20 20:40:57 - 0:00:00 - y: 1\n"
     ]
    }
   ],
   "source": [
    "%env tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon\n",
    "%env CODE_VOCAB_PATH=/home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed\n",
    "! ../build_evaluate_data.sh Bafia $n_samples\n",
    "! ../build_evaluate_data.sh Bulu $n_samples\n",
    "! ../build_evaluate_data.sh MKPAMAN_AMVOE_Ewondo $n_samples\n",
    "! ../build_evaluate_data.sh Limbum $n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limbum_Ngiemboon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon\n",
      "env: CODE_VOCAB_PATH=/home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed\n",
      "*** Cleaning and tokenizing Bafia data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Bafia.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/train.Bafia ...\n",
      "Read 273337 words (4735 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/train.Bafia ...\n",
      "Modified 273337 words from text file.\n",
      "INFO - 05/29/20 20:42:04 - 0:00:00 - Read 6667 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/train.Bafia.pth ...\n",
      "INFO - 05/29/20 20:42:04 - 0:00:00 - 499029 words (6667 unique) in 6361 sentences.\n",
      "INFO - 05/29/20 20:42:04 - 0:00:00 - 164464 unknown words (51 unique), covering 32.96% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Bafia ...\n",
      "Read 35077 words (1960 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Bafia ...\n",
      "Modified 35077 words from text file.\n",
      "INFO - 05/29/20 20:42:05 - 0:00:00 - Read 6667 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Bafia.pth ...\n",
      "INFO - 05/29/20 20:42:05 - 0:00:00 - 63985 words (6667 unique) in 795 sentences.\n",
      "INFO - 05/29/20 20:42:05 - 0:00:00 - 21103 unknown words (31 unique), covering 32.98% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Bafia ...\n",
      "Read 34130 words (1939 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Bafia ...\n",
      "Modified 34130 words from text file.\n",
      "INFO - 05/29/20 20:42:05 - 0:00:00 - Read 6667 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Bafia.pth ...\n",
      "INFO - 05/29/20 20:42:06 - 0:00:00 - 62241 words (6667 unique) in 795 sentences.\n",
      "INFO - 05/29/20 20:42:06 - 0:00:00 - 20704 unknown words (30 unique), covering 33.26% of the data.\n",
      "*** Cleaning and tokenizing Bulu data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Bulu.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/train.Bulu ...\n",
      "Read 693166 words (11175 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/train.Bulu ...\n",
      "Modified 693166 words from text file.\n",
      "INFO - 05/29/20 20:42:06 - 0:00:00 - Read 6667 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/train.Bulu.pth ...\n",
      "INFO - 05/29/20 20:42:07 - 0:00:01 - 1235876 words (6667 unique) in 23118 sentences.\n",
      "INFO - 05/29/20 20:42:07 - 0:00:01 - 48205 unknown words (105 unique), covering 3.90% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Bulu ...\n",
      "Read 86467 words (4734 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Bulu ...\n",
      "Modified 86467 words from text file.\n",
      "INFO - 05/29/20 20:42:08 - 0:00:00 - Read 6667 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Bulu.pth ...\n",
      "INFO - 05/29/20 20:42:08 - 0:00:00 - 153594 words (6667 unique) in 2889 sentences.\n",
      "INFO - 05/29/20 20:42:08 - 0:00:00 - 6139 unknown words (68 unique), covering 4.00% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Bulu ...\n",
      "Read 86828 words (4589 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Bulu ...\n",
      "Modified 86828 words from text file.\n",
      "INFO - 05/29/20 20:42:09 - 0:00:00 - Read 6667 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Bulu.pth ...\n",
      "INFO - 05/29/20 20:42:09 - 0:00:00 - 154736 words (6667 unique) in 2889 sentences.\n",
      "INFO - 05/29/20 20:42:09 - 0:00:00 - 5959 unknown words (72 unique), covering 3.85% of the data.\n",
      "*** Cleaning and tokenizing MKPAMAN_AMVOE_Ewondo data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/MKPAMAN_AMVOE_Ewondo.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/train.MKPAMAN_AMVOE_Ewondo ...\n",
      "Read 148418 words (6898 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/train.MKPAMAN_AMVOE_Ewondo ...\n",
      "Modified 148418 words from text file.\n",
      "INFO - 05/29/20 20:42:10 - 0:00:00 - Read 6667 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/train.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/29/20 20:42:10 - 0:00:00 - 307876 words (6667 unique) in 6357 sentences.\n",
      "INFO - 05/29/20 20:42:10 - 0:00:00 - 2748 unknown words (53 unique), covering 0.89% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.MKPAMAN_AMVOE_Ewondo ...\n",
      "Read 18748 words (2528 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.MKPAMAN_AMVOE_Ewondo ...\n",
      "Modified 18748 words from text file.\n",
      "INFO - 05/29/20 20:42:11 - 0:00:00 - Read 6667 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/29/20 20:42:11 - 0:00:00 - 38704 words (6667 unique) in 794 sentences.\n",
      "INFO - 05/29/20 20:42:11 - 0:00:00 - 312 unknown words (29 unique), covering 0.81% of the data.\n",
      "INFO - 05/29/20 20:42:11 - 0:00:00 - ete: 46\n",
      "INFO - 05/29/20 20:42:11 - 0:00:00 - ve: 45\n",
      "INFO - 05/29/20 20:42:11 - 0:00:00 - nne@@: 43\n",
      "INFO - 05/29/20 20:42:11 - 0:00:00 - usa@@: 23\n",
      "INFO - 05/29/20 20:42:11 - 0:00:00 - any@@: 22\n",
      "INFO - 05/29/20 20:42:11 - 0:00:00 - ate@@: 22\n",
      "INFO - 05/29/20 20:42:11 - 0:00:00 - bab@@: 19\n",
      "INFO - 05/29/20 20:42:11 - 0:00:00 - od@@: 14\n",
      "INFO - 05/29/20 20:42:11 - 0:00:00 - um: 13\n",
      "INFO - 05/29/20 20:42:11 - 0:00:00 - he@@: 9\n",
      "INFO - 05/29/20 20:42:11 - 0:00:00 - mun: 7\n",
      "INFO - 05/29/20 20:42:11 - 0:00:00 - mbon: 6\n",
      "INFO - 05/29/20 20:42:11 - 0:00:00 - mg@@: 6\n",
      "INFO - 05/29/20 20:42:11 - 0:00:00 - ange@@: 5\n",
      "INFO - 05/29/20 20:42:11 - 0:00:00 - oma: 4\n",
      "INFO - 05/29/20 20:42:11 - 0:00:00 - edonia: 4\n",
      "INFO - 05/29/20 20:42:11 - 0:00:00 - sod@@: 3\n",
      "INFO - 05/29/20 20:42:11 - 0:00:00 - us@@: 3\n",
      "INFO - 05/29/20 20:42:11 - 0:00:00 - egip@@: 3\n",
      "INFO - 05/29/20 20:42:11 - 0:00:00 - odi@@: 3\n",
      "INFO - 05/29/20 20:42:11 - 0:00:00 - timo@@: 2\n",
      "INFO - 05/29/20 20:42:11 - 0:00:00 - damas@@: 2\n",
      "INFO - 05/29/20 20:42:11 - 0:00:00 - fes@@: 2\n",
      "INFO - 05/29/20 20:42:11 - 0:00:00 - lome@@: 1\n",
      "INFO - 05/29/20 20:42:11 - 0:00:00 - samari@@: 1\n",
      "INFO - 05/29/20 20:42:11 - 0:00:00 - ip@@: 1\n",
      "INFO - 05/29/20 20:42:11 - 0:00:00 - nai: 1\n",
      "INFO - 05/29/20 20:42:11 - 0:00:00 - nsa@@: 1\n",
      "INFO - 05/29/20 20:42:11 - 0:00:00 - tesaloni@@: 1\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.MKPAMAN_AMVOE_Ewondo ...\n",
      "Read 18332 words (2563 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.MKPAMAN_AMVOE_Ewondo ...\n",
      "Modified 18332 words from text file.\n",
      "INFO - 05/29/20 20:42:11 - 0:00:00 - Read 6667 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/29/20 20:42:11 - 0:00:00 - 37876 words (6667 unique) in 794 sentences.\n",
      "INFO - 05/29/20 20:42:11 - 0:00:00 - 315 unknown words (31 unique), covering 0.83% of the data.\n",
      "*** Cleaning and tokenizing Ghomala data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Ghomala.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/train.Ghomala ...\n",
      "Read 207949 words (2354 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/train.Ghomala ...\n",
      "Modified 207949 words from text file.\n",
      "INFO - 05/29/20 20:42:12 - 0:00:00 - Read 6667 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/train.Ghomala.pth ...\n",
      "INFO - 05/29/20 20:42:12 - 0:00:00 - 401668 words (6667 unique) in 6355 sentences.\n",
      "INFO - 05/29/20 20:42:12 - 0:00:00 - 128243 unknown words (55 unique), covering 31.93% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Ghomala ...\n",
      "Read 25671 words (1126 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Ghomala ...\n",
      "Modified 25671 words from text file.\n",
      "INFO - 05/29/20 20:42:13 - 0:00:00 - Read 6667 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Ghomala.pth ...\n",
      "INFO - 05/29/20 20:42:13 - 0:00:00 - 49588 words (6667 unique) in 794 sentences.\n",
      "INFO - 05/29/20 20:42:13 - 0:00:00 - 15675 unknown words (38 unique), covering 31.61% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Ghomala ...\n",
      "Read 25433 words (1118 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Ghomala ...\n",
      "Modified 25433 words from text file.\n",
      "INFO - 05/29/20 20:42:14 - 0:00:00 - Read 6667 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Ghomala.pth ...\n",
      "INFO - 05/29/20 20:42:14 - 0:00:00 - 49306 words (6667 unique) in 794 sentences.\n",
      "INFO - 05/29/20 20:42:14 - 0:00:00 - 15602 unknown words (43 unique), covering 31.64% of the data.\n"
     ]
    }
   ],
   "source": [
    "%env tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon\n",
    "%env CODE_VOCAB_PATH=/home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed\n",
    "! ../build_evaluate_data.sh Bafia $n_samples\n",
    "! ../build_evaluate_data.sh Bulu $n_samples\n",
    "! ../build_evaluate_data.sh MKPAMAN_AMVOE_Ewondo $n_samples\n",
    "! ../build_evaluate_data.sh Ghomala $n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: exp_id=maml\n",
      "env: batch_size=32\n",
      "env: max_epoch=100\n",
      "env: stopping_criterion=_valid_mlm_ppl,10\n",
      "env: eval_bleu=False\n",
      "env: remove_long_sentences_train=True\n",
      "env: remove_long_sentences_valid=True\n",
      "env: remove_long_sentences_test=True\n",
      "env: train_n_samples=-1\n",
      "env: valid_n_samples=-1\n",
      "env: test_n_samples=-1\n",
      "env: epoch_size=5000\n"
     ]
    }
   ],
   "source": [
    "%env exp_id=maml\n",
    "%env batch_size=32\n",
    "%env max_epoch=100\n",
    "%env stopping_criterion=_valid_mlm_ppl,10\n",
    "%env eval_bleu=False\n",
    "%env remove_long_sentences_train=True\n",
    "%env remove_long_sentences_valid=True\n",
    "%env remove_long_sentences_test=True\n",
    "%env train_n_samples=-1\n",
    "%env valid_n_samples=-1\n",
    "%env test_n_samples=-1\n",
    "%env epoch_size=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod +x ../copy_rename.sh\n",
    "! chmod +x ../evaluate.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Bafia_Bulu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: dump_path=/home/jupyter/models/africa/cluster1\n",
      "env: exp_name=mlm_tlm_BafiaBulu\n",
      "env: data_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu\n",
      "env: lgs=Bafia-Bulu\n",
      "env: mlm_steps=Bafia,Bulu\n",
      "env: tgt_pair=Bafia-Bulu\n",
      "env: src_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu\n",
      "env: tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu\n"
     ]
    }
   ],
   "source": [
    "%env dump_path=/home/jupyter/models/africa/cluster1\n",
    "%env exp_name=mlm_tlm_BafiaBulu\n",
    "%env data_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu\n",
    "%env lgs=Bafia-Bulu\n",
    "%env mlm_steps=Bafia,Bulu\n",
    "%env tgt_pair=Bafia-Bulu\n",
    "%env src_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu\n",
    "%env tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Bafia_Bulu vs Ewondo and Ghomala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bafia\n",
      "MKPAMAN_AMVOE_Ewondo\n",
      "../copy_rename.sh: line 24: [: missing `]'\n",
      "../copy_rename.sh: line 24: [: missing `]'\n",
      "../copy_rename.sh: line 24: [: missing `]'\n",
      "Bulu\n",
      "Ghomala\n",
      "../copy_rename.sh: line 24: [: missing `]'\n",
      "../copy_rename.sh: line 24: [: missing `]'\n",
      "../copy_rename.sh: line 24: [: missing `]'\n"
     ]
    }
   ],
   "source": [
    "! ../copy_rename.sh $src_path $tgt_path MKPAMAN_AMVOE_Ewondo-Ghomala $tgt_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/29/20 21:18:24 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/29/20 21:18:24 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_BafiaBulu --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu' --lgs 'Bafia-Bulu' --clm_steps '' --mlm_steps 'Bafia,Bulu' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaBulu/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_BafiaBulu\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Bafia', 1: 'Bulu'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Bafia': 0, 'Bulu': 1}\n",
      "                                     langs: ['Bafia', 'Bulu']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Bafia-Bulu']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Bafia', None), ('Bulu', None)]\n",
      "                                     mono_dataset: {'Bafia': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/train.Bafia.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bafia.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bafia.pth'}, 'Bulu': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/train.Bulu.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bulu.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bulu.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': True, 'valid': True, 'test': True}\n",
      "                                     remove_long_sentences_test: True\n",
      "                                     remove_long_sentences_train: True\n",
      "                                     remove_long_sentences_valid: True\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/29/20 21:18:24 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaBulu/maml\n",
      "                                     \n",
      "INFO - 05/29/20 21:18:24 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_BafiaBulu --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu' --lgs 'Bafia-Bulu' --clm_steps '' --mlm_steps 'Bafia,Bulu' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/29/20 21:18:24 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/29/20 21:18:24 - 0:00:00 - ============ langs: Bafia, Bulu\n",
      "INFO - 05/29/20 21:18:24 - 0:00:00 - ============ Monolingual data (Bafia)\n",
      "INFO - 05/29/20 21:18:24 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bafia.pth ...\n",
      "INFO - 05/29/20 21:18:24 - 0:00:00 - 28143 words (8683 unique) in 794 sentences. 2003 unknown words (183 unique) covering 7.12% of the data.\n",
      "INFO - 05/29/20 21:18:24 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/29/20 21:18:24 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bafia.pth ...\n",
      "INFO - 05/29/20 21:18:24 - 0:00:00 - 27446 words (8683 unique) in 794 sentences. 2017 unknown words (194 unique) covering 7.35% of the data.\n",
      "INFO - 05/29/20 21:18:24 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/29/20 21:18:24 - 0:00:00 - ============ Monolingual data (Bulu)\n",
      "INFO - 05/29/20 21:18:24 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bulu.pth ...\n",
      "INFO - 05/29/20 21:18:24 - 0:00:00 - 46932 words (8683 unique) in 794 sentences. 13140 unknown words (91 unique) covering 28.00% of the data.\n",
      "INFO - 05/29/20 21:18:24 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/29/20 21:18:24 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bulu.pth ...\n",
      "INFO - 05/29/20 21:18:24 - 0:00:00 - 46777 words (8683 unique) in 794 sentences. 13157 unknown words (77 unique) covering 28.13% of the data.\n",
      "INFO - 05/29/20 21:18:24 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/29/20 21:18:24 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/29/20 21:18:24 - 0:00:00 - Monolingual data   - valid -        Bafia:       794\n",
      "INFO - 05/29/20 21:18:24 - 0:00:00 - Monolingual data   -  test -        Bafia:       794\n",
      "INFO - 05/29/20 21:18:24 - 0:00:00 - Monolingual data   - valid -         Bulu:       794\n",
      "INFO - 05/29/20 21:18:24 - 0:00:00 - Monolingual data   -  test -         Bulu:       794\n",
      "\n",
      "INFO - 05/29/20 21:18:24 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(8683, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=8683, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/29/20 21:18:24 - 0:00:01 - Number of parameters (model): 85005803\n",
      "INFO - 05/29/20 21:18:28 - 0:00:04 - Found 0 memories.\n",
      "INFO - 05/29/20 21:18:28 - 0:00:04 - Found 6 FFN.\n",
      "INFO - 05/29/20 21:18:28 - 0:00:04 - Found 102 parameters in model.\n",
      "INFO - 05/29/20 21:18:28 - 0:00:04 - Optimizers: model\n",
      "WARNING - 05/29/20 21:18:28 - 0:00:04 - Reloading checkpoint from /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaBulu/maml/checkpoint.pth ...\n",
      "WARNING - 05/29/20 21:18:37 - 0:00:14 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/29/20 21:18:37 - 0:00:14 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/29/20 21:18:37 - 0:00:14 - Checkpoint reloaded. Resuming at epoch 68 / iteration 4556 ...\n",
      "INFO - 05/29/20 21:18:38 - 0:00:15 - epoch -> 68.000000\n",
      "INFO - 05/29/20 21:18:38 - 0:00:15 - valid_Bafia_mlm_ppl -> 13276.172741\n",
      "INFO - 05/29/20 21:18:38 - 0:00:15 - valid_Bafia_mlm_acc -> 13.730570\n",
      "INFO - 05/29/20 21:18:38 - 0:00:15 - valid_Bulu_mlm_ppl -> 17231.375469\n",
      "INFO - 05/29/20 21:18:38 - 0:00:15 - valid_Bulu_mlm_acc -> 10.103627\n",
      "INFO - 05/29/20 21:18:38 - 0:00:15 - valid_mlm_ppl -> 15253.774105\n",
      "INFO - 05/29/20 21:18:38 - 0:00:15 - valid_mlm_acc -> 11.917098\n",
      "INFO - 05/29/20 21:18:38 - 0:00:15 - test_Bafia_mlm_ppl -> 17329.347496\n",
      "INFO - 05/29/20 21:18:38 - 0:00:15 - test_Bafia_mlm_acc -> 12.435233\n",
      "INFO - 05/29/20 21:18:38 - 0:00:15 - test_Bulu_mlm_ppl -> 28076.020250\n",
      "INFO - 05/29/20 21:18:38 - 0:00:15 - test_Bulu_mlm_acc -> 6.735751\n",
      "INFO - 05/29/20 21:18:38 - 0:00:15 - test_mlm_ppl -> 22702.683873\n",
      "INFO - 05/29/20 21:18:38 - 0:00:15 - test_mlm_acc -> 9.585492\n",
      "INFO - 05/29/20 21:18:38 - 0:00:15 - __log__:{\"epoch\": 68, \"valid_Bafia_mlm_ppl\": 13276.172740656073, \"valid_Bafia_mlm_acc\": 13.73056994818653, \"valid_Bulu_mlm_ppl\": 17231.375469428207, \"valid_Bulu_mlm_acc\": 10.103626943005182, \"valid_mlm_ppl\": 15253.77410504214, \"valid_mlm_acc\": 11.917098445595855, \"test_Bafia_mlm_ppl\": 17329.347495957052, \"test_Bafia_mlm_acc\": 12.435233160621761, \"test_Bulu_mlm_ppl\": 28076.02025028376, \"test_Bulu_mlm_acc\": 6.7357512953367875, \"test_mlm_ppl\": 22702.683873120404, \"test_mlm_acc\": 9.585492227979275}\n"
     ]
    }
   ],
   "source": [
    "! ../evaluate.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bafia to MKPAMAN_AMVOE_Ewondo\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "Bulu to Ghomala\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n"
     ]
    }
   ],
   "source": [
    "#because the same dir : put the rename file on place\n",
    "! ../copy_rename.sh $src_path $tgt_path $tgt_pair MKPAMAN_AMVOE_Ewondo-Ghomala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Bafia_Bulu vs Limbum and Ngiemboon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limbum to Bafia\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "Ngiemboon to Bulu\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n"
     ]
    }
   ],
   "source": [
    "! ../copy_rename.sh $src_path $tgt_path Limbum-Ngiemboon $tgt_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/29/20 21:22:12 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/29/20 21:22:12 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_BafiaBulu --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu' --lgs 'Bafia-Bulu' --clm_steps '' --mlm_steps 'Bafia,Bulu' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaBulu/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_BafiaBulu\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Bafia', 1: 'Bulu'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Bafia': 0, 'Bulu': 1}\n",
      "                                     langs: ['Bafia', 'Bulu']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Bafia-Bulu']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Bafia', None), ('Bulu', None)]\n",
      "                                     mono_dataset: {'Bafia': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/train.Bafia.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bafia.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bafia.pth'}, 'Bulu': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/train.Bulu.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bulu.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bulu.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': True, 'valid': True, 'test': True}\n",
      "                                     remove_long_sentences_test: True\n",
      "                                     remove_long_sentences_train: True\n",
      "                                     remove_long_sentences_valid: True\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/29/20 21:22:12 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaBulu/maml\n",
      "                                     \n",
      "INFO - 05/29/20 21:22:12 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_BafiaBulu --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu' --lgs 'Bafia-Bulu' --clm_steps '' --mlm_steps 'Bafia,Bulu' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/29/20 21:22:12 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/29/20 21:22:12 - 0:00:00 - ============ langs: Bafia, Bulu\n",
      "INFO - 05/29/20 21:22:12 - 0:00:00 - ============ Monolingual data (Bafia)\n",
      "INFO - 05/29/20 21:22:12 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bafia.pth ...\n",
      "INFO - 05/29/20 21:22:12 - 0:00:00 - 49216 words (8683 unique) in 792 sentences. 2934 unknown words (101 unique) covering 5.96% of the data.\n",
      "INFO - 05/29/20 21:22:12 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/29/20 21:22:12 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bafia.pth ...\n",
      "INFO - 05/29/20 21:22:12 - 0:00:00 - 48967 words (8683 unique) in 792 sentences. 2812 unknown words (99 unique) covering 5.74% of the data.\n",
      "INFO - 05/29/20 21:22:12 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/29/20 21:22:12 - 0:00:00 - ============ Monolingual data (Bulu)\n",
      "INFO - 05/29/20 21:22:12 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bulu.pth ...\n",
      "INFO - 05/29/20 21:22:12 - 0:00:00 - 80874 words (8683 unique) in 793 sentences. 23668 unknown words (99 unique) covering 29.27% of the data.\n",
      "INFO - 05/29/20 21:22:12 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/29/20 21:22:12 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bulu.pth ...\n",
      "INFO - 05/29/20 21:22:12 - 0:00:00 - 80250 words (8683 unique) in 793 sentences. 23519 unknown words (92 unique) covering 29.31% of the data.\n",
      "INFO - 05/29/20 21:22:12 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/29/20 21:22:12 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/29/20 21:22:12 - 0:00:00 - Monolingual data   - valid -        Bafia:       792\n",
      "INFO - 05/29/20 21:22:12 - 0:00:00 - Monolingual data   -  test -        Bafia:       792\n",
      "INFO - 05/29/20 21:22:12 - 0:00:00 - Monolingual data   - valid -         Bulu:       793\n",
      "INFO - 05/29/20 21:22:12 - 0:00:00 - Monolingual data   -  test -         Bulu:       793\n",
      "\n",
      "INFO - 05/29/20 21:22:12 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(8683, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=8683, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/29/20 21:22:12 - 0:00:01 - Number of parameters (model): 85005803\n",
      "INFO - 05/29/20 21:22:16 - 0:00:04 - Found 0 memories.\n",
      "INFO - 05/29/20 21:22:16 - 0:00:04 - Found 6 FFN.\n",
      "INFO - 05/29/20 21:22:16 - 0:00:04 - Found 102 parameters in model.\n",
      "INFO - 05/29/20 21:22:16 - 0:00:04 - Optimizers: model\n",
      "WARNING - 05/29/20 21:22:16 - 0:00:04 - Reloading checkpoint from /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaBulu/maml/checkpoint.pth ...\n",
      "WARNING - 05/29/20 21:22:17 - 0:00:05 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/29/20 21:22:17 - 0:00:05 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/29/20 21:22:17 - 0:00:05 - Checkpoint reloaded. Resuming at epoch 68 / iteration 4556 ...\n",
      "INFO - 05/29/20 21:22:18 - 0:00:06 - epoch -> 68.000000\n",
      "INFO - 05/29/20 21:22:18 - 0:00:06 - valid_Bafia_mlm_ppl -> 16517.240203\n",
      "INFO - 05/29/20 21:22:18 - 0:00:06 - valid_Bafia_mlm_acc -> 11.139896\n",
      "INFO - 05/29/20 21:22:18 - 0:00:06 - valid_Bulu_mlm_ppl -> 14578.268295\n",
      "INFO - 05/29/20 21:22:18 - 0:00:06 - valid_Bulu_mlm_acc -> 11.658031\n",
      "INFO - 05/29/20 21:22:18 - 0:00:06 - valid_mlm_ppl -> 15547.754249\n",
      "INFO - 05/29/20 21:22:18 - 0:00:06 - valid_mlm_acc -> 11.398964\n",
      "INFO - 05/29/20 21:22:18 - 0:00:06 - test_Bafia_mlm_ppl -> 18053.884178\n",
      "INFO - 05/29/20 21:22:18 - 0:00:06 - test_Bafia_mlm_acc -> 10.103627\n",
      "INFO - 05/29/20 21:22:18 - 0:00:06 - test_Bulu_mlm_ppl -> 28849.012418\n",
      "INFO - 05/29/20 21:22:18 - 0:00:06 - test_Bulu_mlm_acc -> 7.253886\n",
      "INFO - 05/29/20 21:22:18 - 0:00:06 - test_mlm_ppl -> 23451.448298\n",
      "INFO - 05/29/20 21:22:18 - 0:00:06 - test_mlm_acc -> 8.678756\n",
      "INFO - 05/29/20 21:22:18 - 0:00:06 - __log__:{\"epoch\": 68, \"valid_Bafia_mlm_ppl\": 16517.240203257745, \"valid_Bafia_mlm_acc\": 11.139896373056995, \"valid_Bulu_mlm_ppl\": 14578.268294950869, \"valid_Bulu_mlm_acc\": 11.658031088082902, \"valid_mlm_ppl\": 15547.754249104306, \"valid_mlm_acc\": 11.39896373056995, \"test_Bafia_mlm_ppl\": 18053.88417772409, \"test_Bafia_mlm_acc\": 10.103626943005182, \"test_Bulu_mlm_ppl\": 28849.012418184928, \"test_Bulu_mlm_acc\": 7.253886010362694, \"test_mlm_ppl\": 23451.448297954506, \"test_mlm_acc\": 8.678756476683938}\n"
     ]
    }
   ],
   "source": [
    "! ../evaluate.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bafia to Limbum\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "Bulu to Ngiemboon\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n"
     ]
    }
   ],
   "source": [
    "# because the same dir : put the rename file on place\n",
    "! ../copy_rename.sh $src_path $tgt_path $tgt_pair Limbum-Ngiemboon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bafia_Ewondo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: dump_path=/home/jupyter/models/africa/cluster1\n",
      "env: exp_name=mlm_tlm_BafiaEwondo\n",
      "env: data_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo\n",
      "env: lgs=Bafia-MKPAMAN_AMVOE_Ewondo\n",
      "env: mlm_steps=Bafia,MKPAMAN_AMVOE_Ewondo\n",
      "env: tgt_pair=Bafia-MKPAMAN_AMVOE_Ewondo\n",
      "env: src_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo\n",
      "env: tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo\n"
     ]
    }
   ],
   "source": [
    "%env dump_path=/home/jupyter/models/africa/cluster1\n",
    "%env exp_name=mlm_tlm_BafiaEwondo\n",
    "%env data_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo\n",
    "%env lgs=Bafia-MKPAMAN_AMVOE_Ewondo\n",
    "%env mlm_steps=Bafia,MKPAMAN_AMVOE_Ewondo\n",
    "%env tgt_pair=Bafia-MKPAMAN_AMVOE_Ewondo\n",
    "%env src_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo\n",
    "%env tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Bafia_Ewondo vs Bulu and Ghomala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bulu to Bafia\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "Ghomala to MKPAMAN_AMVOE_Ewondo\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/29/20 21:34:23 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/29/20 21:34:23 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_BafiaEwondo --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo' --lgs 'Bafia-MKPAMAN_AMVOE_Ewondo' --clm_steps '' --mlm_steps 'Bafia,MKPAMAN_AMVOE_Ewondo' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaEwondo/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_BafiaEwondo\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Bafia', 1: 'MKPAMAN_AMVOE_Ewondo'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Bafia': 0, 'MKPAMAN_AMVOE_Ewondo': 1}\n",
      "                                     langs: ['Bafia', 'MKPAMAN_AMVOE_Ewondo']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Bafia-MKPAMAN_AMVOE_Ewondo']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Bafia', None), ('MKPAMAN_AMVOE_Ewondo', None)]\n",
      "                                     mono_dataset: {'Bafia': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/train.Bafia.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.Bafia.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.Bafia.pth'}, 'MKPAMAN_AMVOE_Ewondo': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/train.MKPAMAN_AMVOE_Ewondo.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': True, 'valid': True, 'test': True}\n",
      "                                     remove_long_sentences_test: True\n",
      "                                     remove_long_sentences_train: True\n",
      "                                     remove_long_sentences_valid: True\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/29/20 21:34:23 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaEwondo/maml\n",
      "                                     \n",
      "INFO - 05/29/20 21:34:23 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_BafiaEwondo --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo' --lgs 'Bafia-MKPAMAN_AMVOE_Ewondo' --clm_steps '' --mlm_steps 'Bafia,MKPAMAN_AMVOE_Ewondo' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/29/20 21:34:23 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/29/20 21:34:23 - 0:00:00 - ============ langs: Bafia, MKPAMAN_AMVOE_Ewondo\n",
      "INFO - 05/29/20 21:34:23 - 0:00:00 - ============ Monolingual data (Bafia)\n",
      "INFO - 05/29/20 21:34:23 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.Bafia.pth ...\n",
      "INFO - 05/29/20 21:34:23 - 0:00:00 - 127164 words (9128 unique) in 2889 sentences. 7355 unknown words (194 unique) covering 5.78% of the data.\n",
      "INFO - 05/29/20 21:34:23 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/29/20 21:34:23 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.Bafia.pth ...\n",
      "INFO - 05/29/20 21:34:23 - 0:00:00 - 127880 words (9128 unique) in 2889 sentences. 7359 unknown words (206 unique) covering 5.75% of the data.\n",
      "INFO - 05/29/20 21:34:23 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/29/20 21:34:23 - 0:00:00 - ============ Monolingual data (MKPAMAN_AMVOE_Ewondo)\n",
      "INFO - 05/29/20 21:34:23 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/29/20 21:34:23 - 0:00:00 - 46896 words (9128 unique) in 794 sentences. 11305 unknown words (71 unique) covering 24.11% of the data.\n",
      "INFO - 05/29/20 21:34:23 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/29/20 21:34:23 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/29/20 21:34:23 - 0:00:00 - 46753 words (9128 unique) in 794 sentences. 11249 unknown words (60 unique) covering 24.06% of the data.\n",
      "INFO - 05/29/20 21:34:23 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/29/20 21:34:23 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/29/20 21:34:23 - 0:00:00 - Monolingual data   - valid -        Bafia:      2889\n",
      "INFO - 05/29/20 21:34:23 - 0:00:00 - Monolingual data   -  test -        Bafia:      2889\n",
      "INFO - 05/29/20 21:34:23 - 0:00:00 - Monolingual data   - valid - MKPAMAN_AMVOE_Ewondo:       794\n",
      "INFO - 05/29/20 21:34:23 - 0:00:00 - Monolingual data   -  test - MKPAMAN_AMVOE_Ewondo:       794\n",
      "\n",
      "INFO - 05/29/20 21:34:24 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(9128, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=9128, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/29/20 21:34:24 - 0:00:01 - Number of parameters (model): 85461928\n",
      "INFO - 05/29/20 21:34:27 - 0:00:04 - Found 0 memories.\n",
      "INFO - 05/29/20 21:34:27 - 0:00:04 - Found 6 FFN.\n",
      "INFO - 05/29/20 21:34:27 - 0:00:04 - Found 102 parameters in model.\n",
      "INFO - 05/29/20 21:34:27 - 0:00:04 - Optimizers: model\n",
      "WARNING - 05/29/20 21:34:27 - 0:00:04 - Reloading checkpoint from /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaEwondo/maml/checkpoint.pth ...\n",
      "WARNING - 05/29/20 21:34:39 - 0:00:16 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/29/20 21:34:39 - 0:00:16 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/29/20 21:34:39 - 0:00:16 - Checkpoint reloaded. Resuming at epoch 40 / iteration 5320 ...\n",
      "INFO - 05/29/20 21:34:40 - 0:00:17 - epoch -> 40.000000\n",
      "INFO - 05/29/20 21:34:40 - 0:00:17 - valid_Bafia_mlm_ppl -> 5453.634113\n",
      "INFO - 05/29/20 21:34:40 - 0:00:17 - valid_Bafia_mlm_acc -> 14.507772\n",
      "INFO - 05/29/20 21:34:40 - 0:00:17 - valid_MKPAMAN_AMVOE_Ewondo_mlm_ppl -> 8168.744218\n",
      "INFO - 05/29/20 21:34:40 - 0:00:17 - valid_MKPAMAN_AMVOE_Ewondo_mlm_acc -> 11.139896\n",
      "INFO - 05/29/20 21:34:40 - 0:00:17 - valid_mlm_ppl -> 6811.189165\n",
      "INFO - 05/29/20 21:34:40 - 0:00:17 - valid_mlm_acc -> 12.823834\n",
      "INFO - 05/29/20 21:34:40 - 0:00:17 - test_Bafia_mlm_ppl -> 12095.695955\n",
      "INFO - 05/29/20 21:34:40 - 0:00:17 - test_Bafia_mlm_acc -> 6.217617\n",
      "INFO - 05/29/20 21:34:40 - 0:00:17 - test_MKPAMAN_AMVOE_Ewondo_mlm_ppl -> 15838.565415\n",
      "INFO - 05/29/20 21:34:40 - 0:00:17 - test_MKPAMAN_AMVOE_Ewondo_mlm_acc -> 5.958549\n",
      "INFO - 05/29/20 21:34:40 - 0:00:17 - test_mlm_ppl -> 13967.130685\n",
      "INFO - 05/29/20 21:34:40 - 0:00:17 - test_mlm_acc -> 6.088083\n",
      "INFO - 05/29/20 21:34:40 - 0:00:17 - __log__:{\"epoch\": 40, \"valid_Bafia_mlm_ppl\": 5453.634112610449, \"valid_Bafia_mlm_acc\": 14.507772020725389, \"valid_MKPAMAN_AMVOE_Ewondo_mlm_ppl\": 8168.744217733733, \"valid_MKPAMAN_AMVOE_Ewondo_mlm_acc\": 11.139896373056995, \"valid_mlm_ppl\": 6811.189165172091, \"valid_mlm_acc\": 12.823834196891191, \"test_Bafia_mlm_ppl\": 12095.695954507135, \"test_Bafia_mlm_acc\": 6.217616580310881, \"test_MKPAMAN_AMVOE_Ewondo_mlm_ppl\": 15838.565415088604, \"test_MKPAMAN_AMVOE_Ewondo_mlm_acc\": 5.958549222797927, \"test_mlm_ppl\": 13967.13068479787, \"test_mlm_acc\": 6.0880829015544045}\n",
      "Bafia to Bulu\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "MKPAMAN_AMVOE_Ewondo to Ghomala\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n"
     ]
    }
   ],
   "source": [
    "! ../copy_rename.sh $src_path $tgt_path Bulu-Ghomala $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : put the rename file on place\n",
    "! ../copy_rename.sh $src_path $tgt_path $tgt_pair Bulu-Ghomala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limbum to Bafia\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "Ngiemboon to MKPAMAN_AMVOE_Ewondo\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/29/20 21:35:03 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/29/20 21:35:03 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_BafiaEwondo --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo' --lgs 'Bafia-MKPAMAN_AMVOE_Ewondo' --clm_steps '' --mlm_steps 'Bafia,MKPAMAN_AMVOE_Ewondo' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaEwondo/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_BafiaEwondo\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Bafia', 1: 'MKPAMAN_AMVOE_Ewondo'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Bafia': 0, 'MKPAMAN_AMVOE_Ewondo': 1}\n",
      "                                     langs: ['Bafia', 'MKPAMAN_AMVOE_Ewondo']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Bafia-MKPAMAN_AMVOE_Ewondo']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Bafia', None), ('MKPAMAN_AMVOE_Ewondo', None)]\n",
      "                                     mono_dataset: {'Bafia': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/train.Bafia.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.Bafia.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.Bafia.pth'}, 'MKPAMAN_AMVOE_Ewondo': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/train.MKPAMAN_AMVOE_Ewondo.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': True, 'valid': True, 'test': True}\n",
      "                                     remove_long_sentences_test: True\n",
      "                                     remove_long_sentences_train: True\n",
      "                                     remove_long_sentences_valid: True\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/29/20 21:35:03 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaEwondo/maml\n",
      "                                     \n",
      "INFO - 05/29/20 21:35:03 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_BafiaEwondo --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo' --lgs 'Bafia-MKPAMAN_AMVOE_Ewondo' --clm_steps '' --mlm_steps 'Bafia,MKPAMAN_AMVOE_Ewondo' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/29/20 21:35:03 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/29/20 21:35:03 - 0:00:00 - ============ langs: Bafia, MKPAMAN_AMVOE_Ewondo\n",
      "INFO - 05/29/20 21:35:03 - 0:00:00 - ============ Monolingual data (Bafia)\n",
      "INFO - 05/29/20 21:35:03 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.Bafia.pth ...\n",
      "INFO - 05/29/20 21:35:03 - 0:00:00 - 49193 words (9128 unique) in 792 sentences. 1317 unknown words (68 unique) covering 2.68% of the data.\n",
      "INFO - 05/29/20 21:35:03 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/29/20 21:35:03 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.Bafia.pth ...\n",
      "INFO - 05/29/20 21:35:03 - 0:00:00 - 48934 words (9128 unique) in 792 sentences. 1320 unknown words (69 unique) covering 2.70% of the data.\n",
      "INFO - 05/29/20 21:35:03 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/29/20 21:35:03 - 0:00:00 - ============ Monolingual data (MKPAMAN_AMVOE_Ewondo)\n",
      "INFO - 05/29/20 21:35:03 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/29/20 21:35:03 - 0:00:00 - 82319 words (9128 unique) in 793 sentences. 21496 unknown words (66 unique) covering 26.11% of the data.\n",
      "INFO - 05/29/20 21:35:03 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/29/20 21:35:03 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/29/20 21:35:03 - 0:00:00 - 81950 words (9128 unique) in 793 sentences. 21321 unknown words (65 unique) covering 26.02% of the data.\n",
      "INFO - 05/29/20 21:35:03 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/29/20 21:35:03 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/29/20 21:35:03 - 0:00:00 - Monolingual data   - valid -        Bafia:       792\n",
      "INFO - 05/29/20 21:35:03 - 0:00:00 - Monolingual data   -  test -        Bafia:       792\n",
      "INFO - 05/29/20 21:35:03 - 0:00:00 - Monolingual data   - valid - MKPAMAN_AMVOE_Ewondo:       793\n",
      "INFO - 05/29/20 21:35:03 - 0:00:00 - Monolingual data   -  test - MKPAMAN_AMVOE_Ewondo:       793\n",
      "\n",
      "INFO - 05/29/20 21:35:04 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(9128, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=9128, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/29/20 21:35:04 - 0:00:01 - Number of parameters (model): 85461928\n",
      "INFO - 05/29/20 21:35:08 - 0:00:04 - Found 0 memories.\n",
      "INFO - 05/29/20 21:35:08 - 0:00:04 - Found 6 FFN.\n",
      "INFO - 05/29/20 21:35:08 - 0:00:04 - Found 102 parameters in model.\n",
      "INFO - 05/29/20 21:35:08 - 0:00:04 - Optimizers: model\n",
      "WARNING - 05/29/20 21:35:08 - 0:00:04 - Reloading checkpoint from /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaEwondo/maml/checkpoint.pth ...\n",
      "WARNING - 05/29/20 21:35:08 - 0:00:05 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/29/20 21:35:08 - 0:00:05 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/29/20 21:35:08 - 0:00:05 - Checkpoint reloaded. Resuming at epoch 40 / iteration 5320 ...\n",
      "INFO - 05/29/20 21:35:09 - 0:00:06 - epoch -> 40.000000\n",
      "INFO - 05/29/20 21:35:09 - 0:00:06 - valid_Bafia_mlm_ppl -> 5818.810256\n",
      "INFO - 05/29/20 21:35:09 - 0:00:06 - valid_Bafia_mlm_acc -> 11.139896\n",
      "INFO - 05/29/20 21:35:09 - 0:00:06 - valid_MKPAMAN_AMVOE_Ewondo_mlm_ppl -> 13904.585656\n",
      "INFO - 05/29/20 21:35:09 - 0:00:06 - valid_MKPAMAN_AMVOE_Ewondo_mlm_acc -> 5.440415\n",
      "INFO - 05/29/20 21:35:09 - 0:00:06 - valid_mlm_ppl -> 9861.697956\n",
      "INFO - 05/29/20 21:35:09 - 0:00:06 - valid_mlm_acc -> 8.290155\n",
      "INFO - 05/29/20 21:35:09 - 0:00:06 - test_Bafia_mlm_ppl -> 6198.440898\n",
      "INFO - 05/29/20 21:35:09 - 0:00:06 - test_Bafia_mlm_acc -> 11.139896\n",
      "INFO - 05/29/20 21:35:09 - 0:00:06 - test_MKPAMAN_AMVOE_Ewondo_mlm_ppl -> 9768.934479\n",
      "INFO - 05/29/20 21:35:09 - 0:00:06 - test_MKPAMAN_AMVOE_Ewondo_mlm_acc -> 9.585492\n",
      "INFO - 05/29/20 21:35:09 - 0:00:06 - test_mlm_ppl -> 7983.687689\n",
      "INFO - 05/29/20 21:35:09 - 0:00:06 - test_mlm_acc -> 10.362694\n",
      "INFO - 05/29/20 21:35:09 - 0:00:06 - __log__:{\"epoch\": 40, \"valid_Bafia_mlm_ppl\": 5818.8102556212125, \"valid_Bafia_mlm_acc\": 11.139896373056995, \"valid_MKPAMAN_AMVOE_Ewondo_mlm_ppl\": 13904.585656406767, \"valid_MKPAMAN_AMVOE_Ewondo_mlm_acc\": 5.4404145077720205, \"valid_mlm_ppl\": 9861.69795601399, \"valid_mlm_acc\": 8.290155440414509, \"test_Bafia_mlm_ppl\": 6198.440898431398, \"test_Bafia_mlm_acc\": 11.139896373056995, \"test_MKPAMAN_AMVOE_Ewondo_mlm_ppl\": 9768.93447943443, \"test_MKPAMAN_AMVOE_Ewondo_mlm_acc\": 9.585492227979275, \"test_mlm_ppl\": 7983.687688932914, \"test_mlm_acc\": 10.362694300518136}\n",
      "Bafia to Limbum\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "MKPAMAN_AMVOE_Ewondo to Ngiemboon\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n"
     ]
    }
   ],
   "source": [
    "###### Bafia_Ewondo vs Limbum and Ngiemboon\n",
    "! ../copy_rename.sh $src_path $tgt_path Limbum-Ngiemboon $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : put the rename file on place\n",
    "! ../copy_rename.sh $src_path $tgt_path $tgt_pair Limbum-Ngiemboon "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bulu_Ewondo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: dump_path=/home/jupyter/models/africa/cluster1\n",
      "env: exp_name=mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo\n",
      "env: data_path=/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo\n",
      "env: lgs=Bulu-MKPAMAN_AMVOE_Ewondo\n",
      "env: mlm_steps=Bulu,MKPAMAN_AMVOE_Ewondo\n",
      "env: tgt_pair=Bulu-MKPAMAN_AMVOE_Ewondo\n",
      "env: src_path=/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo\n",
      "env: tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo\n"
     ]
    }
   ],
   "source": [
    "%env dump_path=/home/jupyter/models/africa/cluster1\n",
    "%env exp_name=mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo\n",
    "%env data_path=/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo\n",
    "%env lgs=Bulu-MKPAMAN_AMVOE_Ewondo\n",
    "%env mlm_steps=Bulu,MKPAMAN_AMVOE_Ewondo\n",
    "%env tgt_pair=Bulu-MKPAMAN_AMVOE_Ewondo\n",
    "%env src_path=/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo\n",
    "%env tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Bulu_Ewondo vs Bafia and Ghomala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bafia to Bulu\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "Ghomala to MKPAMAN_AMVOE_Ewondo\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/29/20 21:40:49 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/29/20 21:40:49 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo' --lgs 'Bulu-MKPAMAN_AMVOE_Ewondo' --clm_steps '' --mlm_steps 'Bulu,MKPAMAN_AMVOE_Ewondo' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster1/mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Bulu', 1: 'MKPAMAN_AMVOE_Ewondo'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Bulu': 0, 'MKPAMAN_AMVOE_Ewondo': 1}\n",
      "                                     langs: ['Bulu', 'MKPAMAN_AMVOE_Ewondo']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Bulu-MKPAMAN_AMVOE_Ewondo']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Bulu', None), ('MKPAMAN_AMVOE_Ewondo', None)]\n",
      "                                     mono_dataset: {'Bulu': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/train.Bulu.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.Bulu.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.Bulu.pth'}, 'MKPAMAN_AMVOE_Ewondo': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/train.MKPAMAN_AMVOE_Ewondo.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': True, 'valid': True, 'test': True}\n",
      "                                     remove_long_sentences_test: True\n",
      "                                     remove_long_sentences_train: True\n",
      "                                     remove_long_sentences_valid: True\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/29/20 21:40:49 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster1/mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo/maml\n",
      "                                     \n",
      "INFO - 05/29/20 21:40:49 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo' --lgs 'Bulu-MKPAMAN_AMVOE_Ewondo' --clm_steps '' --mlm_steps 'Bulu,MKPAMAN_AMVOE_Ewondo' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/29/20 21:40:49 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/29/20 21:40:49 - 0:00:00 - ============ langs: Bulu, MKPAMAN_AMVOE_Ewondo\n",
      "INFO - 05/29/20 21:40:49 - 0:00:00 - ============ Monolingual data (Bulu)\n",
      "INFO - 05/29/20 21:40:49 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.Bulu.pth ...\n",
      "INFO - 05/29/20 21:40:49 - 0:00:00 - 63509 words (9061 unique) in 795 sentences. 21991 unknown words (77 unique) covering 34.63% of the data.\n",
      "INFO - 05/29/20 21:40:49 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/29/20 21:40:49 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.Bulu.pth ...\n",
      "INFO - 05/29/20 21:40:49 - 0:00:00 - 61798 words (9061 unique) in 795 sentences. 21546 unknown words (78 unique) covering 34.87% of the data.\n",
      "INFO - 05/29/20 21:40:49 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/29/20 21:40:49 - 0:00:00 - ============ Monolingual data (MKPAMAN_AMVOE_Ewondo)\n",
      "INFO - 05/29/20 21:40:49 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/29/20 21:40:49 - 0:00:00 - 51401 words (9061 unique) in 794 sentences. 16583 unknown words (69 unique) covering 32.26% of the data.\n",
      "INFO - 05/29/20 21:40:49 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/29/20 21:40:49 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/29/20 21:40:49 - 0:00:00 - 51295 words (9061 unique) in 794 sentences. 16397 unknown words (63 unique) covering 31.97% of the data.\n",
      "INFO - 05/29/20 21:40:49 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/29/20 21:40:49 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/29/20 21:40:49 - 0:00:00 - Monolingual data   - valid -         Bulu:       795\n",
      "INFO - 05/29/20 21:40:49 - 0:00:00 - Monolingual data   -  test -         Bulu:       795\n",
      "INFO - 05/29/20 21:40:49 - 0:00:00 - Monolingual data   - valid - MKPAMAN_AMVOE_Ewondo:       794\n",
      "INFO - 05/29/20 21:40:49 - 0:00:00 - Monolingual data   -  test - MKPAMAN_AMVOE_Ewondo:       794\n",
      "\n",
      "INFO - 05/29/20 21:40:50 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(9061, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=9061, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/29/20 21:40:50 - 0:00:01 - Number of parameters (model): 85393253\n",
      "INFO - 05/29/20 21:40:53 - 0:00:04 - Found 0 memories.\n",
      "INFO - 05/29/20 21:40:53 - 0:00:04 - Found 6 FFN.\n",
      "INFO - 05/29/20 21:40:53 - 0:00:04 - Found 102 parameters in model.\n",
      "INFO - 05/29/20 21:40:53 - 0:00:04 - Optimizers: model\n",
      "WARNING - 05/29/20 21:40:53 - 0:00:04 - Reloading checkpoint from /home/jupyter/models/africa/cluster1/mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo/maml/checkpoint.pth ...\n",
      "WARNING - 05/29/20 21:41:03 - 0:00:14 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/29/20 21:41:03 - 0:00:14 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/29/20 21:41:03 - 0:00:14 - Checkpoint reloaded. Resuming at epoch 32 / iteration 8480 ...\n",
      "INFO - 05/29/20 21:41:05 - 0:00:15 - epoch -> 32.000000\n",
      "INFO - 05/29/20 21:41:05 - 0:00:15 - valid_Bulu_mlm_ppl -> 14015.504567\n",
      "INFO - 05/29/20 21:41:05 - 0:00:15 - valid_Bulu_mlm_acc -> 12.176166\n",
      "INFO - 05/29/20 21:41:05 - 0:00:15 - valid_MKPAMAN_AMVOE_Ewondo_mlm_ppl -> 67294.511602\n",
      "INFO - 05/29/20 21:41:05 - 0:00:15 - valid_MKPAMAN_AMVOE_Ewondo_mlm_acc -> 2.331606\n",
      "INFO - 05/29/20 21:41:05 - 0:00:15 - valid_mlm_ppl -> 40655.008084\n",
      "INFO - 05/29/20 21:41:05 - 0:00:15 - valid_mlm_acc -> 7.253886\n",
      "INFO - 05/29/20 21:41:05 - 0:00:15 - test_Bulu_mlm_ppl -> 21165.312705\n",
      "INFO - 05/29/20 21:41:05 - 0:00:15 - test_Bulu_mlm_acc -> 9.326425\n",
      "INFO - 05/29/20 21:41:05 - 0:00:15 - test_MKPAMAN_AMVOE_Ewondo_mlm_ppl -> 33976.706232\n",
      "INFO - 05/29/20 21:41:05 - 0:00:15 - test_MKPAMAN_AMVOE_Ewondo_mlm_acc -> 7.512953\n",
      "INFO - 05/29/20 21:41:05 - 0:00:15 - test_mlm_ppl -> 27571.009469\n",
      "INFO - 05/29/20 21:41:05 - 0:00:15 - test_mlm_acc -> 8.419689\n",
      "INFO - 05/29/20 21:41:05 - 0:00:15 - __log__:{\"epoch\": 32, \"valid_Bulu_mlm_ppl\": 14015.504566652196, \"valid_Bulu_mlm_acc\": 12.176165803108809, \"valid_MKPAMAN_AMVOE_Ewondo_mlm_ppl\": 67294.51160184479, \"valid_MKPAMAN_AMVOE_Ewondo_mlm_acc\": 2.33160621761658, \"valid_mlm_ppl\": 40655.00808424849, \"valid_mlm_acc\": 7.253886010362694, \"test_Bulu_mlm_ppl\": 21165.312705262433, \"test_Bulu_mlm_acc\": 9.32642487046632, \"test_MKPAMAN_AMVOE_Ewondo_mlm_ppl\": 33976.70623202726, \"test_MKPAMAN_AMVOE_Ewondo_mlm_acc\": 7.512953367875648, \"test_mlm_ppl\": 27571.009468644846, \"test_mlm_acc\": 8.419689119170984}\n",
      "Bulu to Bafia\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "MKPAMAN_AMVOE_Ewondo to Ghomala\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n"
     ]
    }
   ],
   "source": [
    "! ../copy_rename.sh $src_path $tgt_path Bafia-Ghomala $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : put the rename file on place\n",
    "! ../copy_rename.sh $src_path $tgt_path $tgt_pair Bafia-Ghomala "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Bulu_Ewondo vs Limbum and Ngiemboon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limbum to Bulu\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "Ngiemboon to MKPAMAN_AMVOE_Ewondo\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/29/20 21:41:24 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/29/20 21:41:24 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo' --lgs 'Bulu-MKPAMAN_AMVOE_Ewondo' --clm_steps '' --mlm_steps 'Bulu,MKPAMAN_AMVOE_Ewondo' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster1/mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Bulu', 1: 'MKPAMAN_AMVOE_Ewondo'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Bulu': 0, 'MKPAMAN_AMVOE_Ewondo': 1}\n",
      "                                     langs: ['Bulu', 'MKPAMAN_AMVOE_Ewondo']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Bulu-MKPAMAN_AMVOE_Ewondo']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Bulu', None), ('MKPAMAN_AMVOE_Ewondo', None)]\n",
      "                                     mono_dataset: {'Bulu': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/train.Bulu.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.Bulu.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.Bulu.pth'}, 'MKPAMAN_AMVOE_Ewondo': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/train.MKPAMAN_AMVOE_Ewondo.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': True, 'valid': True, 'test': True}\n",
      "                                     remove_long_sentences_test: True\n",
      "                                     remove_long_sentences_train: True\n",
      "                                     remove_long_sentences_valid: True\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/29/20 21:41:24 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster1/mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo/maml\n",
      "                                     \n",
      "INFO - 05/29/20 21:41:24 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo' --lgs 'Bulu-MKPAMAN_AMVOE_Ewondo' --clm_steps '' --mlm_steps 'Bulu,MKPAMAN_AMVOE_Ewondo' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/29/20 21:41:24 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/29/20 21:41:24 - 0:00:00 - ============ langs: Bulu, MKPAMAN_AMVOE_Ewondo\n",
      "INFO - 05/29/20 21:41:24 - 0:00:00 - ============ Monolingual data (Bulu)\n",
      "INFO - 05/29/20 21:41:24 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.Bulu.pth ...\n",
      "INFO - 05/29/20 21:41:24 - 0:00:00 - 53665 words (9061 unique) in 792 sentences. 3026 unknown words (73 unique) covering 5.64% of the data.\n",
      "INFO - 05/29/20 21:41:24 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/29/20 21:41:24 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.Bulu.pth ...\n",
      "INFO - 05/29/20 21:41:24 - 0:00:00 - 53291 words (9061 unique) in 792 sentences. 3080 unknown words (77 unique) covering 5.78% of the data.\n",
      "INFO - 05/29/20 21:41:24 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/29/20 21:41:24 - 0:00:00 - ============ Monolingual data (MKPAMAN_AMVOE_Ewondo)\n",
      "INFO - 05/29/20 21:41:24 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/29/20 21:41:24 - 0:00:00 - 84591 words (9061 unique) in 793 sentences. 26386 unknown words (78 unique) covering 31.19% of the data.\n",
      "INFO - 05/29/20 21:41:24 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/29/20 21:41:24 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/29/20 21:41:24 - 0:00:00 - 83972 words (9061 unique) in 793 sentences. 26088 unknown words (80 unique) covering 31.07% of the data.\n",
      "INFO - 05/29/20 21:41:24 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/29/20 21:41:24 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/29/20 21:41:24 - 0:00:00 - Monolingual data   - valid -         Bulu:       792\n",
      "INFO - 05/29/20 21:41:24 - 0:00:00 - Monolingual data   -  test -         Bulu:       792\n",
      "INFO - 05/29/20 21:41:24 - 0:00:00 - Monolingual data   - valid - MKPAMAN_AMVOE_Ewondo:       793\n",
      "INFO - 05/29/20 21:41:24 - 0:00:00 - Monolingual data   -  test - MKPAMAN_AMVOE_Ewondo:       793\n",
      "\n",
      "INFO - 05/29/20 21:41:24 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(9061, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=9061, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/29/20 21:41:24 - 0:00:01 - Number of parameters (model): 85393253\n",
      "INFO - 05/29/20 21:41:28 - 0:00:04 - Found 0 memories.\n",
      "INFO - 05/29/20 21:41:28 - 0:00:04 - Found 6 FFN.\n",
      "INFO - 05/29/20 21:41:28 - 0:00:04 - Found 102 parameters in model.\n",
      "INFO - 05/29/20 21:41:28 - 0:00:04 - Optimizers: model\n",
      "WARNING - 05/29/20 21:41:28 - 0:00:04 - Reloading checkpoint from /home/jupyter/models/africa/cluster1/mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo/maml/checkpoint.pth ...\n",
      "WARNING - 05/29/20 21:41:29 - 0:00:05 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/29/20 21:41:29 - 0:00:05 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/29/20 21:41:29 - 0:00:05 - Checkpoint reloaded. Resuming at epoch 32 / iteration 8480 ...\n",
      "INFO - 05/29/20 21:41:30 - 0:00:06 - epoch -> 32.000000\n",
      "INFO - 05/29/20 21:41:30 - 0:00:06 - valid_Bulu_mlm_ppl -> 16004.677143\n",
      "INFO - 05/29/20 21:41:30 - 0:00:06 - valid_Bulu_mlm_acc -> 4.663212\n",
      "INFO - 05/29/20 21:41:30 - 0:00:06 - valid_MKPAMAN_AMVOE_Ewondo_mlm_ppl -> 34627.958045\n",
      "INFO - 05/29/20 21:41:30 - 0:00:06 - valid_MKPAMAN_AMVOE_Ewondo_mlm_acc -> 7.772021\n",
      "INFO - 05/29/20 21:41:30 - 0:00:06 - valid_mlm_ppl -> 25316.317594\n",
      "INFO - 05/29/20 21:41:30 - 0:00:06 - valid_mlm_acc -> 6.217617\n",
      "INFO - 05/29/20 21:41:30 - 0:00:06 - test_Bulu_mlm_ppl -> 5647.750355\n",
      "INFO - 05/29/20 21:41:30 - 0:00:06 - test_Bulu_mlm_acc -> 10.362694\n",
      "INFO - 05/29/20 21:41:30 - 0:00:06 - test_MKPAMAN_AMVOE_Ewondo_mlm_ppl -> 24776.221700\n",
      "INFO - 05/29/20 21:41:30 - 0:00:06 - test_MKPAMAN_AMVOE_Ewondo_mlm_acc -> 10.103627\n",
      "INFO - 05/29/20 21:41:30 - 0:00:06 - test_mlm_ppl -> 15211.986028\n",
      "INFO - 05/29/20 21:41:30 - 0:00:06 - test_mlm_acc -> 10.233161\n",
      "INFO - 05/29/20 21:41:30 - 0:00:06 - __log__:{\"epoch\": 32, \"valid_Bulu_mlm_ppl\": 16004.677142990246, \"valid_Bulu_mlm_acc\": 4.66321243523316, \"valid_MKPAMAN_AMVOE_Ewondo_mlm_ppl\": 34627.95804540264, \"valid_MKPAMAN_AMVOE_Ewondo_mlm_acc\": 7.772020725388601, \"valid_mlm_ppl\": 25316.317594196444, \"valid_mlm_acc\": 6.217616580310881, \"test_Bulu_mlm_ppl\": 5647.7503551467, \"test_Bulu_mlm_acc\": 10.362694300518134, \"test_MKPAMAN_AMVOE_Ewondo_mlm_ppl\": 24776.221700068065, \"test_MKPAMAN_AMVOE_Ewondo_mlm_acc\": 10.103626943005182, \"test_mlm_ppl\": 15211.986027607381, \"test_mlm_acc\": 10.233160621761659}\n",
      "Bulu to Limbum\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "MKPAMAN_AMVOE_Ewondo to Ngiemboon\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n"
     ]
    }
   ],
   "source": [
    "! ../copy_rename.sh $src_path $tgt_path Limbum-Ngiemboon $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : put the rename file on place\n",
    "! ../copy_rename.sh $src_path $tgt_path $tgt_pair Limbum-Ngiemboon "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ghomala_Limbum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: dump_path=/home/jupyter/models/africa/cluster3\n",
      "env: exp_name=mlm_tlm_GhomalaLimbum\n",
      "env: data_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum\n",
      "env: lgs=Ghomala-Limbum\n",
      "env: mlm_steps=Ghomala,Limbum\n",
      "env: tgt_pair=Ghomala-Limbum\n",
      "env: src_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum\n",
      "env: tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum\n"
     ]
    }
   ],
   "source": [
    "%env dump_path=/home/jupyter/models/africa/cluster3\n",
    "%env exp_name=mlm_tlm_GhomalaLimbum\n",
    "%env data_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum\n",
    "%env lgs=Ghomala-Limbum\n",
    "%env mlm_steps=Ghomala,Limbum\n",
    "%env tgt_pair=Ghomala-Limbum\n",
    "%env src_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum\n",
    "%env tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ghomala_Limbum vs Ngiemboon and Bafia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngiemboon to Ghomala\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "Bafia to Limbum\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/29/20 21:47:51 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/29/20 21:47:51 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaLimbum --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum' --lgs 'Ghomala-Limbum' --clm_steps '' --mlm_steps 'Ghomala,Limbum' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_GhomalaLimbum\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Ghomala', 1: 'Limbum'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Ghomala': 0, 'Limbum': 1}\n",
      "                                     langs: ['Ghomala', 'Limbum']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Ghomala-Limbum']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Ghomala', None), ('Limbum', None)]\n",
      "                                     mono_dataset: {'Ghomala': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/train.Ghomala.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Ghomala.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Ghomala.pth'}, 'Limbum': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/train.Limbum.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Limbum.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Limbum.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': True, 'valid': True, 'test': True}\n",
      "                                     remove_long_sentences_test: True\n",
      "                                     remove_long_sentences_train: True\n",
      "                                     remove_long_sentences_valid: True\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/29/20 21:47:51 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml\n",
      "                                     \n",
      "INFO - 05/29/20 21:47:51 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaLimbum --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum' --lgs 'Ghomala-Limbum' --clm_steps '' --mlm_steps 'Ghomala,Limbum' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/29/20 21:47:51 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/29/20 21:47:51 - 0:00:00 - ============ langs: Ghomala, Limbum\n",
      "INFO - 05/29/20 21:47:51 - 0:00:00 - ============ Monolingual data (Ghomala)\n",
      "INFO - 05/29/20 21:47:51 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Ghomala.pth ...\n",
      "INFO - 05/29/20 21:47:51 - 0:00:00 - 80756 words (5057 unique) in 793 sentences. 32486 unknown words (198 unique) covering 40.23% of the data.\n",
      "INFO - 05/29/20 21:47:51 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/29/20 21:47:51 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Ghomala.pth ...\n",
      "INFO - 05/29/20 21:47:51 - 0:00:00 - 80177 words (5057 unique) in 793 sentences. 32280 unknown words (186 unique) covering 40.26% of the data.\n",
      "INFO - 05/29/20 21:47:51 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/29/20 21:47:51 - 0:00:00 - ============ Monolingual data (Limbum)\n",
      "INFO - 05/29/20 21:47:51 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Limbum.pth ...\n",
      "INFO - 05/29/20 21:47:51 - 0:00:00 - 56604 words (5057 unique) in 795 sentences. 17463 unknown words (227 unique) covering 30.85% of the data.\n",
      "INFO - 05/29/20 21:47:51 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/29/20 21:47:51 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Limbum.pth ...\n",
      "INFO - 05/29/20 21:47:51 - 0:00:00 - 54990 words (5057 unique) in 795 sentences. 17072 unknown words (232 unique) covering 31.05% of the data.\n",
      "INFO - 05/29/20 21:47:51 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/29/20 21:47:51 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/29/20 21:47:51 - 0:00:00 - Monolingual data   - valid -      Ghomala:       793\n",
      "INFO - 05/29/20 21:47:51 - 0:00:00 - Monolingual data   -  test -      Ghomala:       793\n",
      "INFO - 05/29/20 21:47:51 - 0:00:00 - Monolingual data   - valid -       Limbum:       795\n",
      "INFO - 05/29/20 21:47:51 - 0:00:00 - Monolingual data   -  test -       Limbum:       795\n",
      "\n",
      "INFO - 05/29/20 21:47:52 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(5057, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=5057, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/29/20 21:47:52 - 0:00:01 - Number of parameters (model): 81289153\n",
      "INFO - 05/29/20 21:47:55 - 0:00:04 - Found 0 memories.\n",
      "INFO - 05/29/20 21:47:55 - 0:00:04 - Found 6 FFN.\n",
      "INFO - 05/29/20 21:47:55 - 0:00:04 - Found 102 parameters in model.\n",
      "INFO - 05/29/20 21:47:55 - 0:00:04 - Optimizers: model\n",
      "WARNING - 05/29/20 21:47:55 - 0:00:04 - Reloading checkpoint from /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml/checkpoint.pth ...\n",
      "WARNING - 05/29/20 21:48:04 - 0:00:13 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/29/20 21:48:04 - 0:00:13 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/29/20 21:48:04 - 0:00:13 - Checkpoint reloaded. Resuming at epoch 52 / iteration 13728 ...\n",
      "INFO - 05/29/20 21:48:05 - 0:00:14 - epoch -> 52.000000\n",
      "INFO - 05/29/20 21:48:05 - 0:00:14 - valid_Ghomala_mlm_ppl -> 35780.008528\n",
      "INFO - 05/29/20 21:48:05 - 0:00:14 - valid_Ghomala_mlm_acc -> 6.476684\n",
      "INFO - 05/29/20 21:48:05 - 0:00:14 - valid_Limbum_mlm_ppl -> 22817.323187\n",
      "INFO - 05/29/20 21:48:05 - 0:00:14 - valid_Limbum_mlm_acc -> 10.103627\n",
      "INFO - 05/29/20 21:48:05 - 0:00:14 - valid_mlm_ppl -> 29298.665858\n",
      "INFO - 05/29/20 21:48:05 - 0:00:14 - valid_mlm_acc -> 8.290155\n",
      "INFO - 05/29/20 21:48:05 - 0:00:14 - test_Ghomala_mlm_ppl -> 26213.762196\n",
      "INFO - 05/29/20 21:48:05 - 0:00:14 - test_Ghomala_mlm_acc -> 10.362694\n",
      "INFO - 05/29/20 21:48:05 - 0:00:14 - test_Limbum_mlm_ppl -> 45974.770890\n",
      "INFO - 05/29/20 21:48:05 - 0:00:14 - test_Limbum_mlm_acc -> 4.145078\n",
      "INFO - 05/29/20 21:48:05 - 0:00:14 - test_mlm_ppl -> 36094.266543\n",
      "INFO - 05/29/20 21:48:05 - 0:00:14 - test_mlm_acc -> 7.253886\n",
      "INFO - 05/29/20 21:48:05 - 0:00:14 - __log__:{\"epoch\": 52, \"valid_Ghomala_mlm_ppl\": 35780.008527997976, \"valid_Ghomala_mlm_acc\": 6.476683937823834, \"valid_Limbum_mlm_ppl\": 22817.323187481452, \"valid_Limbum_mlm_acc\": 10.103626943005182, \"valid_mlm_ppl\": 29298.665857739714, \"valid_mlm_acc\": 8.290155440414509, \"test_Ghomala_mlm_ppl\": 26213.762195861564, \"test_Ghomala_mlm_acc\": 10.362694300518134, \"test_Limbum_mlm_ppl\": 45974.77089014123, \"test_Limbum_mlm_acc\": 4.1450777202072535, \"test_mlm_ppl\": 36094.2665430014, \"test_mlm_acc\": 7.253886010362693}\n",
      "Ghomala to Ngiemboon\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "Limbum to Bafia\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n"
     ]
    }
   ],
   "source": [
    "! ../copy_rename.sh $src_path $tgt_path Ngiemboon-Bafia $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : put the rename file on place\n",
    "! ../copy_rename.sh $src_path $tgt_path $tgt_pair Ngiemboon-Bafia "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Ghomala_Limbum vs Bulu and Ewondo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bulu to Ghomala\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "MKPAMAN_AMVOE_Ewondo to Limbum\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/29/20 21:48:37 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/29/20 21:48:37 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaLimbum --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum' --lgs 'Ghomala-Limbum' --clm_steps '' --mlm_steps 'Ghomala,Limbum' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_GhomalaLimbum\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Ghomala', 1: 'Limbum'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Ghomala': 0, 'Limbum': 1}\n",
      "                                     langs: ['Ghomala', 'Limbum']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Ghomala-Limbum']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Ghomala', None), ('Limbum', None)]\n",
      "                                     mono_dataset: {'Ghomala': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/train.Ghomala.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Ghomala.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Ghomala.pth'}, 'Limbum': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/train.Limbum.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Limbum.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Limbum.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': True, 'valid': True, 'test': True}\n",
      "                                     remove_long_sentences_test: True\n",
      "                                     remove_long_sentences_train: True\n",
      "                                     remove_long_sentences_valid: True\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/29/20 21:48:37 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml\n",
      "                                     \n",
      "INFO - 05/29/20 21:48:37 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaLimbum --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum' --lgs 'Ghomala-Limbum' --clm_steps '' --mlm_steps 'Ghomala,Limbum' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/29/20 21:48:37 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/29/20 21:48:37 - 0:00:00 - ============ langs: Ghomala, Limbum\n",
      "INFO - 05/29/20 21:48:37 - 0:00:00 - ============ Monolingual data (Ghomala)\n",
      "INFO - 05/29/20 21:48:37 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Ghomala.pth ...\n",
      "INFO - 05/29/20 21:48:37 - 0:00:00 - 147105 words (5057 unique) in 2889 sentences. 22977 unknown words (361 unique) covering 15.62% of the data.\n",
      "INFO - 05/29/20 21:48:37 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/29/20 21:48:37 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Ghomala.pth ...\n",
      "INFO - 05/29/20 21:48:37 - 0:00:00 - 147887 words (5057 unique) in 2889 sentences. 23332 unknown words (354 unique) covering 15.78% of the data.\n",
      "INFO - 05/29/20 21:48:37 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/29/20 21:48:37 - 0:00:00 - ============ Monolingual data (Limbum)\n",
      "INFO - 05/29/20 21:48:37 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Limbum.pth ...\n",
      "INFO - 05/29/20 21:48:37 - 0:00:00 - 35442 words (5057 unique) in 794 sentences. 6333 unknown words (235 unique) covering 17.87% of the data.\n",
      "INFO - 05/29/20 21:48:37 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/29/20 21:48:37 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Limbum.pth ...\n",
      "INFO - 05/29/20 21:48:37 - 0:00:00 - 34684 words (5057 unique) in 794 sentences. 6189 unknown words (251 unique) covering 17.84% of the data.\n",
      "INFO - 05/29/20 21:48:37 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/29/20 21:48:37 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/29/20 21:48:37 - 0:00:00 - Monolingual data   - valid -      Ghomala:      2889\n",
      "INFO - 05/29/20 21:48:37 - 0:00:00 - Monolingual data   -  test -      Ghomala:      2889\n",
      "INFO - 05/29/20 21:48:37 - 0:00:00 - Monolingual data   - valid -       Limbum:       794\n",
      "INFO - 05/29/20 21:48:37 - 0:00:00 - Monolingual data   -  test -       Limbum:       794\n",
      "\n",
      "INFO - 05/29/20 21:48:38 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(5057, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=5057, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/29/20 21:48:38 - 0:00:01 - Number of parameters (model): 81289153\n",
      "INFO - 05/29/20 21:48:41 - 0:00:04 - Found 0 memories.\n",
      "INFO - 05/29/20 21:48:41 - 0:00:04 - Found 6 FFN.\n",
      "INFO - 05/29/20 21:48:41 - 0:00:04 - Found 102 parameters in model.\n",
      "INFO - 05/29/20 21:48:41 - 0:00:04 - Optimizers: model\n",
      "WARNING - 05/29/20 21:48:41 - 0:00:04 - Reloading checkpoint from /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml/checkpoint.pth ...\n",
      "WARNING - 05/29/20 21:48:42 - 0:00:05 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/29/20 21:48:42 - 0:00:05 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/29/20 21:48:42 - 0:00:05 - Checkpoint reloaded. Resuming at epoch 52 / iteration 13728 ...\n",
      "INFO - 05/29/20 21:48:43 - 0:00:06 - epoch -> 52.000000\n",
      "INFO - 05/29/20 21:48:43 - 0:00:06 - valid_Ghomala_mlm_ppl -> 30955.217252\n",
      "INFO - 05/29/20 21:48:43 - 0:00:06 - valid_Ghomala_mlm_acc -> 5.699482\n",
      "INFO - 05/29/20 21:48:43 - 0:00:06 - valid_Limbum_mlm_ppl -> 20814.065733\n",
      "INFO - 05/29/20 21:48:43 - 0:00:06 - valid_Limbum_mlm_acc -> 9.326425\n",
      "INFO - 05/29/20 21:48:43 - 0:00:06 - valid_mlm_ppl -> 25884.641492\n",
      "INFO - 05/29/20 21:48:43 - 0:00:06 - valid_mlm_acc -> 7.512953\n",
      "INFO - 05/29/20 21:48:43 - 0:00:06 - test_Ghomala_mlm_ppl -> 38930.734776\n",
      "INFO - 05/29/20 21:48:43 - 0:00:06 - test_Ghomala_mlm_acc -> 3.367876\n",
      "INFO - 05/29/20 21:48:43 - 0:00:06 - test_Limbum_mlm_ppl -> 15835.891244\n",
      "INFO - 05/29/20 21:48:43 - 0:00:06 - test_Limbum_mlm_acc -> 9.844560\n",
      "INFO - 05/29/20 21:48:43 - 0:00:06 - test_mlm_ppl -> 27383.313010\n",
      "INFO - 05/29/20 21:48:43 - 0:00:06 - test_mlm_acc -> 6.606218\n",
      "INFO - 05/29/20 21:48:43 - 0:00:06 - __log__:{\"epoch\": 52, \"valid_Ghomala_mlm_ppl\": 30955.217251950326, \"valid_Ghomala_mlm_acc\": 5.699481865284974, \"valid_Limbum_mlm_ppl\": 20814.065732840772, \"valid_Limbum_mlm_acc\": 9.32642487046632, \"valid_mlm_ppl\": 25884.64149239555, \"valid_mlm_acc\": 7.512953367875648, \"test_Ghomala_mlm_ppl\": 38930.73477643034, \"test_Ghomala_mlm_acc\": 3.3678756476683938, \"test_Limbum_mlm_ppl\": 15835.89124408615, \"test_Limbum_mlm_acc\": 9.844559585492227, \"test_mlm_ppl\": 27383.313010258244, \"test_mlm_acc\": 6.60621761658031}\n",
      "Ghomala to Bulu\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "Limbum to MKPAMAN_AMVOE_Ewondo\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n"
     ]
    }
   ],
   "source": [
    "! ../copy_rename.sh $src_path $tgt_path Bulu-MKPAMAN_AMVOE_Ewondo $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : put the rename file on place\n",
    "! ../copy_rename.sh $src_path $tgt_path $tgt_pair Bulu-MKPAMAN_AMVOE_Ewondo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ghomala_Ngiemboon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: dump_path=/home/jupyter/models/africa/cluster3\n",
      "env: exp_name=mlm_tlm_GhomalaNgiemboon\n",
      "env: data_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon\n",
      "env: lgs=Ghomala-Ngiemboon\n",
      "env: mlm_steps=Ghomala,Ngiemboon\n",
      "env: tgt_pair=Ghomala-Ngiemboon\n",
      "env: src_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon\n",
      "env: tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon\n"
     ]
    }
   ],
   "source": [
    "%env dump_path=/home/jupyter/models/africa/cluster3\n",
    "%env exp_name=mlm_tlm_GhomalaNgiemboon\n",
    "%env data_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon\n",
    "%env lgs=Ghomala-Ngiemboon\n",
    "%env mlm_steps=Ghomala,Ngiemboon\n",
    "%env tgt_pair=Ghomala-Ngiemboon\n",
    "%env src_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon\n",
    "%env tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ghomala_Ngiemboon vs Limbum and Bafia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limbum to Ghomala\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "Bafia to Ngiemboon\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/29/20 21:53:33 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/29/20 21:53:33 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon' --lgs 'Ghomala-Ngiemboon' --clm_steps '' --mlm_steps 'Ghomala,Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_GhomalaNgiemboon\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Ghomala', 1: 'Ngiemboon'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Ghomala': 0, 'Ngiemboon': 1}\n",
      "                                     langs: ['Ghomala', 'Ngiemboon']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Ghomala-Ngiemboon']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Ghomala', None), ('Ngiemboon', None)]\n",
      "                                     mono_dataset: {'Ghomala': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/train.Ghomala.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ghomala.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ghomala.pth'}, 'Ngiemboon': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/train.Ngiemboon.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ngiemboon.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ngiemboon.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': True, 'valid': True, 'test': True}\n",
      "                                     remove_long_sentences_test: True\n",
      "                                     remove_long_sentences_train: True\n",
      "                                     remove_long_sentences_valid: True\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/29/20 21:53:33 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml\n",
      "                                     \n",
      "INFO - 05/29/20 21:53:33 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon' --lgs 'Ghomala-Ngiemboon' --clm_steps '' --mlm_steps 'Ghomala,Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/29/20 21:53:33 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/29/20 21:53:33 - 0:00:00 - ============ langs: Ghomala, Ngiemboon\n",
      "INFO - 05/29/20 21:53:33 - 0:00:00 - ============ Monolingual data (Ghomala)\n",
      "INFO - 05/29/20 21:53:33 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ghomala.pth ...\n",
      "INFO - 05/29/20 21:53:33 - 0:00:00 - 58011 words (6715 unique) in 792 sentences. 302 unknown words (26 unique) covering 0.52% of the data.\n",
      "INFO - 05/29/20 21:53:33 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/29/20 21:53:33 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ghomala.pth ...\n",
      "INFO - 05/29/20 21:53:33 - 0:00:00 - 57927 words (6715 unique) in 792 sentences. 300 unknown words (28 unique) covering 0.52% of the data.\n",
      "INFO - 05/29/20 21:53:33 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/29/20 21:53:33 - 0:00:00 - ============ Monolingual data (Ngiemboon)\n",
      "INFO - 05/29/20 21:53:33 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 21:53:33 - 0:00:00 - 62901 words (6715 unique) in 795 sentences. 8304 unknown words (41 unique) covering 13.20% of the data.\n",
      "INFO - 05/29/20 21:53:33 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/29/20 21:53:33 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 21:53:33 - 0:00:00 - 61045 words (6715 unique) in 795 sentences. 8156 unknown words (40 unique) covering 13.36% of the data.\n",
      "INFO - 05/29/20 21:53:33 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/29/20 21:53:33 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/29/20 21:53:33 - 0:00:00 - Monolingual data   - valid -      Ghomala:       792\n",
      "INFO - 05/29/20 21:53:33 - 0:00:00 - Monolingual data   -  test -      Ghomala:       792\n",
      "INFO - 05/29/20 21:53:33 - 0:00:00 - Monolingual data   - valid -    Ngiemboon:       795\n",
      "INFO - 05/29/20 21:53:33 - 0:00:00 - Monolingual data   -  test -    Ngiemboon:       795\n",
      "\n",
      "INFO - 05/29/20 21:53:34 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(6715, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=6715, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/29/20 21:53:34 - 0:00:01 - Number of parameters (model): 82988603\n",
      "INFO - 05/29/20 21:53:37 - 0:00:04 - Found 0 memories.\n",
      "INFO - 05/29/20 21:53:37 - 0:00:04 - Found 6 FFN.\n",
      "INFO - 05/29/20 21:53:37 - 0:00:04 - Found 102 parameters in model.\n",
      "INFO - 05/29/20 21:53:37 - 0:00:04 - Optimizers: model\n",
      "WARNING - 05/29/20 21:53:37 - 0:00:04 - Reloading checkpoint from /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml/checkpoint.pth ...\n",
      "WARNING - 05/29/20 21:53:48 - 0:00:15 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/29/20 21:53:48 - 0:00:15 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/29/20 21:53:48 - 0:00:15 - Checkpoint reloaded. Resuming at epoch 40 / iteration 10600 ...\n",
      "INFO - 05/29/20 21:53:49 - 0:00:16 - epoch -> 40.000000\n",
      "INFO - 05/29/20 21:53:49 - 0:00:16 - valid_Ghomala_mlm_ppl -> 4410.677766\n",
      "INFO - 05/29/20 21:53:49 - 0:00:16 - valid_Ghomala_mlm_acc -> 6.476684\n",
      "INFO - 05/29/20 21:53:49 - 0:00:16 - valid_Ngiemboon_mlm_ppl -> 29009.271961\n",
      "INFO - 05/29/20 21:53:49 - 0:00:16 - valid_Ngiemboon_mlm_acc -> 1.554404\n",
      "INFO - 05/29/20 21:53:49 - 0:00:16 - valid_mlm_ppl -> 16709.974864\n",
      "INFO - 05/29/20 21:53:49 - 0:00:16 - valid_mlm_acc -> 4.015544\n",
      "INFO - 05/29/20 21:53:49 - 0:00:16 - test_Ghomala_mlm_ppl -> 2245.677649\n",
      "INFO - 05/29/20 21:53:49 - 0:00:16 - test_Ghomala_mlm_acc -> 12.694301\n",
      "INFO - 05/29/20 21:53:49 - 0:00:16 - test_Ngiemboon_mlm_ppl -> 14611.916512\n",
      "INFO - 05/29/20 21:53:49 - 0:00:16 - test_Ngiemboon_mlm_acc -> 9.326425\n",
      "INFO - 05/29/20 21:53:49 - 0:00:16 - test_mlm_ppl -> 8428.797080\n",
      "INFO - 05/29/20 21:53:49 - 0:00:16 - test_mlm_acc -> 11.010363\n",
      "INFO - 05/29/20 21:53:49 - 0:00:16 - __log__:{\"epoch\": 40, \"valid_Ghomala_mlm_ppl\": 4410.67776621426, \"valid_Ghomala_mlm_acc\": 6.476683937823834, \"valid_Ngiemboon_mlm_ppl\": 29009.271961198316, \"valid_Ngiemboon_mlm_acc\": 1.5544041450777202, \"valid_mlm_ppl\": 16709.974863706288, \"valid_mlm_acc\": 4.015544041450777, \"test_Ghomala_mlm_ppl\": 2245.6776487794787, \"test_Ghomala_mlm_acc\": 12.694300518134716, \"test_Ngiemboon_mlm_ppl\": 14611.916511952673, \"test_Ngiemboon_mlm_acc\": 9.32642487046632, \"test_mlm_ppl\": 8428.797080366076, \"test_mlm_acc\": 11.010362694300518}\n",
      "Ghomala to Limbum\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "Ngiemboon to Bafia\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n"
     ]
    }
   ],
   "source": [
    "! ../copy_rename.sh $src_path $tgt_path Limbum-Bafia $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : put the rename file on place\n",
    "! ../copy_rename.sh $src_path $tgt_path $tgt_pair Limbum-Bafia "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ghomala_Ngiemboon vs Bulu and Ewondo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bulu to Ghomala\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "MKPAMAN_AMVOE_Ewondo to Ngiemboon\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/29/20 21:54:22 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/29/20 21:54:22 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon' --lgs 'Ghomala-Ngiemboon' --clm_steps '' --mlm_steps 'Ghomala,Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_GhomalaNgiemboon\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Ghomala', 1: 'Ngiemboon'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Ghomala': 0, 'Ngiemboon': 1}\n",
      "                                     langs: ['Ghomala', 'Ngiemboon']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Ghomala-Ngiemboon']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Ghomala', None), ('Ngiemboon', None)]\n",
      "                                     mono_dataset: {'Ghomala': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/train.Ghomala.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ghomala.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ghomala.pth'}, 'Ngiemboon': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/train.Ngiemboon.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ngiemboon.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ngiemboon.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': True, 'valid': True, 'test': True}\n",
      "                                     remove_long_sentences_test: True\n",
      "                                     remove_long_sentences_train: True\n",
      "                                     remove_long_sentences_valid: True\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/29/20 21:54:22 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml\n",
      "                                     \n",
      "INFO - 05/29/20 21:54:22 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon' --lgs 'Ghomala-Ngiemboon' --clm_steps '' --mlm_steps 'Ghomala,Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/29/20 21:54:22 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/29/20 21:54:22 - 0:00:00 - ============ langs: Ghomala, Ngiemboon\n",
      "INFO - 05/29/20 21:54:22 - 0:00:00 - ============ Monolingual data (Ghomala)\n",
      "INFO - 05/29/20 21:54:22 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ghomala.pth ...\n",
      "INFO - 05/29/20 21:54:22 - 0:00:00 - 167498 words (6715 unique) in 2889 sentences. 5743 unknown words (89 unique) covering 3.43% of the data.\n",
      "INFO - 05/29/20 21:54:22 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/29/20 21:54:22 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ghomala.pth ...\n",
      "INFO - 05/29/20 21:54:22 - 0:00:00 - 168469 words (6715 unique) in 2889 sentences. 5684 unknown words (89 unique) covering 3.37% of the data.\n",
      "INFO - 05/29/20 21:54:22 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/29/20 21:54:22 - 0:00:00 - ============ Monolingual data (Ngiemboon)\n",
      "INFO - 05/29/20 21:54:22 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 21:54:22 - 0:00:00 - 38743 words (6715 unique) in 794 sentences. 1156 unknown words (43 unique) covering 2.98% of the data.\n",
      "INFO - 05/29/20 21:54:22 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/29/20 21:54:22 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 21:54:22 - 0:00:00 - 38014 words (6715 unique) in 794 sentences. 1130 unknown words (43 unique) covering 2.97% of the data.\n",
      "INFO - 05/29/20 21:54:22 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/29/20 21:54:22 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/29/20 21:54:22 - 0:00:00 - Monolingual data   - valid -      Ghomala:      2889\n",
      "INFO - 05/29/20 21:54:22 - 0:00:00 - Monolingual data   -  test -      Ghomala:      2889\n",
      "INFO - 05/29/20 21:54:22 - 0:00:00 - Monolingual data   - valid -    Ngiemboon:       794\n",
      "INFO - 05/29/20 21:54:22 - 0:00:00 - Monolingual data   -  test -    Ngiemboon:       794\n",
      "\n",
      "INFO - 05/29/20 21:54:23 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(6715, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=6715, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/29/20 21:54:23 - 0:00:01 - Number of parameters (model): 82988603\n",
      "INFO - 05/29/20 21:54:27 - 0:00:04 - Found 0 memories.\n",
      "INFO - 05/29/20 21:54:27 - 0:00:04 - Found 6 FFN.\n",
      "INFO - 05/29/20 21:54:27 - 0:00:04 - Found 102 parameters in model.\n",
      "INFO - 05/29/20 21:54:27 - 0:00:04 - Optimizers: model\n",
      "WARNING - 05/29/20 21:54:27 - 0:00:04 - Reloading checkpoint from /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml/checkpoint.pth ...\n",
      "WARNING - 05/29/20 21:54:27 - 0:00:05 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/29/20 21:54:27 - 0:00:05 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/29/20 21:54:27 - 0:00:05 - Checkpoint reloaded. Resuming at epoch 40 / iteration 10600 ...\n",
      "INFO - 05/29/20 21:54:28 - 0:00:06 - epoch -> 40.000000\n",
      "INFO - 05/29/20 21:54:28 - 0:00:06 - valid_Ghomala_mlm_ppl -> 3391.627440\n",
      "INFO - 05/29/20 21:54:28 - 0:00:06 - valid_Ghomala_mlm_acc -> 9.067358\n",
      "INFO - 05/29/20 21:54:28 - 0:00:06 - valid_Ngiemboon_mlm_ppl -> 2950.096540\n",
      "INFO - 05/29/20 21:54:28 - 0:00:06 - valid_Ngiemboon_mlm_acc -> 9.067358\n",
      "INFO - 05/29/20 21:54:28 - 0:00:06 - valid_mlm_ppl -> 3170.861990\n",
      "INFO - 05/29/20 21:54:28 - 0:00:06 - valid_mlm_acc -> 9.067358\n",
      "INFO - 05/29/20 21:54:28 - 0:00:06 - test_Ghomala_mlm_ppl -> 2163.588904\n",
      "INFO - 05/29/20 21:54:28 - 0:00:06 - test_Ghomala_mlm_acc -> 8.808290\n",
      "INFO - 05/29/20 21:54:28 - 0:00:06 - test_Ngiemboon_mlm_ppl -> 3138.737452\n",
      "INFO - 05/29/20 21:54:28 - 0:00:06 - test_Ngiemboon_mlm_acc -> 8.031088\n",
      "INFO - 05/29/20 21:54:28 - 0:00:06 - test_mlm_ppl -> 2651.163178\n",
      "INFO - 05/29/20 21:54:28 - 0:00:06 - test_mlm_acc -> 8.419689\n",
      "INFO - 05/29/20 21:54:28 - 0:00:06 - __log__:{\"epoch\": 40, \"valid_Ghomala_mlm_ppl\": 3391.6274400309753, \"valid_Ghomala_mlm_acc\": 9.067357512953368, \"valid_Ngiemboon_mlm_ppl\": 2950.0965398563153, \"valid_Ngiemboon_mlm_acc\": 9.067357512953368, \"valid_mlm_ppl\": 3170.8619899436453, \"valid_mlm_acc\": 9.067357512953368, \"test_Ghomala_mlm_ppl\": 2163.5889039338617, \"test_Ghomala_mlm_acc\": 8.808290155440414, \"test_Ngiemboon_mlm_ppl\": 3138.73745205894, \"test_Ngiemboon_mlm_acc\": 8.031088082901555, \"test_mlm_ppl\": 2651.163177996401, \"test_mlm_acc\": 8.419689119170984}\n",
      "Ghomala to Bulu\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "Ngiemboon to MKPAMAN_AMVOE_Ewondo\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n"
     ]
    }
   ],
   "source": [
    "! ../copy_rename.sh $src_path $tgt_path Bulu-MKPAMAN_AMVOE_Ewondo $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : put the rename file on place\n",
    "! ../copy_rename.sh $src_path $tgt_path $tgt_pair Bulu-MKPAMAN_AMVOE_Ewondo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limbum_Ngiemboon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: dump_path=/home/jupyter/models/africa/cluster3\n",
      "env: exp_name=mlm_tlm_LimbumNgiemboon\n",
      "env: data_path=/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon\n",
      "env: lgs=Limbum-Ngiemboon\n",
      "env: mlm_steps=Limbum,Ngiemboon\n",
      "env: tgt_pair=Limbum-Ngiemboon\n",
      "env: src_path=/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon\n",
      "env: tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon\n"
     ]
    }
   ],
   "source": [
    "%env dump_path=/home/jupyter/models/africa/cluster3\n",
    "%env exp_name=mlm_tlm_LimbumNgiemboon\n",
    "%env data_path=/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon\n",
    "%env lgs=Limbum-Ngiemboon\n",
    "%env mlm_steps=Limbum,Ngiemboon\n",
    "%env tgt_pair=Limbum-Ngiemboon\n",
    "%env src_path=/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon\n",
    "%env tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Limbum_Ngiemboon vs Ghomala and Bafia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ghomala to Limbum\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "Bafia to Ngiemboon\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/29/20 21:58:59 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/29/20 21:58:59 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_LimbumNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon' --lgs 'Limbum-Ngiemboon' --clm_steps '' --mlm_steps 'Limbum,Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_LimbumNgiemboon\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Limbum', 1: 'Ngiemboon'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Limbum': 0, 'Ngiemboon': 1}\n",
      "                                     langs: ['Limbum', 'Ngiemboon']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Limbum-Ngiemboon']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Limbum', None), ('Ngiemboon', None)]\n",
      "                                     mono_dataset: {'Limbum': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/train.Limbum.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Limbum.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Limbum.pth'}, 'Ngiemboon': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/train.Ngiemboon.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Ngiemboon.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Ngiemboon.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': True, 'valid': True, 'test': True}\n",
      "                                     remove_long_sentences_test: True\n",
      "                                     remove_long_sentences_train: True\n",
      "                                     remove_long_sentences_valid: True\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/29/20 21:58:59 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml\n",
      "                                     \n",
      "INFO - 05/29/20 21:58:59 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_LimbumNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon' --lgs 'Limbum-Ngiemboon' --clm_steps '' --mlm_steps 'Limbum,Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/29/20 21:58:59 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/29/20 21:58:59 - 0:00:00 - ============ langs: Limbum, Ngiemboon\n",
      "INFO - 05/29/20 21:58:59 - 0:00:00 - ============ Monolingual data (Limbum)\n",
      "INFO - 05/29/20 21:58:59 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Limbum.pth ...\n",
      "INFO - 05/29/20 21:58:59 - 0:00:00 - 49588 words (6667 unique) in 794 sentences. 15675 unknown words (38 unique) covering 31.61% of the data.\n",
      "INFO - 05/29/20 21:58:59 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/29/20 21:58:59 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Limbum.pth ...\n",
      "INFO - 05/29/20 21:58:59 - 0:00:00 - 49306 words (6667 unique) in 794 sentences. 15602 unknown words (43 unique) covering 31.64% of the data.\n",
      "INFO - 05/29/20 21:58:59 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/29/20 21:58:59 - 0:00:00 - ============ Monolingual data (Ngiemboon)\n",
      "INFO - 05/29/20 21:58:59 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 21:58:59 - 0:00:00 - 63985 words (6667 unique) in 795 sentences. 21103 unknown words (31 unique) covering 32.98% of the data.\n",
      "INFO - 05/29/20 21:58:59 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/29/20 21:58:59 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 21:58:59 - 0:00:00 - 62241 words (6667 unique) in 795 sentences. 20704 unknown words (30 unique) covering 33.26% of the data.\n",
      "INFO - 05/29/20 21:58:59 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/29/20 21:58:59 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/29/20 21:58:59 - 0:00:00 - Monolingual data   - valid -       Limbum:       794\n",
      "INFO - 05/29/20 21:58:59 - 0:00:00 - Monolingual data   -  test -       Limbum:       794\n",
      "INFO - 05/29/20 21:58:59 - 0:00:00 - Monolingual data   - valid -    Ngiemboon:       795\n",
      "INFO - 05/29/20 21:58:59 - 0:00:00 - Monolingual data   -  test -    Ngiemboon:       795\n",
      "\n",
      "INFO - 05/29/20 21:59:00 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(6667, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=6667, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/29/20 21:59:00 - 0:00:01 - Number of parameters (model): 82939403\n",
      "INFO - 05/29/20 21:59:03 - 0:00:04 - Found 0 memories.\n",
      "INFO - 05/29/20 21:59:03 - 0:00:04 - Found 6 FFN.\n",
      "INFO - 05/29/20 21:59:03 - 0:00:04 - Found 102 parameters in model.\n",
      "INFO - 05/29/20 21:59:03 - 0:00:04 - Optimizers: model\n",
      "WARNING - 05/29/20 21:59:03 - 0:00:04 - Reloading checkpoint from /home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml/checkpoint.pth ...\n",
      "WARNING - 05/29/20 21:59:13 - 0:00:14 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/29/20 21:59:13 - 0:00:14 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/29/20 21:59:13 - 0:00:14 - Checkpoint reloaded. Resuming at epoch 60 / iteration 15840 ...\n",
      "INFO - 05/29/20 21:59:14 - 0:00:15 - epoch -> 60.000000\n",
      "INFO - 05/29/20 21:59:14 - 0:00:15 - valid_Limbum_mlm_ppl -> 51129.374071\n",
      "INFO - 05/29/20 21:59:14 - 0:00:15 - valid_Limbum_mlm_acc -> 3.108808\n",
      "INFO - 05/29/20 21:59:14 - 0:00:15 - valid_Ngiemboon_mlm_ppl -> 23034.547024\n",
      "INFO - 05/29/20 21:59:14 - 0:00:15 - valid_Ngiemboon_mlm_acc -> 11.139896\n",
      "INFO - 05/29/20 21:59:14 - 0:00:15 - valid_mlm_ppl -> 37081.960547\n",
      "INFO - 05/29/20 21:59:14 - 0:00:15 - valid_mlm_acc -> 7.124352\n",
      "INFO - 05/29/20 21:59:14 - 0:00:15 - test_Limbum_mlm_ppl -> 46943.496480\n",
      "INFO - 05/29/20 21:59:14 - 0:00:15 - test_Limbum_mlm_acc -> 2.072539\n",
      "INFO - 05/29/20 21:59:14 - 0:00:15 - test_Ngiemboon_mlm_ppl -> 33527.303238\n",
      "INFO - 05/29/20 21:59:14 - 0:00:15 - test_Ngiemboon_mlm_acc -> 9.585492\n",
      "INFO - 05/29/20 21:59:14 - 0:00:15 - test_mlm_ppl -> 40235.399859\n",
      "INFO - 05/29/20 21:59:14 - 0:00:15 - test_mlm_acc -> 5.829016\n",
      "INFO - 05/29/20 21:59:14 - 0:00:15 - __log__:{\"epoch\": 60, \"valid_Limbum_mlm_ppl\": 51129.37407050733, \"valid_Limbum_mlm_acc\": 3.1088082901554404, \"valid_Ngiemboon_mlm_ppl\": 23034.547024428324, \"valid_Ngiemboon_mlm_acc\": 11.139896373056995, \"valid_mlm_ppl\": 37081.960547467825, \"valid_mlm_acc\": 7.124352331606218, \"test_Limbum_mlm_ppl\": 46943.496479704074, \"test_Limbum_mlm_acc\": 2.0725388601036268, \"test_Ngiemboon_mlm_ppl\": 33527.303237715794, \"test_Ngiemboon_mlm_acc\": 9.585492227979275, \"test_mlm_ppl\": 40235.399858709934, \"test_mlm_acc\": 5.829015544041451}\n",
      "Limbum to Ghomala\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "Ngiemboon to Bafia\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n"
     ]
    }
   ],
   "source": [
    "! ../copy_rename.sh $src_path $tgt_path Ghomala-Bafia $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : put the rename file on place\n",
    "! ../copy_rename.sh $src_path $tgt_path $tgt_pair Ghomala-Bafia "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Limbum_Ngiemboon vs Bulu and Ewondo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bulu to Limbum\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "MKPAMAN_AMVOE_Ewondo to Ngiemboon\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/29/20 22:00:01 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/29/20 22:00:01 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_LimbumNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon' --lgs 'Limbum-Ngiemboon' --clm_steps '' --mlm_steps 'Limbum,Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_LimbumNgiemboon\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Limbum', 1: 'Ngiemboon'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Limbum': 0, 'Ngiemboon': 1}\n",
      "                                     langs: ['Limbum', 'Ngiemboon']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Limbum-Ngiemboon']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Limbum', None), ('Ngiemboon', None)]\n",
      "                                     mono_dataset: {'Limbum': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/train.Limbum.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Limbum.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Limbum.pth'}, 'Ngiemboon': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/train.Ngiemboon.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Ngiemboon.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Ngiemboon.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': True, 'valid': True, 'test': True}\n",
      "                                     remove_long_sentences_test: True\n",
      "                                     remove_long_sentences_train: True\n",
      "                                     remove_long_sentences_valid: True\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/29/20 22:00:01 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml\n",
      "                                     \n",
      "INFO - 05/29/20 22:00:01 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_LimbumNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon' --lgs 'Limbum-Ngiemboon' --clm_steps '' --mlm_steps 'Limbum,Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/29/20 22:00:01 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/29/20 22:00:01 - 0:00:00 - ============ langs: Limbum, Ngiemboon\n",
      "INFO - 05/29/20 22:00:01 - 0:00:00 - ============ Monolingual data (Limbum)\n",
      "INFO - 05/29/20 22:00:01 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Limbum.pth ...\n",
      "INFO - 05/29/20 22:00:01 - 0:00:00 - 153594 words (6667 unique) in 2889 sentences. 6139 unknown words (68 unique) covering 4.00% of the data.\n",
      "INFO - 05/29/20 22:00:01 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/29/20 22:00:01 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Limbum.pth ...\n",
      "INFO - 05/29/20 22:00:01 - 0:00:00 - 154736 words (6667 unique) in 2889 sentences. 5959 unknown words (72 unique) covering 3.85% of the data.\n",
      "INFO - 05/29/20 22:00:01 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/29/20 22:00:01 - 0:00:00 - ============ Monolingual data (Ngiemboon)\n",
      "INFO - 05/29/20 22:00:01 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 22:00:01 - 0:00:00 - 38704 words (6667 unique) in 794 sentences. 312 unknown words (29 unique) covering 0.81% of the data.\n",
      "INFO - 05/29/20 22:00:01 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/29/20 22:00:01 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Ngiemboon.pth ...\n",
      "INFO - 05/29/20 22:00:01 - 0:00:00 - 37876 words (6667 unique) in 794 sentences. 315 unknown words (31 unique) covering 0.83% of the data.\n",
      "INFO - 05/29/20 22:00:01 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/29/20 22:00:01 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/29/20 22:00:01 - 0:00:00 - Monolingual data   - valid -       Limbum:      2889\n",
      "INFO - 05/29/20 22:00:01 - 0:00:00 - Monolingual data   -  test -       Limbum:      2889\n",
      "INFO - 05/29/20 22:00:01 - 0:00:00 - Monolingual data   - valid -    Ngiemboon:       794\n",
      "INFO - 05/29/20 22:00:01 - 0:00:00 - Monolingual data   -  test -    Ngiemboon:       794\n",
      "\n",
      "INFO - 05/29/20 22:00:02 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(6667, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=6667, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/29/20 22:00:02 - 0:00:01 - Number of parameters (model): 82939403\n",
      "INFO - 05/29/20 22:00:06 - 0:00:04 - Found 0 memories.\n",
      "INFO - 05/29/20 22:00:06 - 0:00:04 - Found 6 FFN.\n",
      "INFO - 05/29/20 22:00:06 - 0:00:04 - Found 102 parameters in model.\n",
      "INFO - 05/29/20 22:00:06 - 0:00:04 - Optimizers: model\n",
      "WARNING - 05/29/20 22:00:06 - 0:00:04 - Reloading checkpoint from /home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml/checkpoint.pth ...\n",
      "WARNING - 05/29/20 22:00:06 - 0:00:05 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/29/20 22:00:06 - 0:00:05 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/29/20 22:00:06 - 0:00:05 - Checkpoint reloaded. Resuming at epoch 60 / iteration 15840 ...\n",
      "INFO - 05/29/20 22:00:07 - 0:00:06 - epoch -> 60.000000\n",
      "INFO - 05/29/20 22:00:07 - 0:00:06 - valid_Limbum_mlm_ppl -> 10392.934356\n",
      "INFO - 05/29/20 22:00:07 - 0:00:06 - valid_Limbum_mlm_acc -> 10.880829\n",
      "INFO - 05/29/20 22:00:07 - 0:00:06 - valid_Ngiemboon_mlm_ppl -> 3142.050697\n",
      "INFO - 05/29/20 22:00:07 - 0:00:06 - valid_Ngiemboon_mlm_acc -> 12.176166\n",
      "INFO - 05/29/20 22:00:07 - 0:00:06 - valid_mlm_ppl -> 6767.492526\n",
      "INFO - 05/29/20 22:00:07 - 0:00:06 - valid_mlm_acc -> 11.528497\n",
      "INFO - 05/29/20 22:00:07 - 0:00:06 - test_Limbum_mlm_ppl -> 16197.095080\n",
      "INFO - 05/29/20 22:00:07 - 0:00:06 - test_Limbum_mlm_acc -> 4.404145\n",
      "INFO - 05/29/20 22:00:07 - 0:00:06 - test_Ngiemboon_mlm_ppl -> 3494.447179\n",
      "INFO - 05/29/20 22:00:07 - 0:00:06 - test_Ngiemboon_mlm_acc -> 12.435233\n",
      "INFO - 05/29/20 22:00:07 - 0:00:06 - test_mlm_ppl -> 9845.771130\n",
      "INFO - 05/29/20 22:00:07 - 0:00:06 - test_mlm_acc -> 8.419689\n",
      "INFO - 05/29/20 22:00:07 - 0:00:06 - __log__:{\"epoch\": 60, \"valid_Limbum_mlm_ppl\": 10392.934356193988, \"valid_Limbum_mlm_acc\": 10.880829015544041, \"valid_Ngiemboon_mlm_ppl\": 3142.0506966500147, \"valid_Ngiemboon_mlm_acc\": 12.176165803108809, \"valid_mlm_ppl\": 6767.492526422001, \"valid_mlm_acc\": 11.528497409326425, \"test_Limbum_mlm_ppl\": 16197.095080228026, \"test_Limbum_mlm_acc\": 4.404145077720207, \"test_Ngiemboon_mlm_ppl\": 3494.4471790665284, \"test_Ngiemboon_mlm_acc\": 12.435233160621761, \"test_mlm_ppl\": 9845.771129647277, \"test_mlm_acc\": 8.419689119170984}\n",
      "Limbum to Bulu\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "Ngiemboon to MKPAMAN_AMVOE_Ewondo\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n",
      "../copy_rename.sh: line 23: [: missing `]'\n"
     ]
    }
   ],
   "source": [
    "! ../copy_rename.sh $src_path $tgt_path Bulu-MKPAMAN_AMVOE_Ewondo $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : put the rename file on place\n",
    "! ../copy_rename.sh $src_path $tgt_path $tgt_pair Bulu-MKPAMAN_AMVOE_Ewondo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
