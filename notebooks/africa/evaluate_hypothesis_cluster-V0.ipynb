{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(bookmark:HOME) -> /home/jupyter/meta_XLM/XLM\n",
      "/home/jupyter/meta_XLM/XLM\n"
     ]
    }
   ],
   "source": [
    "%bookmark HOME \"/home/jupyter/meta_XLM/XLM\" \n",
    "%cd -b HOME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: csv_path=/home/jupyter\n",
      "env: output_dir=/home/jupyter/data/evaluation_hypothesis\n",
      "env: data_type=mono\n"
     ]
    }
   ],
   "source": [
    "%env csv_path=/home/jupyter\n",
    "%env output_dir=/home/jupyter/data/evaluation_hypothesis\n",
    "%env data_type=mono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: languages=Bafia,Bafia\n",
      "Bafia\n",
      "======= Read 7950 totals samples\n",
      "======= Delete 0 samples\n",
      "======= Save 7950 samples\n",
      "env: languages=Bulu,Bulu\n",
      "Bulu\n",
      "======= Read 7950 totals samples\n",
      "======= Delete 4 samples\n",
      "======= Save 7946 samples\n",
      "env: languages=MKPAMAN_AMVOE_Ewondo,MKPAMAN_AMVOE_Ewondo\n",
      "MKPAMAN_AMVOE_Ewondo\n",
      "======= Read 7950 totals samples\n",
      "======= Delete 6 samples\n",
      "======= Save 7944 samples\n",
      "env: languages=Ghomala,Ghomala\n",
      "Ghomala\n",
      "======= Read 7950 totals samples\n",
      "======= Delete 8 samples\n",
      "======= Save 7942 samples\n",
      "env: languages=Limbum,Limbum\n",
      "Limbum\n",
      "======= Read 7950 totals samples\n",
      "======= Delete 31 samples\n",
      "======= Save 7919 samples\n",
      "env: languages=Ngiemboon,Ngiemboon\n",
      "Ngiemboon\n",
      "======= Read 7950 totals samples\n",
      "======= Delete 21 samples\n",
      "======= Save 7929 samples\n"
     ]
    }
   ],
   "source": [
    "# \"--new_only\" True To make sure our corpora's the same size.\n",
    "%env languages=Bafia,Bafia\n",
    "! python ../bible.py --csv_path $csv_path --output_dir $output_dir --data_type $data_type --languages $languages --new_only True\n",
    "\n",
    "%env languages=Bulu,Bulu\n",
    "! python ../bible.py --csv_path $csv_path --output_dir $output_dir --data_type $data_type --languages $languages --new_only True\n",
    "\n",
    "%env languages=MKPAMAN_AMVOE_Ewondo,MKPAMAN_AMVOE_Ewondo\n",
    "! python ../bible.py --csv_path $csv_path --output_dir $output_dir --data_type $data_type --languages $languages --new_only True\n",
    "\n",
    "%env languages=Ghomala,Ghomala\n",
    "! python ../bible.py --csv_path $csv_path --output_dir $output_dir --data_type $data_type --languages $languages --new_only True\n",
    "\n",
    "%env languages=Limbum,Limbum\n",
    "! python ../bible.py --csv_path $csv_path --output_dir $output_dir --data_type $data_type --languages $languages --new_only True\n",
    "\n",
    "%env languages=Ngiemboon,Ngiemboon\n",
    "! python ../bible.py --csv_path $csv_path --output_dir $output_dir --data_type $data_type --languages $languages --new_only True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select below each language pair trained for the purpose of MLM+TLM, and for this pair we use its code and vocabulary to process the other pairs in order to start the evaluation; and in this evaluation we rename the evaluated pair data files into the data files of the evaluating pairs.\n",
    "That's long enough!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: src_path=/home/jupyter/data/evaluation_hypothesis\n",
      "env: threads_for_tokenizer=16\n",
      "env: n_samples=-1\n",
      "env: test_size=50\n",
      "env: val_size=50\n",
      "env: TOKENIZE=tools/tokenizer_our.sh\n",
      "env: LOWER_REMOVE_ACCENT=tools/lowercase_and_remove_accent.py\n",
      "env: FASTBPE=tools/fastBPE/fast\n"
     ]
    }
   ],
   "source": [
    "%env src_path=/home/jupyter/data/evaluation_hypothesis\n",
    "%env threads_for_tokenizer=16 \n",
    "%env n_samples=-1\n",
    "# No need for training data\n",
    "%env test_size=50            \n",
    "%env val_size=50\n",
    "# tools paths\n",
    "%env TOKENIZE=tools/tokenizer_our.sh\n",
    "%env LOWER_REMOVE_ACCENT=tools/lowercase_and_remove_accent.py\n",
    "%env FASTBPE=tools/fastBPE/fast\n",
    "! chmod +x $FASTBPE\n",
    "! chmod +x tools/mosesdecoder/scripts/tokenizer/*.perl\n",
    "! chmod +x ../build_evaluate_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/jupyter/models/africa/evaluation_hypothesis’: File exists\n"
     ]
    }
   ],
   "source": [
    "! mkdir /home/jupyter/models/africa/evaluation_hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bafia_Bulu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu\n",
      "env: CODE_VOCAB_PATH=/home/jupyter/models/africa/cluster1/data/Bafia_Bulu/processed\n",
      "*** Cleaning and tokenizing MKPAMAN_AMVOE_Ewondo data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/MKPAMAN_AMVOE_Ewondo.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Bulu/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.MKPAMAN_AMVOE_Ewondo ...\n",
      "Read 92553 words (5633 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.MKPAMAN_AMVOE_Ewondo ...\n",
      "Modified 92553 words from text file.\n",
      "INFO - 05/30/20 14:54:42 - 0:00:00 - Read 8683 words from the vocabulary file.\n",
      "\n",
      "Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/30/20 14:54:42 - 0:00:00 - 139257 words (8683 unique) in 3972 sentences.\n",
      "INFO - 05/30/20 14:54:42 - 0:00:00 - 10272 unknown words (307 unique), covering 7.38% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Bulu/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.MKPAMAN_AMVOE_Ewondo ...\n",
      "Read 92913 words (5645 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.MKPAMAN_AMVOE_Ewondo ...\n",
      "Modified 92913 words from text file.\n",
      "INFO - 05/30/20 14:54:42 - 0:00:00 - Read 8683 words from the vocabulary file.\n",
      "\n",
      "Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/30/20 14:54:42 - 0:00:00 - 139672 words (8683 unique) in 3972 sentences.\n",
      "INFO - 05/30/20 14:54:42 - 0:00:00 - 10274 unknown words (310 unique), covering 7.36% of the data.\n",
      "*** Cleaning and tokenizing Ghomala data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Ghomala.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Bulu/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Ghomala ...\n",
      "Read 130199 words (2023 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Ghomala ...\n",
      "Modified 130199 words from text file.\n",
      "INFO - 05/30/20 14:54:43 - 0:00:00 - Read 8683 words from the vocabulary file.\n",
      "\n",
      "Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Ghomala.pth ...\n",
      "INFO - 05/30/20 14:54:43 - 0:00:00 - 237957 words (8683 unique) in 3971 sentences.\n",
      "INFO - 05/30/20 14:54:43 - 0:00:00 - 66767 unknown words (151 unique), covering 28.06% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Bulu/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Ghomala ...\n",
      "Read 128842 words (2006 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Ghomala ...\n",
      "Modified 128842 words from text file.\n",
      "INFO - 05/30/20 14:54:44 - 0:00:00 - Read 8683 words from the vocabulary file.\n",
      "\n",
      "Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Ghomala.pth ...\n",
      "INFO - 05/30/20 14:54:44 - 0:00:00 - 236355 words (8683 unique) in 3971 sentences.\n",
      "INFO - 05/30/20 14:54:44 - 0:00:00 - 66761 unknown words (144 unique), covering 28.25% of the data.\n",
      "*** Cleaning and tokenizing Limbum data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Limbum.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Bulu/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Limbum ...\n",
      "Read 161707 words (2300 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Limbum ...\n",
      "Modified 161707 words from text file.\n",
      "INFO - 05/30/20 14:54:45 - 0:00:00 - Read 8683 words from the vocabulary file.\n",
      "\n",
      "Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Limbum.pth ...\n",
      "INFO - 05/30/20 14:54:45 - 0:00:00 - 245577 words (8683 unique) in 3960 sentences.\n",
      "INFO - 05/30/20 14:54:45 - 0:00:00 - 14110 unknown words (148 unique), covering 5.75% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Bulu/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Limbum ...\n",
      "Read 159756 words (2332 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Limbum ...\n",
      "Modified 159756 words from text file.\n",
      "INFO - 05/30/20 14:54:45 - 0:00:00 - Read 8683 words from the vocabulary file.\n",
      "\n",
      "Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Limbum.pth ...\n",
      "INFO - 05/30/20 14:54:45 - 0:00:00 - 243331 words (8683 unique) in 3960 sentences.\n",
      "INFO - 05/30/20 14:54:45 - 0:00:00 - 14201 unknown words (153 unique), covering 5.84% of the data.\n",
      "*** Cleaning and tokenizing Ngiemboon data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Ngiemboon.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Bulu/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Ngiemboon ...\n",
      "Read 127319 words (8753 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Ngiemboon ...\n",
      "Modified 127319 words from text file.\n",
      "INFO - 05/30/20 14:54:46 - 0:00:00 - Read 8683 words from the vocabulary file.\n",
      "\n",
      "Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Ngiemboon.pth ...\n",
      "INFO - 05/30/20 14:54:46 - 0:00:00 - 401306 words (8683 unique) in 3965 sentences.\n",
      "INFO - 05/30/20 14:54:46 - 0:00:00 - 117914 unknown words (144 unique), covering 29.38% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Bulu/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Ngiemboon ...\n",
      "Read 129161 words (8801 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Ngiemboon ...\n",
      "Modified 129161 words from text file.\n",
      "INFO - 05/30/20 14:54:47 - 0:00:00 - Read 8683 words from the vocabulary file.\n",
      "\n",
      "Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Ngiemboon.pth ...\n",
      "INFO - 05/30/20 14:54:47 - 0:00:00 - 407079 words (8683 unique) in 3965 sentences.\n",
      "INFO - 05/30/20 14:54:47 - 0:00:00 - 118933 unknown words (138 unique), covering 29.22% of the data.\n"
     ]
    }
   ],
   "source": [
    "%env tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu\n",
    "%env CODE_VOCAB_PATH=/home/jupyter/models/africa/cluster1/data/Bafia_Bulu/processed\n",
    "! ../build_evaluate_data.sh MKPAMAN_AMVOE_Ewondo $n_samples\n",
    "! ../build_evaluate_data.sh Ghomala $n_samples\n",
    "! ../build_evaluate_data.sh Limbum $n_samples\n",
    "! ../build_evaluate_data.sh Ngiemboon $n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bafia_Ewondo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo\n",
      "env: CODE_VOCAB_PATH=/home/jupyter/models/africa/cluster1/data/Bafia_Ewondo/processed\n",
      "*** Cleaning and tokenizing Bulu data ... ***\n",
      "Tokenizer Version 1.1\n",
      "Language: Bulu\n",
      "Number of threads: 16\n",
      "WARNING: No known abbreviations for language 'Bulu', attempting fall-back to English version...\n",
      "*** Tokenized (+ lowercase + accent-removal) Bulu data to /home/jupyter/data/evaluation_hypothesis/Bulu.all ***\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Bulu ...\n",
      "Read 112522 words (4114 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Bulu ...\n",
      "Modified 112522 words from text file.\n",
      "INFO - 05/30/20 12:00:56 - 0:00:00 - Read 9128 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.Bulu.pth ...\n",
      "INFO - 05/30/20 12:00:56 - 0:00:00 - 160924 words (9128 unique) in 3973 sentences.\n",
      "INFO - 05/30/20 12:00:56 - 0:00:00 - 10022 unknown words (176 unique), covering 6.23% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Bulu ...\n",
      "Read 112697 words (4066 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Bulu ...\n",
      "Modified 112697 words from text file.\n",
      "INFO - 05/30/20 12:00:57 - 0:00:00 - Read 9128 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.Bulu.pth ...\n",
      "INFO - 05/30/20 12:00:57 - 0:00:00 - 161537 words (9128 unique) in 3973 sentences.\n",
      "INFO - 05/30/20 12:00:57 - 0:00:00 - 9851 unknown words (187 unique), covering 6.10% of the data.\n",
      "*** Cleaning and tokenizing Ghomala data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Ghomala.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Ghomala ...\n",
      "Read 130199 words (2023 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Ghomala ...\n",
      "Modified 130199 words from text file.\n",
      "INFO - 05/30/20 12:00:58 - 0:00:00 - Read 9128 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.Ghomala.pth ...\n",
      "INFO - 05/30/20 12:00:58 - 0:00:00 - 237746 words (9128 unique) in 3971 sentences.\n",
      "INFO - 05/30/20 12:00:58 - 0:00:00 - 57409 unknown words (107 unique), covering 24.15% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Ghomala ...\n",
      "Read 128842 words (2006 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Ghomala ...\n",
      "Modified 128842 words from text file.\n",
      "INFO - 05/30/20 12:00:59 - 0:00:00 - Read 9128 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.Ghomala.pth ...\n",
      "INFO - 05/30/20 12:00:59 - 0:00:00 - 236183 words (9128 unique) in 3971 sentences.\n",
      "INFO - 05/30/20 12:00:59 - 0:00:00 - 57363 unknown words (105 unique), covering 24.29% of the data.\n",
      "*** Cleaning and tokenizing Limbum data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Limbum.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Limbum ...\n",
      "Read 161707 words (2300 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Limbum ...\n",
      "Modified 161707 words from text file.\n",
      "INFO - 05/30/20 12:01:00 - 0:00:00 - Read 9128 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.Limbum.pth ...\n",
      "INFO - 05/30/20 12:01:00 - 0:00:00 - 246312 words (9128 unique) in 3960 sentences.\n",
      "INFO - 05/30/20 12:01:00 - 0:00:00 - 6171 unknown words (99 unique), covering 2.51% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Limbum ...\n",
      "Read 159756 words (2332 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Limbum ...\n",
      "Modified 159756 words from text file.\n",
      "INFO - 05/30/20 12:01:01 - 0:00:00 - Read 9128 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.Limbum.pth ...\n",
      "INFO - 05/30/20 12:01:01 - 0:00:00 - 243718 words (9128 unique) in 3960 sentences.\n",
      "INFO - 05/30/20 12:01:01 - 0:00:00 - 6390 unknown words (104 unique), covering 2.62% of the data.\n",
      "*** Cleaning and tokenizing Ngiemboon data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Ngiemboon.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Ngiemboon ...\n",
      "Read 127319 words (8753 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Ngiemboon ...\n",
      "Modified 127319 words from text file.\n",
      "INFO - 05/30/20 12:01:02 - 0:00:00 - Read 9128 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.Ngiemboon.pth ...\n",
      "INFO - 05/30/20 12:01:03 - 0:00:00 - 409388 words (9128 unique) in 3965 sentences.\n",
      "INFO - 05/30/20 12:01:03 - 0:00:00 - 106977 unknown words (106 unique), covering 26.13% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bafia_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Ngiemboon ...\n",
      "Read 129161 words (8801 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Ngiemboon ...\n",
      "Modified 129161 words from text file.\n",
      "INFO - 05/30/20 12:01:03 - 0:00:00 - Read 9128 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.Ngiemboon.pth ...\n",
      "INFO - 05/30/20 12:01:04 - 0:00:00 - 414994 words (9128 unique) in 3965 sentences.\n",
      "INFO - 05/30/20 12:01:04 - 0:00:00 - 108121 unknown words (109 unique), covering 26.05% of the data.\n"
     ]
    }
   ],
   "source": [
    "%env tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo\n",
    "%env CODE_VOCAB_PATH=/home/jupyter/models/africa/cluster1/data/Bafia_Ewondo/processed\n",
    "! ../build_evaluate_data.sh Bulu $n_samples\n",
    "! ../build_evaluate_data.sh Ghomala $n_samples\n",
    "! ../build_evaluate_data.sh Limbum $n_samples\n",
    "! ../build_evaluate_data.sh Ngiemboon $n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bulu_Ewondo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo\n",
      "env: CODE_VOCAB_PATH=/home/jupyter/models/africa/cluster1/data/Bulu_MKPAMAN_AMVOE_Ewondo/processed\n",
      "*** Cleaning and tokenizing Bafia data ... ***\n",
      "Tokenizer Version 1.1\n",
      "Language: Bafia\n",
      "Number of threads: 16\n",
      "WARNING: No known abbreviations for language 'Bafia', attempting fall-back to English version...\n",
      "*** Tokenized (+ lowercase + accent-removal) Bafia data to /home/jupyter/data/evaluation_hypothesis/Bafia.all ***\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bulu_MKPAMAN_AMVOE_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Bafia ...\n",
      "Read 170752 words (3936 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Bafia ...\n",
      "Modified 170752 words from text file.\n",
      "INFO - 05/30/20 12:02:48 - 0:00:00 - Read 9061 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.Bafia.pth ...\n",
      "INFO - 05/30/20 12:02:48 - 0:00:00 - 309529 words (9061 unique) in 3975 sentences.\n",
      "INFO - 05/30/20 12:02:48 - 0:00:00 - 106352 unknown words (126 unique), covering 34.36% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bulu_MKPAMAN_AMVOE_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Bafia ...\n",
      "Read 171762 words (3961 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Bafia ...\n",
      "Modified 171762 words from text file.\n",
      "INFO - 05/30/20 12:02:49 - 0:00:00 - Read 9061 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.Bafia.pth ...\n",
      "INFO - 05/30/20 12:02:49 - 0:00:00 - 311653 words (9061 unique) in 3975 sentences.\n",
      "INFO - 05/30/20 12:02:49 - 0:00:00 - 108247 unknown words (118 unique), covering 34.73% of the data.\n",
      "*** Cleaning and tokenizing Ghomala data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Ghomala.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bulu_MKPAMAN_AMVOE_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Ghomala ...\n",
      "Read 130199 words (2023 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Ghomala ...\n",
      "Modified 130199 words from text file.\n",
      "INFO - 05/30/20 12:02:50 - 0:00:00 - Read 9061 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.Ghomala.pth ...\n",
      "INFO - 05/30/20 12:02:50 - 0:00:00 - 260864 words (9061 unique) in 3971 sentences.\n",
      "INFO - 05/30/20 12:02:50 - 0:00:00 - 84253 unknown words (105 unique), covering 32.30% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bulu_MKPAMAN_AMVOE_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Ghomala ...\n",
      "Read 128842 words (2006 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Ghomala ...\n",
      "Modified 128842 words from text file.\n",
      "INFO - 05/30/20 12:02:51 - 0:00:00 - Read 9061 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.Ghomala.pth ...\n",
      "INFO - 05/30/20 12:02:51 - 0:00:00 - 259104 words (9061 unique) in 3971 sentences.\n",
      "INFO - 05/30/20 12:02:51 - 0:00:00 - 83798 unknown words (115 unique), covering 32.34% of the data.\n",
      "*** Cleaning and tokenizing Limbum data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Limbum.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bulu_MKPAMAN_AMVOE_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Limbum ...\n",
      "Read 161707 words (2300 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Limbum ...\n",
      "Modified 161707 words from text file.\n",
      "INFO - 05/30/20 12:02:52 - 0:00:00 - Read 9061 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.Limbum.pth ...\n",
      "INFO - 05/30/20 12:02:52 - 0:00:00 - 268013 words (9061 unique) in 3960 sentences.\n",
      "INFO - 05/30/20 12:02:52 - 0:00:00 - 15273 unknown words (105 unique), covering 5.70% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bulu_MKPAMAN_AMVOE_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Limbum ...\n",
      "Read 159756 words (2332 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Limbum ...\n",
      "Modified 159756 words from text file.\n",
      "INFO - 05/30/20 12:02:53 - 0:00:00 - Read 9061 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.Limbum.pth ...\n",
      "INFO - 05/30/20 12:02:53 - 0:00:00 - 265072 words (9061 unique) in 3960 sentences.\n",
      "INFO - 05/30/20 12:02:53 - 0:00:00 - 14978 unknown words (108 unique), covering 5.65% of the data.\n",
      "*** Cleaning and tokenizing Ngiemboon data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Ngiemboon.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bulu_MKPAMAN_AMVOE_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Ngiemboon ...\n",
      "Read 127319 words (8753 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Ngiemboon ...\n",
      "Modified 127319 words from text file.\n",
      "INFO - 05/30/20 12:02:54 - 0:00:00 - Read 9061 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.Ngiemboon.pth ...\n",
      "INFO - 05/30/20 12:02:54 - 0:00:00 - 419575 words (9061 unique) in 3965 sentences.\n",
      "INFO - 05/30/20 12:02:54 - 0:00:00 - 130652 unknown words (131 unique), covering 31.14% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster1/data/Bulu_MKPAMAN_AMVOE_Ewondo/processed/codes ...\n",
      "Read 10000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Ngiemboon ...\n",
      "Read 129161 words (8801 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Ngiemboon ...\n",
      "Modified 129161 words from text file.\n",
      "INFO - 05/30/20 12:02:55 - 0:00:00 - Read 9061 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.Ngiemboon.pth ...\n",
      "INFO - 05/30/20 12:02:56 - 0:00:00 - 425658 words (9061 unique) in 3965 sentences.\n",
      "INFO - 05/30/20 12:02:56 - 0:00:00 - 132100 unknown words (134 unique), covering 31.03% of the data.\n"
     ]
    }
   ],
   "source": [
    "%env tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo\n",
    "%env CODE_VOCAB_PATH=/home/jupyter/models/africa/cluster1/data/Bulu_MKPAMAN_AMVOE_Ewondo/processed\n",
    "! ../build_evaluate_data.sh Bafia $n_samples\n",
    "! ../build_evaluate_data.sh Ghomala $n_samples\n",
    "! ../build_evaluate_data.sh Limbum $n_samples\n",
    "! ../build_evaluate_data.sh Ngiemboon $n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ghomala_Limbum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum\n",
      "env: CODE_VOCAB_PATH=/home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed\n",
      "*** Cleaning and tokenizing Bafia data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Bafia.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Bafia ...\n",
      "Read 170752 words (3936 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Bafia ...\n",
      "Modified 170752 words from text file.\n",
      "INFO - 05/30/20 12:03:07 - 0:00:00 - Read 5057 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Bafia.pth ...\n",
      "INFO - 05/30/20 12:03:07 - 0:00:00 - 275494 words (5057 unique) in 3975 sentences.\n",
      "INFO - 05/30/20 12:03:07 - 0:00:00 - 84987 unknown words (333 unique), covering 30.85% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Bafia ...\n",
      "Read 171762 words (3961 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Bafia ...\n",
      "Modified 171762 words from text file.\n",
      "INFO - 05/30/20 12:03:08 - 0:00:00 - Read 5057 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Bafia.pth ...\n",
      "INFO - 05/30/20 12:03:08 - 0:00:00 - 277454 words (5057 unique) in 3975 sentences.\n",
      "INFO - 05/30/20 12:03:08 - 0:00:00 - 86097 unknown words (322 unique), covering 31.03% of the data.\n",
      "*** Cleaning and tokenizing Bulu data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Bulu.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Bulu ...\n",
      "Read 112522 words (4114 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Bulu ...\n",
      "Modified 112522 words from text file.\n",
      "INFO - 05/30/20 12:03:09 - 0:00:00 - Read 5057 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Bulu.pth ...\n",
      "INFO - 05/30/20 12:03:09 - 0:00:00 - 189585 words (5057 unique) in 3973 sentences.\n",
      "INFO - 05/30/20 12:03:09 - 0:00:00 - 30222 unknown words (339 unique), covering 15.94% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Bulu ...\n",
      "Read 112697 words (4066 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Bulu ...\n",
      "Modified 112697 words from text file.\n",
      "INFO - 05/30/20 12:03:10 - 0:00:00 - Read 5057 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Bulu.pth ...\n",
      "INFO - 05/30/20 12:03:10 - 0:00:00 - 189586 words (5057 unique) in 3973 sentences.\n",
      "INFO - 05/30/20 12:03:10 - 0:00:00 - 30074 unknown words (328 unique), covering 15.86% of the data.\n",
      "*** Cleaning and tokenizing MKPAMAN_AMVOE_Ewondo data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/MKPAMAN_AMVOE_Ewondo.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.MKPAMAN_AMVOE_Ewondo ...\n",
      "Read 92553 words (5633 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.MKPAMAN_AMVOE_Ewondo ...\n",
      "Modified 92553 words from text file.\n",
      "INFO - 05/30/20 12:03:11 - 0:00:00 - Read 5057 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/30/20 12:03:11 - 0:00:00 - 175753 words (5057 unique) in 3972 sentences.\n",
      "INFO - 05/30/20 12:03:11 - 0:00:00 - 31273 unknown words (349 unique), covering 17.79% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.MKPAMAN_AMVOE_Ewondo ...\n",
      "Read 92913 words (5645 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.MKPAMAN_AMVOE_Ewondo ...\n",
      "Modified 92913 words from text file.\n",
      "INFO - 05/30/20 12:03:12 - 0:00:00 - Read 5057 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/30/20 12:03:12 - 0:00:00 - 175999 words (5057 unique) in 3972 sentences.\n",
      "INFO - 05/30/20 12:03:12 - 0:00:00 - 31429 unknown words (346 unique), covering 17.86% of the data.\n",
      "*** Cleaning and tokenizing Ngiemboon data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Ngiemboon.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Ngiemboon ...\n",
      "Read 127319 words (8753 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Ngiemboon ...\n",
      "Modified 127319 words from text file.\n",
      "INFO - 05/30/20 12:03:13 - 0:00:00 - Read 5057 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Ngiemboon.pth ...\n",
      "INFO - 05/30/20 12:03:14 - 0:00:00 - 400875 words (5057 unique) in 3965 sentences.\n",
      "INFO - 05/30/20 12:03:14 - 0:00:00 - 161875 unknown words (278 unique), covering 40.38% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Ngiemboon ...\n",
      "Read 129161 words (8801 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Ngiemboon ...\n",
      "Modified 129161 words from text file.\n",
      "INFO - 05/30/20 12:03:14 - 0:00:00 - Read 5057 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Ngiemboon.pth ...\n",
      "INFO - 05/30/20 12:03:15 - 0:00:00 - 406640 words (5057 unique) in 3965 sentences.\n",
      "INFO - 05/30/20 12:03:15 - 0:00:00 - 163784 unknown words (277 unique), covering 40.28% of the data.\n"
     ]
    }
   ],
   "source": [
    "%env tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum\n",
    "%env CODE_VOCAB_PATH=/home/jupyter/models/africa/cluster3/data/Ghomala_Limbum/processed\n",
    "! ../build_evaluate_data.sh Bafia $n_samples\n",
    "! ../build_evaluate_data.sh Bulu $n_samples\n",
    "! ../build_evaluate_data.sh MKPAMAN_AMVOE_Ewondo $n_samples\n",
    "! ../build_evaluate_data.sh Ngiemboon $n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ghomala_Ngiemboon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon\n",
      "env: CODE_VOCAB_PATH=/home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed\n",
      "*** Cleaning and tokenizing Bafia data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Bafia.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Bafia ...\n",
      "Read 170752 words (3936 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Bafia ...\n",
      "Modified 170752 words from text file.\n",
      "INFO - 05/30/20 12:03:30 - 0:00:00 - Read 6715 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Bafia.pth ...\n",
      "INFO - 05/30/20 12:03:30 - 0:00:00 - 305709 words (6715 unique) in 3975 sentences.\n",
      "INFO - 05/30/20 12:03:30 - 0:00:00 - 40441 unknown words (62 unique), covering 13.23% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Bafia ...\n",
      "Read 171762 words (3961 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Bafia ...\n",
      "Modified 171762 words from text file.\n",
      "INFO - 05/30/20 12:03:31 - 0:00:00 - Read 6715 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Bafia.pth ...\n",
      "INFO - 05/30/20 12:03:31 - 0:00:00 - 308014 words (6715 unique) in 3975 sentences.\n",
      "INFO - 05/30/20 12:03:31 - 0:00:00 - 41032 unknown words (63 unique), covering 13.32% of the data.\n",
      "*** Cleaning and tokenizing Bulu data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Bulu.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Bulu ...\n",
      "Read 112522 words (4114 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Bulu ...\n",
      "Modified 112522 words from text file.\n",
      "INFO - 05/30/20 12:03:32 - 0:00:00 - Read 6715 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Bulu.pth ...\n",
      "INFO - 05/30/20 12:03:32 - 0:00:00 - 215224 words (6715 unique) in 3973 sentences.\n",
      "INFO - 05/30/20 12:03:32 - 0:00:00 - 7894 unknown words (73 unique), covering 3.67% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Bulu ...\n",
      "Read 112697 words (4066 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Bulu ...\n",
      "Modified 112697 words from text file.\n",
      "INFO - 05/30/20 12:03:33 - 0:00:00 - Read 6715 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Bulu.pth ...\n",
      "INFO - 05/30/20 12:03:33 - 0:00:00 - 215027 words (6715 unique) in 3973 sentences.\n",
      "INFO - 05/30/20 12:03:33 - 0:00:00 - 7866 unknown words (77 unique), covering 3.66% of the data.\n",
      "*** Cleaning and tokenizing MKPAMAN_AMVOE_Ewondo data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/MKPAMAN_AMVOE_Ewondo.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.MKPAMAN_AMVOE_Ewondo ...\n",
      "Read 92553 words (5633 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.MKPAMAN_AMVOE_Ewondo ...\n",
      "Modified 92553 words from text file.\n",
      "INFO - 05/30/20 12:03:34 - 0:00:00 - Read 6715 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/30/20 12:03:34 - 0:00:00 - 192457 words (6715 unique) in 3972 sentences.\n",
      "INFO - 05/30/20 12:03:34 - 0:00:00 - 5763 unknown words (66 unique), covering 2.99% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.MKPAMAN_AMVOE_Ewondo ...\n",
      "Read 92913 words (5645 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.MKPAMAN_AMVOE_Ewondo ...\n",
      "Modified 92913 words from text file.\n",
      "INFO - 05/30/20 12:03:35 - 0:00:00 - Read 6715 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/30/20 12:03:35 - 0:00:00 - 192845 words (6715 unique) in 3972 sentences.\n",
      "INFO - 05/30/20 12:03:35 - 0:00:00 - 5780 unknown words (63 unique), covering 3.00% of the data.\n",
      "*** Cleaning and tokenizing Limbum data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Limbum.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Limbum ...\n",
      "Read 161707 words (2300 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Limbum ...\n",
      "Modified 161707 words from text file.\n",
      "INFO - 05/30/20 12:03:36 - 0:00:00 - Read 6715 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Limbum.pth ...\n",
      "INFO - 05/30/20 12:03:36 - 0:00:00 - 290647 words (6715 unique) in 3960 sentences.\n",
      "INFO - 05/30/20 12:03:36 - 0:00:00 - 1408 unknown words (39 unique), covering 0.48% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Limbum ...\n",
      "Read 159756 words (2332 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Limbum ...\n",
      "Modified 159756 words from text file.\n",
      "INFO - 05/30/20 12:03:37 - 0:00:00 - Read 6715 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Limbum.pth ...\n",
      "INFO - 05/30/20 12:03:37 - 0:00:00 - 287818 words (6715 unique) in 3960 sentences.\n",
      "INFO - 05/30/20 12:03:37 - 0:00:00 - 1440 unknown words (42 unique), covering 0.50% of the data.\n"
     ]
    }
   ],
   "source": [
    "%env tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon\n",
    "%env CODE_VOCAB_PATH=/home/jupyter/models/africa/cluster3/data/Ghomala_Ngiemboon/processed\n",
    "! ../build_evaluate_data.sh Bafia $n_samples\n",
    "! ../build_evaluate_data.sh Bulu $n_samples\n",
    "! ../build_evaluate_data.sh MKPAMAN_AMVOE_Ewondo $n_samples\n",
    "! ../build_evaluate_data.sh Limbum $n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limbum_Ngiemboon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon\n",
      "env: CODE_VOCAB_PATH=/home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed\n",
      "*** Cleaning and tokenizing Bafia data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Bafia.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Bafia ...\n",
      "Read 170752 words (3936 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Bafia ...\n",
      "Modified 170752 words from text file.\n",
      "INFO - 05/30/20 12:04:38 - 0:00:00 - Read 6667 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Bafia.pth ...\n",
      "INFO - 05/30/20 12:04:38 - 0:00:00 - 311407 words (6667 unique) in 3975 sentences.\n",
      "INFO - 05/30/20 12:04:38 - 0:00:00 - 102253 unknown words (48 unique), covering 32.84% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Bafia ...\n",
      "Read 171762 words (3961 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Bafia ...\n",
      "Modified 171762 words from text file.\n",
      "INFO - 05/30/20 12:04:39 - 0:00:00 - Read 6667 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Bafia.pth ...\n",
      "INFO - 05/30/20 12:04:39 - 0:00:00 - 313817 words (6667 unique) in 3975 sentences.\n",
      "INFO - 05/30/20 12:04:39 - 0:00:00 - 104021 unknown words (41 unique), covering 33.15% of the data.\n",
      "*** Cleaning and tokenizing Bulu data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Bulu.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Bulu ...\n",
      "Read 112522 words (4114 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Bulu ...\n",
      "Modified 112522 words from text file.\n",
      "INFO - 05/30/20 12:04:40 - 0:00:00 - Read 6667 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Bulu.pth ...\n",
      "INFO - 05/30/20 12:04:40 - 0:00:00 - 200114 words (6667 unique) in 3973 sentences.\n",
      "INFO - 05/30/20 12:04:40 - 0:00:00 - 7647 unknown words (52 unique), covering 3.82% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Bulu ...\n",
      "Read 112697 words (4066 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Bulu ...\n",
      "Modified 112697 words from text file.\n",
      "INFO - 05/30/20 12:04:41 - 0:00:00 - Read 6667 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Bulu.pth ...\n",
      "INFO - 05/30/20 12:04:41 - 0:00:00 - 199901 words (6667 unique) in 3973 sentences.\n",
      "INFO - 05/30/20 12:04:41 - 0:00:00 - 7721 unknown words (58 unique), covering 3.86% of the data.\n",
      "*** Cleaning and tokenizing MKPAMAN_AMVOE_Ewondo data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/MKPAMAN_AMVOE_Ewondo.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.MKPAMAN_AMVOE_Ewondo ...\n",
      "Read 92553 words (5633 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.MKPAMAN_AMVOE_Ewondo ...\n",
      "Modified 92553 words from text file.\n",
      "INFO - 05/30/20 12:04:42 - 0:00:00 - Read 6667 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/30/20 12:04:42 - 0:00:00 - 191892 words (6667 unique) in 3972 sentences.\n",
      "INFO - 05/30/20 12:04:42 - 0:00:00 - 1732 unknown words (49 unique), covering 0.90% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.MKPAMAN_AMVOE_Ewondo ...\n",
      "Read 92913 words (5645 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.MKPAMAN_AMVOE_Ewondo ...\n",
      "Modified 92913 words from text file.\n",
      "INFO - 05/30/20 12:04:43 - 0:00:00 - Read 6667 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/30/20 12:04:43 - 0:00:00 - 192496 words (6667 unique) in 3972 sentences.\n",
      "INFO - 05/30/20 12:04:43 - 0:00:00 - 1642 unknown words (46 unique), covering 0.85% of the data.\n",
      "*** Cleaning and tokenizing Ghomala data ... ***\n",
      "file /home/jupyter/data/evaluation_hypothesis/Ghomala.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora and binarize everything using preprocess.py.***\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/valid.Ghomala ...\n",
      "Read 130199 words (2023 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/valid.Ghomala ...\n",
      "Modified 130199 words from text file.\n",
      "INFO - 05/30/20 12:04:44 - 0:00:00 - Read 6667 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Ghomala.pth ...\n",
      "INFO - 05/30/20 12:04:44 - 0:00:00 - 251161 words (6667 unique) in 3971 sentences.\n",
      "INFO - 05/30/20 12:04:44 - 0:00:00 - 79982 unknown words (52 unique), covering 31.84% of the data.\n",
      "Loading codes from /home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed/codes ...\n",
      "Read 7000 codes from the codes file.\n",
      "Loading vocabulary from /home/jupyter/data/evaluation_hypothesis/test.Ghomala ...\n",
      "Read 128842 words (2006 unique) from text file.\n",
      "Applying BPE to /home/jupyter/data/evaluation_hypothesis/test.Ghomala ...\n",
      "Modified 128842 words from text file.\n",
      "INFO - 05/30/20 12:04:45 - 0:00:00 - Read 6667 words from the vocabulary file.\n",
      "\n",
      "Saving the data to /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Ghomala.pth ...\n",
      "INFO - 05/30/20 12:04:45 - 0:00:00 - 249379 words (6667 unique) in 3971 sentences.\n",
      "INFO - 05/30/20 12:04:45 - 0:00:00 - 79534 unknown words (54 unique), covering 31.89% of the data.\n"
     ]
    }
   ],
   "source": [
    "%env tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon\n",
    "%env CODE_VOCAB_PATH=/home/jupyter/models/africa/cluster3/data/Limbum_Ngiemboon/processed\n",
    "! ../build_evaluate_data.sh Bafia $n_samples\n",
    "! ../build_evaluate_data.sh Bulu $n_samples\n",
    "! ../build_evaluate_data.sh MKPAMAN_AMVOE_Ewondo $n_samples\n",
    "! ../build_evaluate_data.sh Ghomala $n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: exp_id=maml\n",
      "env: batch_size=32\n",
      "env: max_epoch=100\n",
      "env: stopping_criterion=_valid_mlm_ppl,10\n",
      "env: eval_bleu=False\n",
      "env: remove_long_sentences_train=False\n",
      "env: remove_long_sentences_valid=False\n",
      "env: remove_long_sentences_test=False\n",
      "env: train_n_samples=-1\n",
      "env: valid_n_samples=-1\n",
      "env: test_n_samples=-1\n",
      "env: epoch_size=5000\n"
     ]
    }
   ],
   "source": [
    "%env exp_id=maml\n",
    "%env batch_size=32\n",
    "%env max_epoch=100\n",
    "%env stopping_criterion=_valid_mlm_ppl,10\n",
    "%env eval_bleu=False\n",
    "%env remove_long_sentences_train=False\n",
    "%env remove_long_sentences_valid=False\n",
    "%env remove_long_sentences_test=False\n",
    "%env train_n_samples=-1\n",
    "%env valid_n_samples=-1\n",
    "%env test_n_samples=-1\n",
    "%env epoch_size=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod +x ../copy_rename.sh\n",
    "! chmod +x ../evaluate.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bafia_Bulu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: dump_path=/home/jupyter/models/africa/cluster1\n",
      "env: exp_name=mlm_tlm_BafiaBulu\n",
      "env: data_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu\n",
      "env: lgs=Bafia-Bulu\n",
      "env: mlm_steps=Bafia,Bulu\n",
      "env: tgt_pair=Bafia-Bulu\n",
      "env: src_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu\n",
      "env: tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu\n"
     ]
    }
   ],
   "source": [
    "%env dump_path=/home/jupyter/models/africa/cluster1\n",
    "%env exp_name=mlm_tlm_BafiaBulu\n",
    "%env data_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu\n",
    "%env lgs=Bafia-Bulu\n",
    "%env mlm_steps=Bafia,Bulu\n",
    "%env tgt_pair=Bafia-Bulu\n",
    "%env src_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu\n",
    "%env tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu\n",
    "\n",
    "# creation of the dummy files so that the experiment does not bug\n",
    "! touch /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/train.Bafia.pth\n",
    "! touch /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/train.Bulu.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Bafia_Bulu vs Ewondo and Ghomala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "MKPAMAN_AMVOE_Ewondo to Bafia\n",
      "Ghomala to Bulu\n",
      "=====================\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/30/20 14:55:19 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/30/20 14:55:19 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_BafiaBulu --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu' --lgs 'Bafia-Bulu' --clm_steps '' --mlm_steps 'Bafia,Bulu' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaBulu/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_BafiaBulu\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Bafia', 1: 'Bulu'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Bafia': 0, 'Bulu': 1}\n",
      "                                     langs: ['Bafia', 'Bulu']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Bafia-Bulu']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Bafia', None), ('Bulu', None)]\n",
      "                                     mono_dataset: {'Bafia': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/train.Bafia.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bafia.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bafia.pth'}, 'Bulu': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/train.Bulu.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bulu.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bulu.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': False, 'valid': False, 'test': False}\n",
      "                                     remove_long_sentences_test: False\n",
      "                                     remove_long_sentences_train: False\n",
      "                                     remove_long_sentences_valid: False\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/30/20 14:55:19 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaBulu/maml\n",
      "                                     \n",
      "INFO - 05/30/20 14:55:19 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_BafiaBulu --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu' --lgs 'Bafia-Bulu' --clm_steps '' --mlm_steps 'Bafia,Bulu' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/30/20 14:55:19 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/30/20 14:55:19 - 0:00:00 - ============ langs: Bafia, Bulu\n",
      "INFO - 05/30/20 14:55:19 - 0:00:00 - ============ Monolingual data (Bafia)\n",
      "INFO - 05/30/20 14:55:19 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bafia.pth ...\n",
      "INFO - 05/30/20 14:55:19 - 0:00:00 - 139257 words (8683 unique) in 3972 sentences. 10272 unknown words (307 unique) covering 7.38% of the data.\n",
      "INFO - 05/30/20 14:55:19 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/30/20 14:55:19 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bafia.pth ...\n",
      "INFO - 05/30/20 14:55:19 - 0:00:00 - 139672 words (8683 unique) in 3972 sentences. 10274 unknown words (310 unique) covering 7.36% of the data.\n",
      "INFO - 05/30/20 14:55:19 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/30/20 14:55:19 - 0:00:00 - ============ Monolingual data (Bulu)\n",
      "INFO - 05/30/20 14:55:19 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bulu.pth ...\n",
      "INFO - 05/30/20 14:55:19 - 0:00:00 - 237957 words (8683 unique) in 3971 sentences. 66767 unknown words (151 unique) covering 28.06% of the data.\n",
      "INFO - 05/30/20 14:55:19 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/30/20 14:55:19 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bulu.pth ...\n",
      "INFO - 05/30/20 14:55:19 - 0:00:00 - 236355 words (8683 unique) in 3971 sentences. 66761 unknown words (144 unique) covering 28.25% of the data.\n",
      "INFO - 05/30/20 14:55:19 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/30/20 14:55:19 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/30/20 14:55:19 - 0:00:00 - Monolingual data   - valid -        Bafia:      3972\n",
      "INFO - 05/30/20 14:55:19 - 0:00:00 - Monolingual data   -  test -        Bafia:      3972\n",
      "INFO - 05/30/20 14:55:19 - 0:00:00 - Monolingual data   - valid -         Bulu:      3971\n",
      "INFO - 05/30/20 14:55:19 - 0:00:00 - Monolingual data   -  test -         Bulu:      3971\n",
      "\n",
      "INFO - 05/30/20 14:55:19 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(8683, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=8683, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/30/20 14:55:19 - 0:00:01 - Number of parameters (model): 85005803\n",
      "INFO - 05/30/20 14:55:26 - 0:00:07 - Found 0 memories.\n",
      "INFO - 05/30/20 14:55:26 - 0:00:07 - Found 6 FFN.\n",
      "INFO - 05/30/20 14:55:26 - 0:00:07 - Found 102 parameters in model.\n",
      "INFO - 05/30/20 14:55:26 - 0:00:07 - Optimizers: model\n",
      "WARNING - 05/30/20 14:55:26 - 0:00:07 - Reloading checkpoint from /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaBulu/maml/checkpoint.pth ...\n",
      "WARNING - 05/30/20 14:55:26 - 0:00:08 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/30/20 14:55:26 - 0:00:08 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/30/20 14:55:26 - 0:00:08 - Checkpoint reloaded. Resuming at epoch 68 / iteration 4556 ...\n",
      "INFO - 05/30/20 14:55:29 - 0:00:10 - epoch -> 68.000000\n",
      "INFO - 05/30/20 14:55:29 - 0:00:10 - valid_Bafia_mlm_ppl -> 21177.556427\n",
      "INFO - 05/30/20 14:55:29 - 0:00:10 - valid_Bafia_mlm_acc -> 9.585492\n",
      "INFO - 05/30/20 14:55:29 - 0:00:10 - valid_Bulu_mlm_ppl -> 8979.833757\n",
      "INFO - 05/30/20 14:55:29 - 0:00:10 - valid_Bulu_mlm_acc -> 13.212435\n",
      "INFO - 05/30/20 14:55:29 - 0:00:10 - valid_mlm_ppl -> 15078.695092\n",
      "INFO - 05/30/20 14:55:29 - 0:00:10 - valid_mlm_acc -> 11.398964\n",
      "INFO - 05/30/20 14:55:29 - 0:00:10 - test_Bafia_mlm_ppl -> 15672.662427\n",
      "INFO - 05/30/20 14:55:29 - 0:00:10 - test_Bafia_mlm_acc -> 12.694301\n",
      "INFO - 05/30/20 14:55:29 - 0:00:10 - test_Bulu_mlm_ppl -> 35251.883376\n",
      "INFO - 05/30/20 14:55:29 - 0:00:10 - test_Bulu_mlm_acc -> 4.663212\n",
      "INFO - 05/30/20 14:55:29 - 0:00:10 - test_mlm_ppl -> 25462.272902\n",
      "INFO - 05/30/20 14:55:29 - 0:00:10 - test_mlm_acc -> 8.678756\n",
      "INFO - 05/30/20 14:55:29 - 0:00:10 - __log__:{\"epoch\": 68, \"valid_Bafia_mlm_ppl\": 21177.556426980704, \"valid_Bafia_mlm_acc\": 9.585492227979275, \"valid_Bulu_mlm_ppl\": 8979.83375663103, \"valid_Bulu_mlm_acc\": 13.212435233160623, \"valid_mlm_ppl\": 15078.695091805866, \"valid_mlm_acc\": 11.39896373056995, \"test_Bafia_mlm_ppl\": 15672.66242720843, \"test_Bafia_mlm_acc\": 12.694300518134716, \"test_Bulu_mlm_ppl\": 35251.88337595063, \"test_Bulu_mlm_acc\": 4.66321243523316, \"test_mlm_ppl\": 25462.27290157953, \"test_mlm_acc\": 8.678756476683938}\n",
      "=====================\n",
      "Bafia to MKPAMAN_AMVOE_Ewondo\n",
      "Bulu to Ghomala\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "! ../copy_rename.sh $src_path $tgt_path MKPAMAN_AMVOE_Ewondo-Ghomala $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : put the rename file on place\n",
    "! ../copy_rename.sh $src_path $tgt_path $tgt_pair MKPAMAN_AMVOE_Ewondo-Ghomala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Bafia_Bulu vs Limbum and Ngiemboon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Limbum to Bafia\n",
      "Ngiemboon to Bulu\n",
      "=====================\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/30/20 14:55:37 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/30/20 14:55:37 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_BafiaBulu --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu' --lgs 'Bafia-Bulu' --clm_steps '' --mlm_steps 'Bafia,Bulu' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaBulu/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_BafiaBulu\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Bafia', 1: 'Bulu'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Bafia': 0, 'Bulu': 1}\n",
      "                                     langs: ['Bafia', 'Bulu']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Bafia-Bulu']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Bafia', None), ('Bulu', None)]\n",
      "                                     mono_dataset: {'Bafia': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/train.Bafia.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bafia.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bafia.pth'}, 'Bulu': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/train.Bulu.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bulu.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bulu.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': False, 'valid': False, 'test': False}\n",
      "                                     remove_long_sentences_test: False\n",
      "                                     remove_long_sentences_train: False\n",
      "                                     remove_long_sentences_valid: False\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/30/20 14:55:37 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaBulu/maml\n",
      "                                     \n",
      "INFO - 05/30/20 14:55:37 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_BafiaBulu --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu' --lgs 'Bafia-Bulu' --clm_steps '' --mlm_steps 'Bafia,Bulu' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/30/20 14:55:37 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/30/20 14:55:37 - 0:00:00 - ============ langs: Bafia, Bulu\n",
      "INFO - 05/30/20 14:55:37 - 0:00:00 - ============ Monolingual data (Bafia)\n",
      "INFO - 05/30/20 14:55:37 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bafia.pth ...\n",
      "INFO - 05/30/20 14:55:37 - 0:00:00 - 245577 words (8683 unique) in 3960 sentences. 14110 unknown words (148 unique) covering 5.75% of the data.\n",
      "INFO - 05/30/20 14:55:37 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/30/20 14:55:37 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bafia.pth ...\n",
      "INFO - 05/30/20 14:55:37 - 0:00:00 - 243331 words (8683 unique) in 3960 sentences. 14201 unknown words (153 unique) covering 5.84% of the data.\n",
      "INFO - 05/30/20 14:55:37 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/30/20 14:55:37 - 0:00:00 - ============ Monolingual data (Bulu)\n",
      "INFO - 05/30/20 14:55:37 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/valid.Bulu.pth ...\n",
      "INFO - 05/30/20 14:55:37 - 0:00:00 - 401306 words (8683 unique) in 3965 sentences. 117914 unknown words (144 unique) covering 29.38% of the data.\n",
      "INFO - 05/30/20 14:55:37 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/30/20 14:55:37 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Bulu/test.Bulu.pth ...\n",
      "INFO - 05/30/20 14:55:37 - 0:00:00 - 407079 words (8683 unique) in 3965 sentences. 118933 unknown words (138 unique) covering 29.22% of the data.\n",
      "INFO - 05/30/20 14:55:37 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/30/20 14:55:37 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/30/20 14:55:37 - 0:00:00 - Monolingual data   - valid -        Bafia:      3960\n",
      "INFO - 05/30/20 14:55:37 - 0:00:00 - Monolingual data   -  test -        Bafia:      3960\n",
      "INFO - 05/30/20 14:55:37 - 0:00:00 - Monolingual data   - valid -         Bulu:      3965\n",
      "INFO - 05/30/20 14:55:37 - 0:00:00 - Monolingual data   -  test -         Bulu:      3965\n",
      "\n",
      "INFO - 05/30/20 14:55:37 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(8683, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=8683, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/30/20 14:55:37 - 0:00:01 - Number of parameters (model): 85005803\n",
      "INFO - 05/30/20 14:55:44 - 0:00:07 - Found 0 memories.\n",
      "INFO - 05/30/20 14:55:44 - 0:00:07 - Found 6 FFN.\n",
      "INFO - 05/30/20 14:55:44 - 0:00:07 - Found 102 parameters in model.\n",
      "INFO - 05/30/20 14:55:44 - 0:00:07 - Optimizers: model\n",
      "WARNING - 05/30/20 14:55:44 - 0:00:07 - Reloading checkpoint from /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaBulu/maml/checkpoint.pth ...\n",
      "WARNING - 05/30/20 14:55:45 - 0:00:08 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/30/20 14:55:45 - 0:00:08 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/30/20 14:55:45 - 0:00:08 - Checkpoint reloaded. Resuming at epoch 68 / iteration 4556 ...\n",
      "INFO - 05/30/20 14:55:47 - 0:00:10 - epoch -> 68.000000\n",
      "INFO - 05/30/20 14:55:47 - 0:00:10 - valid_Bafia_mlm_ppl -> 26115.303447\n",
      "INFO - 05/30/20 14:55:47 - 0:00:10 - valid_Bafia_mlm_acc -> 6.217617\n",
      "INFO - 05/30/20 14:55:47 - 0:00:10 - valid_Bulu_mlm_ppl -> 18990.775180\n",
      "INFO - 05/30/20 14:55:47 - 0:00:10 - valid_Bulu_mlm_acc -> 10.880829\n",
      "INFO - 05/30/20 14:55:47 - 0:00:10 - valid_mlm_ppl -> 22553.039313\n",
      "INFO - 05/30/20 14:55:47 - 0:00:10 - valid_mlm_acc -> 8.549223\n",
      "INFO - 05/30/20 14:55:47 - 0:00:10 - test_Bafia_mlm_ppl -> 49951.915818\n",
      "INFO - 05/30/20 14:55:47 - 0:00:10 - test_Bafia_mlm_acc -> 3.626943\n",
      "INFO - 05/30/20 14:55:47 - 0:00:10 - test_Bulu_mlm_ppl -> 19686.628410\n",
      "INFO - 05/30/20 14:55:47 - 0:00:10 - test_Bulu_mlm_acc -> 5.958549\n",
      "INFO - 05/30/20 14:55:47 - 0:00:10 - test_mlm_ppl -> 34819.272114\n",
      "INFO - 05/30/20 14:55:47 - 0:00:10 - test_mlm_acc -> 4.792746\n",
      "INFO - 05/30/20 14:55:47 - 0:00:10 - __log__:{\"epoch\": 68, \"valid_Bafia_mlm_ppl\": 26115.303446753318, \"valid_Bafia_mlm_acc\": 6.217616580310881, \"valid_Bulu_mlm_ppl\": 18990.775180127912, \"valid_Bulu_mlm_acc\": 10.880829015544041, \"valid_mlm_ppl\": 22553.039313440615, \"valid_mlm_acc\": 8.549222797927461, \"test_Bafia_mlm_ppl\": 49951.915817957546, \"test_Bafia_mlm_acc\": 3.626943005181347, \"test_Bulu_mlm_ppl\": 19686.62841042008, \"test_Bulu_mlm_acc\": 5.958549222797927, \"test_mlm_ppl\": 34819.27211418881, \"test_mlm_acc\": 4.7927461139896375}\n",
      "=====================\n",
      "Bafia to Limbum\n",
      "Bulu to Ngiemboon\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "! ../copy_rename.sh $src_path $tgt_path Limbum-Ngiemboon $tgt_pair\n",
    "! ../evaluate.sh\n",
    "# because the same dir : put the rename file on place\n",
    "! ../copy_rename.sh $src_path $tgt_path $tgt_pair Limbum-Ngiemboon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bafia_Ewondo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: dump_path=/home/jupyter/models/africa/cluster1\n",
      "env: exp_name=mlm_tlm_BafiaEwondo\n",
      "env: data_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo\n",
      "env: lgs=Bafia-MKPAMAN_AMVOE_Ewondo\n",
      "env: mlm_steps=Bafia,MKPAMAN_AMVOE_Ewondo\n",
      "env: tgt_pair=Bafia-MKPAMAN_AMVOE_Ewondo\n",
      "env: src_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo\n",
      "env: tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo\n"
     ]
    }
   ],
   "source": [
    "%env dump_path=/home/jupyter/models/africa/cluster1\n",
    "%env exp_name=mlm_tlm_BafiaEwondo\n",
    "%env data_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo\n",
    "%env lgs=Bafia-MKPAMAN_AMVOE_Ewondo\n",
    "%env mlm_steps=Bafia,MKPAMAN_AMVOE_Ewondo\n",
    "%env tgt_pair=Bafia-MKPAMAN_AMVOE_Ewondo\n",
    "%env src_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo\n",
    "%env tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo\n",
    "\n",
    "# creation of the dummy files so that the experiment does not bug\n",
    "! touch /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/train.Bafia.pth\n",
    "! touch /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/train.MKPAMAN_AMVOE_Ewondo.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Bafia_Ewondo vs Bulu and Ghomala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Bulu to Bafia\n",
      "Ghomala to MKPAMAN_AMVOE_Ewondo\n",
      "=====================\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/30/20 14:56:21 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/30/20 14:56:21 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_BafiaEwondo --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo' --lgs 'Bafia-MKPAMAN_AMVOE_Ewondo' --clm_steps '' --mlm_steps 'Bafia,MKPAMAN_AMVOE_Ewondo' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaEwondo/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_BafiaEwondo\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Bafia', 1: 'MKPAMAN_AMVOE_Ewondo'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Bafia': 0, 'MKPAMAN_AMVOE_Ewondo': 1}\n",
      "                                     langs: ['Bafia', 'MKPAMAN_AMVOE_Ewondo']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Bafia-MKPAMAN_AMVOE_Ewondo']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Bafia', None), ('MKPAMAN_AMVOE_Ewondo', None)]\n",
      "                                     mono_dataset: {'Bafia': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/train.Bafia.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.Bafia.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.Bafia.pth'}, 'MKPAMAN_AMVOE_Ewondo': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/train.MKPAMAN_AMVOE_Ewondo.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': False, 'valid': False, 'test': False}\n",
      "                                     remove_long_sentences_test: False\n",
      "                                     remove_long_sentences_train: False\n",
      "                                     remove_long_sentences_valid: False\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/30/20 14:56:21 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaEwondo/maml\n",
      "                                     \n",
      "INFO - 05/30/20 14:56:21 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_BafiaEwondo --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo' --lgs 'Bafia-MKPAMAN_AMVOE_Ewondo' --clm_steps '' --mlm_steps 'Bafia,MKPAMAN_AMVOE_Ewondo' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/30/20 14:56:21 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/30/20 14:56:21 - 0:00:00 - ============ langs: Bafia, MKPAMAN_AMVOE_Ewondo\n",
      "INFO - 05/30/20 14:56:21 - 0:00:00 - ============ Monolingual data (Bafia)\n",
      "INFO - 05/30/20 14:56:21 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.Bafia.pth ...\n",
      "INFO - 05/30/20 14:56:21 - 0:00:00 - 160924 words (9128 unique) in 3973 sentences. 10022 unknown words (176 unique) covering 6.23% of the data.\n",
      "INFO - 05/30/20 14:56:21 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/30/20 14:56:21 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.Bafia.pth ...\n",
      "INFO - 05/30/20 14:56:21 - 0:00:00 - 161537 words (9128 unique) in 3973 sentences. 9851 unknown words (187 unique) covering 6.10% of the data.\n",
      "INFO - 05/30/20 14:56:21 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/30/20 14:56:21 - 0:00:00 - ============ Monolingual data (MKPAMAN_AMVOE_Ewondo)\n",
      "INFO - 05/30/20 14:56:21 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/30/20 14:56:21 - 0:00:00 - 237746 words (9128 unique) in 3971 sentences. 57409 unknown words (107 unique) covering 24.15% of the data.\n",
      "INFO - 05/30/20 14:56:21 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/30/20 14:56:21 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/30/20 14:56:21 - 0:00:00 - 236183 words (9128 unique) in 3971 sentences. 57363 unknown words (105 unique) covering 24.29% of the data.\n",
      "INFO - 05/30/20 14:56:21 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/30/20 14:56:21 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/30/20 14:56:21 - 0:00:00 - Monolingual data   - valid -        Bafia:      3973\n",
      "INFO - 05/30/20 14:56:21 - 0:00:00 - Monolingual data   -  test -        Bafia:      3973\n",
      "INFO - 05/30/20 14:56:21 - 0:00:00 - Monolingual data   - valid - MKPAMAN_AMVOE_Ewondo:      3971\n",
      "INFO - 05/30/20 14:56:21 - 0:00:00 - Monolingual data   -  test - MKPAMAN_AMVOE_Ewondo:      3971\n",
      "\n",
      "INFO - 05/30/20 14:56:22 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(9128, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=9128, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/30/20 14:56:22 - 0:00:01 - Number of parameters (model): 85461928\n",
      "INFO - 05/30/20 14:56:27 - 0:00:06 - Found 0 memories.\n",
      "INFO - 05/30/20 14:56:27 - 0:00:06 - Found 6 FFN.\n",
      "INFO - 05/30/20 14:56:27 - 0:00:06 - Found 102 parameters in model.\n",
      "INFO - 05/30/20 14:56:27 - 0:00:06 - Optimizers: model\n",
      "WARNING - 05/30/20 14:56:27 - 0:00:06 - Reloading checkpoint from /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaEwondo/maml/checkpoint.pth ...\n",
      "WARNING - 05/30/20 14:56:27 - 0:00:07 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/30/20 14:56:27 - 0:00:07 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/30/20 14:56:27 - 0:00:07 - Checkpoint reloaded. Resuming at epoch 40 / iteration 5320 ...\n",
      "INFO - 05/30/20 14:56:30 - 0:00:09 - epoch -> 40.000000\n",
      "INFO - 05/30/20 14:56:30 - 0:00:09 - valid_Bafia_mlm_ppl -> 6503.445888\n",
      "INFO - 05/30/20 14:56:30 - 0:00:09 - valid_Bafia_mlm_acc -> 13.471503\n",
      "INFO - 05/30/20 14:56:30 - 0:00:09 - valid_MKPAMAN_AMVOE_Ewondo_mlm_ppl -> 9497.315817\n",
      "INFO - 05/30/20 14:56:30 - 0:00:09 - valid_MKPAMAN_AMVOE_Ewondo_mlm_acc -> 9.067358\n",
      "INFO - 05/30/20 14:56:30 - 0:00:09 - valid_mlm_ppl -> 8000.380852\n",
      "INFO - 05/30/20 14:56:30 - 0:00:09 - valid_mlm_acc -> 11.269430\n",
      "INFO - 05/30/20 14:56:30 - 0:00:09 - test_Bafia_mlm_ppl -> 10428.270520\n",
      "INFO - 05/30/20 14:56:30 - 0:00:09 - test_Bafia_mlm_acc -> 9.326425\n",
      "INFO - 05/30/20 14:56:30 - 0:00:09 - test_MKPAMAN_AMVOE_Ewondo_mlm_ppl -> 6747.058388\n",
      "INFO - 05/30/20 14:56:30 - 0:00:09 - test_MKPAMAN_AMVOE_Ewondo_mlm_acc -> 11.658031\n",
      "INFO - 05/30/20 14:56:30 - 0:00:09 - test_mlm_ppl -> 8587.664454\n",
      "INFO - 05/30/20 14:56:30 - 0:00:09 - test_mlm_acc -> 10.492228\n",
      "INFO - 05/30/20 14:56:30 - 0:00:09 - __log__:{\"epoch\": 40, \"valid_Bafia_mlm_ppl\": 6503.445887926964, \"valid_Bafia_mlm_acc\": 13.471502590673575, \"valid_MKPAMAN_AMVOE_Ewondo_mlm_ppl\": 9497.31581660711, \"valid_MKPAMAN_AMVOE_Ewondo_mlm_acc\": 9.067357512953368, \"valid_mlm_ppl\": 8000.380852267037, \"valid_mlm_acc\": 11.269430051813472, \"test_Bafia_mlm_ppl\": 10428.270520449894, \"test_Bafia_mlm_acc\": 9.32642487046632, \"test_MKPAMAN_AMVOE_Ewondo_mlm_ppl\": 6747.058387689667, \"test_MKPAMAN_AMVOE_Ewondo_mlm_acc\": 11.658031088082902, \"test_mlm_ppl\": 8587.664454069782, \"test_mlm_acc\": 10.492227979274611}\n",
      "=====================\n",
      "Bafia to Bulu\n",
      "MKPAMAN_AMVOE_Ewondo to Ghomala\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "! ../copy_rename.sh $src_path $tgt_path Bulu-Ghomala $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : put the rename file on place\n",
    "! ../copy_rename.sh $src_path $tgt_path $tgt_pair Bulu-Ghomala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Limbum to Bafia\n",
      "Ngiemboon to MKPAMAN_AMVOE_Ewondo\n",
      "=====================\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/30/20 14:57:04 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/30/20 14:57:04 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_BafiaEwondo --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo' --lgs 'Bafia-MKPAMAN_AMVOE_Ewondo' --clm_steps '' --mlm_steps 'Bafia,MKPAMAN_AMVOE_Ewondo' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaEwondo/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_BafiaEwondo\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Bafia', 1: 'MKPAMAN_AMVOE_Ewondo'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Bafia': 0, 'MKPAMAN_AMVOE_Ewondo': 1}\n",
      "                                     langs: ['Bafia', 'MKPAMAN_AMVOE_Ewondo']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Bafia-MKPAMAN_AMVOE_Ewondo']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Bafia', None), ('MKPAMAN_AMVOE_Ewondo', None)]\n",
      "                                     mono_dataset: {'Bafia': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/train.Bafia.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.Bafia.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.Bafia.pth'}, 'MKPAMAN_AMVOE_Ewondo': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/train.MKPAMAN_AMVOE_Ewondo.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': False, 'valid': False, 'test': False}\n",
      "                                     remove_long_sentences_test: False\n",
      "                                     remove_long_sentences_train: False\n",
      "                                     remove_long_sentences_valid: False\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/30/20 14:57:04 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaEwondo/maml\n",
      "                                     \n",
      "INFO - 05/30/20 14:57:04 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_BafiaEwondo --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo' --lgs 'Bafia-MKPAMAN_AMVOE_Ewondo' --clm_steps '' --mlm_steps 'Bafia,MKPAMAN_AMVOE_Ewondo' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/30/20 14:57:04 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/30/20 14:57:04 - 0:00:00 - ============ langs: Bafia, MKPAMAN_AMVOE_Ewondo\n",
      "INFO - 05/30/20 14:57:04 - 0:00:00 - ============ Monolingual data (Bafia)\n",
      "INFO - 05/30/20 14:57:04 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.Bafia.pth ...\n",
      "INFO - 05/30/20 14:57:04 - 0:00:00 - 246312 words (9128 unique) in 3960 sentences. 6171 unknown words (99 unique) covering 2.51% of the data.\n",
      "INFO - 05/30/20 14:57:04 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/30/20 14:57:04 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.Bafia.pth ...\n",
      "INFO - 05/30/20 14:57:04 - 0:00:00 - 243718 words (9128 unique) in 3960 sentences. 6390 unknown words (104 unique) covering 2.62% of the data.\n",
      "INFO - 05/30/20 14:57:04 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/30/20 14:57:04 - 0:00:00 - ============ Monolingual data (MKPAMAN_AMVOE_Ewondo)\n",
      "INFO - 05/30/20 14:57:04 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/30/20 14:57:04 - 0:00:00 - 409388 words (9128 unique) in 3965 sentences. 106977 unknown words (106 unique) covering 26.13% of the data.\n",
      "INFO - 05/30/20 14:57:04 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/30/20 14:57:04 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bafia_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/30/20 14:57:04 - 0:00:00 - 414994 words (9128 unique) in 3965 sentences. 108121 unknown words (109 unique) covering 26.05% of the data.\n",
      "INFO - 05/30/20 14:57:04 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/30/20 14:57:04 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/30/20 14:57:04 - 0:00:00 - Monolingual data   - valid -        Bafia:      3960\n",
      "INFO - 05/30/20 14:57:04 - 0:00:00 - Monolingual data   -  test -        Bafia:      3960\n",
      "INFO - 05/30/20 14:57:04 - 0:00:00 - Monolingual data   - valid - MKPAMAN_AMVOE_Ewondo:      3965\n",
      "INFO - 05/30/20 14:57:04 - 0:00:00 - Monolingual data   -  test - MKPAMAN_AMVOE_Ewondo:      3965\n",
      "\n",
      "INFO - 05/30/20 14:57:05 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(9128, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=9128, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/30/20 14:57:05 - 0:00:01 - Number of parameters (model): 85461928\n",
      "INFO - 05/30/20 14:57:11 - 0:00:07 - Found 0 memories.\n",
      "INFO - 05/30/20 14:57:11 - 0:00:07 - Found 6 FFN.\n",
      "INFO - 05/30/20 14:57:11 - 0:00:07 - Found 102 parameters in model.\n",
      "INFO - 05/30/20 14:57:11 - 0:00:07 - Optimizers: model\n",
      "WARNING - 05/30/20 14:57:11 - 0:00:07 - Reloading checkpoint from /home/jupyter/models/africa/cluster1/mlm_tlm_BafiaEwondo/maml/checkpoint.pth ...\n",
      "WARNING - 05/30/20 14:57:11 - 0:00:08 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/30/20 14:57:11 - 0:00:08 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/30/20 14:57:11 - 0:00:08 - Checkpoint reloaded. Resuming at epoch 40 / iteration 5320 ...\n",
      "INFO - 05/30/20 14:57:14 - 0:00:10 - epoch -> 40.000000\n",
      "INFO - 05/30/20 14:57:14 - 0:00:10 - valid_Bafia_mlm_ppl -> 11210.565702\n",
      "INFO - 05/30/20 14:57:14 - 0:00:10 - valid_Bafia_mlm_acc -> 7.512953\n",
      "INFO - 05/30/20 14:57:14 - 0:00:10 - valid_MKPAMAN_AMVOE_Ewondo_mlm_ppl -> 15771.164979\n",
      "INFO - 05/30/20 14:57:14 - 0:00:10 - valid_MKPAMAN_AMVOE_Ewondo_mlm_acc -> 5.440415\n",
      "INFO - 05/30/20 14:57:14 - 0:00:10 - valid_mlm_ppl -> 13490.865340\n",
      "INFO - 05/30/20 14:57:14 - 0:00:10 - valid_mlm_acc -> 6.476684\n",
      "INFO - 05/30/20 14:57:14 - 0:00:10 - test_Bafia_mlm_ppl -> 8554.842547\n",
      "INFO - 05/30/20 14:57:14 - 0:00:10 - test_Bafia_mlm_acc -> 9.067358\n",
      "INFO - 05/30/20 14:57:14 - 0:00:10 - test_MKPAMAN_AMVOE_Ewondo_mlm_ppl -> 12096.604159\n",
      "INFO - 05/30/20 14:57:14 - 0:00:10 - test_MKPAMAN_AMVOE_Ewondo_mlm_acc -> 6.476684\n",
      "INFO - 05/30/20 14:57:14 - 0:00:10 - test_mlm_ppl -> 10325.723353\n",
      "INFO - 05/30/20 14:57:14 - 0:00:10 - test_mlm_acc -> 7.772021\n",
      "INFO - 05/30/20 14:57:14 - 0:00:10 - __log__:{\"epoch\": 40, \"valid_Bafia_mlm_ppl\": 11210.565701825893, \"valid_Bafia_mlm_acc\": 7.512953367875648, \"valid_MKPAMAN_AMVOE_Ewondo_mlm_ppl\": 15771.164978753719, \"valid_MKPAMAN_AMVOE_Ewondo_mlm_acc\": 5.4404145077720205, \"valid_mlm_ppl\": 13490.865340289805, \"valid_mlm_acc\": 6.476683937823834, \"test_Bafia_mlm_ppl\": 8554.842547292164, \"test_Bafia_mlm_acc\": 9.067357512953368, \"test_MKPAMAN_AMVOE_Ewondo_mlm_ppl\": 12096.604158699396, \"test_MKPAMAN_AMVOE_Ewondo_mlm_acc\": 6.476683937823834, \"test_mlm_ppl\": 10325.723352995781, \"test_mlm_acc\": 7.772020725388601}\n",
      "=====================\n",
      "Bafia to Limbum\n",
      "MKPAMAN_AMVOE_Ewondo to Ngiemboon\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "###### Bafia_Ewondo vs Limbum and Ngiemboon\n",
    "! ../copy_rename.sh $src_path $tgt_path Limbum-Ngiemboon $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : put the rename file on place\n",
    "! ../copy_rename.sh $src_path $tgt_path $tgt_pair Limbum-Ngiemboon "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bulu_Ewondo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: dump_path=/home/jupyter/models/africa/cluster1\n",
      "env: exp_name=mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo\n",
      "env: data_path=/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo\n",
      "env: lgs=Bulu-MKPAMAN_AMVOE_Ewondo\n",
      "env: mlm_steps=Bulu,MKPAMAN_AMVOE_Ewondo\n",
      "env: tgt_pair=Bulu-MKPAMAN_AMVOE_Ewondo\n",
      "env: src_path=/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo\n",
      "env: tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo\n"
     ]
    }
   ],
   "source": [
    "%env dump_path=/home/jupyter/models/africa/cluster1\n",
    "%env exp_name=mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo\n",
    "%env data_path=/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo\n",
    "%env lgs=Bulu-MKPAMAN_AMVOE_Ewondo\n",
    "%env mlm_steps=Bulu,MKPAMAN_AMVOE_Ewondo\n",
    "%env tgt_pair=Bulu-MKPAMAN_AMVOE_Ewondo\n",
    "%env src_path=/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo\n",
    "%env tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo\n",
    "\n",
    "# creation of the dummy files so that the experiment does not bug\n",
    "! touch /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/train.Bulu.pth\n",
    "! touch /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/train.MKPAMAN_AMVOE_Ewondo.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Bulu_Ewondo vs Bafia and Ghomala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Bafia to Bulu\n",
      "Ghomala to MKPAMAN_AMVOE_Ewondo\n",
      "=====================\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/30/20 14:57:53 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/30/20 14:57:53 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo' --lgs 'Bulu-MKPAMAN_AMVOE_Ewondo' --clm_steps '' --mlm_steps 'Bulu,MKPAMAN_AMVOE_Ewondo' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster1/mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Bulu', 1: 'MKPAMAN_AMVOE_Ewondo'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Bulu': 0, 'MKPAMAN_AMVOE_Ewondo': 1}\n",
      "                                     langs: ['Bulu', 'MKPAMAN_AMVOE_Ewondo']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Bulu-MKPAMAN_AMVOE_Ewondo']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Bulu', None), ('MKPAMAN_AMVOE_Ewondo', None)]\n",
      "                                     mono_dataset: {'Bulu': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/train.Bulu.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.Bulu.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.Bulu.pth'}, 'MKPAMAN_AMVOE_Ewondo': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/train.MKPAMAN_AMVOE_Ewondo.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': False, 'valid': False, 'test': False}\n",
      "                                     remove_long_sentences_test: False\n",
      "                                     remove_long_sentences_train: False\n",
      "                                     remove_long_sentences_valid: False\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/30/20 14:57:53 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster1/mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo/maml\n",
      "                                     \n",
      "INFO - 05/30/20 14:57:53 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo' --lgs 'Bulu-MKPAMAN_AMVOE_Ewondo' --clm_steps '' --mlm_steps 'Bulu,MKPAMAN_AMVOE_Ewondo' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/30/20 14:57:53 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/30/20 14:57:53 - 0:00:00 - ============ langs: Bulu, MKPAMAN_AMVOE_Ewondo\n",
      "INFO - 05/30/20 14:57:53 - 0:00:00 - ============ Monolingual data (Bulu)\n",
      "INFO - 05/30/20 14:57:53 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.Bulu.pth ...\n",
      "INFO - 05/30/20 14:57:53 - 0:00:00 - 309529 words (9061 unique) in 3975 sentences. 106352 unknown words (126 unique) covering 34.36% of the data.\n",
      "INFO - 05/30/20 14:57:53 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/30/20 14:57:53 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.Bulu.pth ...\n",
      "INFO - 05/30/20 14:57:53 - 0:00:00 - 311653 words (9061 unique) in 3975 sentences. 108247 unknown words (118 unique) covering 34.73% of the data.\n",
      "INFO - 05/30/20 14:57:53 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/30/20 14:57:53 - 0:00:00 - ============ Monolingual data (MKPAMAN_AMVOE_Ewondo)\n",
      "INFO - 05/30/20 14:57:53 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/30/20 14:57:53 - 0:00:00 - 260864 words (9061 unique) in 3971 sentences. 84253 unknown words (105 unique) covering 32.30% of the data.\n",
      "INFO - 05/30/20 14:57:53 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/30/20 14:57:53 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/30/20 14:57:53 - 0:00:00 - 259104 words (9061 unique) in 3971 sentences. 83798 unknown words (115 unique) covering 32.34% of the data.\n",
      "INFO - 05/30/20 14:57:53 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/30/20 14:57:53 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/30/20 14:57:53 - 0:00:00 - Monolingual data   - valid -         Bulu:      3975\n",
      "INFO - 05/30/20 14:57:53 - 0:00:00 - Monolingual data   -  test -         Bulu:      3975\n",
      "INFO - 05/30/20 14:57:53 - 0:00:00 - Monolingual data   - valid - MKPAMAN_AMVOE_Ewondo:      3971\n",
      "INFO - 05/30/20 14:57:53 - 0:00:00 - Monolingual data   -  test - MKPAMAN_AMVOE_Ewondo:      3971\n",
      "\n",
      "INFO - 05/30/20 14:57:54 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(9061, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=9061, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/30/20 14:57:54 - 0:00:01 - Number of parameters (model): 85393253\n",
      "INFO - 05/30/20 14:57:59 - 0:00:06 - Found 0 memories.\n",
      "INFO - 05/30/20 14:57:59 - 0:00:06 - Found 6 FFN.\n",
      "INFO - 05/30/20 14:57:59 - 0:00:06 - Found 102 parameters in model.\n",
      "INFO - 05/30/20 14:57:59 - 0:00:06 - Optimizers: model\n",
      "WARNING - 05/30/20 14:57:59 - 0:00:06 - Reloading checkpoint from /home/jupyter/models/africa/cluster1/mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo/maml/checkpoint.pth ...\n",
      "WARNING - 05/30/20 14:58:00 - 0:00:07 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/30/20 14:58:00 - 0:00:07 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/30/20 14:58:00 - 0:00:07 - Checkpoint reloaded. Resuming at epoch 32 / iteration 8480 ...\n",
      "INFO - 05/30/20 14:58:03 - 0:00:10 - epoch -> 32.000000\n",
      "INFO - 05/30/20 14:58:03 - 0:00:10 - valid_Bulu_mlm_ppl -> 23647.041771\n",
      "INFO - 05/30/20 14:58:03 - 0:00:10 - valid_Bulu_mlm_acc -> 7.512953\n",
      "INFO - 05/30/20 14:58:03 - 0:00:10 - valid_MKPAMAN_AMVOE_Ewondo_mlm_ppl -> 22671.785812\n",
      "INFO - 05/30/20 14:58:03 - 0:00:10 - valid_MKPAMAN_AMVOE_Ewondo_mlm_acc -> 7.772021\n",
      "INFO - 05/30/20 14:58:03 - 0:00:10 - valid_mlm_ppl -> 23159.413791\n",
      "INFO - 05/30/20 14:58:03 - 0:00:10 - valid_mlm_acc -> 7.642487\n",
      "INFO - 05/30/20 14:58:03 - 0:00:10 - test_Bulu_mlm_ppl -> 36625.892083\n",
      "INFO - 05/30/20 14:58:03 - 0:00:10 - test_Bulu_mlm_acc -> 4.145078\n",
      "INFO - 05/30/20 14:58:03 - 0:00:10 - test_MKPAMAN_AMVOE_Ewondo_mlm_ppl -> 34877.150659\n",
      "INFO - 05/30/20 14:58:03 - 0:00:10 - test_MKPAMAN_AMVOE_Ewondo_mlm_acc -> 6.994819\n",
      "INFO - 05/30/20 14:58:03 - 0:00:10 - test_mlm_ppl -> 35751.521371\n",
      "INFO - 05/30/20 14:58:03 - 0:00:10 - test_mlm_acc -> 5.569948\n",
      "INFO - 05/30/20 14:58:03 - 0:00:10 - __log__:{\"epoch\": 32, \"valid_Bulu_mlm_ppl\": 23647.041770569955, \"valid_Bulu_mlm_acc\": 7.512953367875648, \"valid_MKPAMAN_AMVOE_Ewondo_mlm_ppl\": 22671.78581187161, \"valid_MKPAMAN_AMVOE_Ewondo_mlm_acc\": 7.772020725388601, \"valid_mlm_ppl\": 23159.41379122078, \"valid_mlm_acc\": 7.642487046632125, \"test_Bulu_mlm_ppl\": 36625.89208344548, \"test_Bulu_mlm_acc\": 4.1450777202072535, \"test_MKPAMAN_AMVOE_Ewondo_mlm_ppl\": 34877.150659435465, \"test_MKPAMAN_AMVOE_Ewondo_mlm_acc\": 6.994818652849741, \"test_mlm_ppl\": 35751.52137144047, \"test_mlm_acc\": 5.569948186528498}\n",
      "=====================\n",
      "Bulu to Bafia\n",
      "MKPAMAN_AMVOE_Ewondo to Ghomala\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "! ../copy_rename.sh $src_path $tgt_path Bafia-Ghomala $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : put the rename file on place\n",
    "! ../copy_rename.sh $src_path $tgt_path $tgt_pair Bafia-Ghomala "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Bulu_Ewondo vs Limbum and Ngiemboon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Limbum to Bulu\n",
      "Ngiemboon to MKPAMAN_AMVOE_Ewondo\n",
      "=====================\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/30/20 14:58:27 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/30/20 14:58:27 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo' --lgs 'Bulu-MKPAMAN_AMVOE_Ewondo' --clm_steps '' --mlm_steps 'Bulu,MKPAMAN_AMVOE_Ewondo' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster1/mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Bulu', 1: 'MKPAMAN_AMVOE_Ewondo'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Bulu': 0, 'MKPAMAN_AMVOE_Ewondo': 1}\n",
      "                                     langs: ['Bulu', 'MKPAMAN_AMVOE_Ewondo']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Bulu-MKPAMAN_AMVOE_Ewondo']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Bulu', None), ('MKPAMAN_AMVOE_Ewondo', None)]\n",
      "                                     mono_dataset: {'Bulu': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/train.Bulu.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.Bulu.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.Bulu.pth'}, 'MKPAMAN_AMVOE_Ewondo': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/train.MKPAMAN_AMVOE_Ewondo.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': False, 'valid': False, 'test': False}\n",
      "                                     remove_long_sentences_test: False\n",
      "                                     remove_long_sentences_train: False\n",
      "                                     remove_long_sentences_valid: False\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/30/20 14:58:27 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster1/mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo/maml\n",
      "                                     \n",
      "INFO - 05/30/20 14:58:27 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo --exp_id maml --dump_path '/home/jupyter/models/africa/cluster1' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo' --lgs 'Bulu-MKPAMAN_AMVOE_Ewondo' --clm_steps '' --mlm_steps 'Bulu,MKPAMAN_AMVOE_Ewondo' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/30/20 14:58:27 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/30/20 14:58:27 - 0:00:00 - ============ langs: Bulu, MKPAMAN_AMVOE_Ewondo\n",
      "INFO - 05/30/20 14:58:27 - 0:00:00 - ============ Monolingual data (Bulu)\n",
      "INFO - 05/30/20 14:58:27 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.Bulu.pth ...\n",
      "INFO - 05/30/20 14:58:27 - 0:00:00 - 268013 words (9061 unique) in 3960 sentences. 15273 unknown words (105 unique) covering 5.70% of the data.\n",
      "INFO - 05/30/20 14:58:27 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/30/20 14:58:27 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.Bulu.pth ...\n",
      "INFO - 05/30/20 14:58:27 - 0:00:00 - 265072 words (9061 unique) in 3960 sentences. 14978 unknown words (108 unique) covering 5.65% of the data.\n",
      "INFO - 05/30/20 14:58:27 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/30/20 14:58:27 - 0:00:00 - ============ Monolingual data (MKPAMAN_AMVOE_Ewondo)\n",
      "INFO - 05/30/20 14:58:27 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/valid.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/30/20 14:58:27 - 0:00:00 - 419575 words (9061 unique) in 3965 sentences. 130652 unknown words (131 unique) covering 31.14% of the data.\n",
      "INFO - 05/30/20 14:58:27 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/30/20 14:58:27 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Bulu_Ewondo/test.MKPAMAN_AMVOE_Ewondo.pth ...\n",
      "INFO - 05/30/20 14:58:27 - 0:00:00 - 425658 words (9061 unique) in 3965 sentences. 132100 unknown words (134 unique) covering 31.03% of the data.\n",
      "INFO - 05/30/20 14:58:27 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/30/20 14:58:27 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/30/20 14:58:27 - 0:00:00 - Monolingual data   - valid -         Bulu:      3960\n",
      "INFO - 05/30/20 14:58:27 - 0:00:00 - Monolingual data   -  test -         Bulu:      3960\n",
      "INFO - 05/30/20 14:58:27 - 0:00:00 - Monolingual data   - valid - MKPAMAN_AMVOE_Ewondo:      3965\n",
      "INFO - 05/30/20 14:58:27 - 0:00:00 - Monolingual data   -  test - MKPAMAN_AMVOE_Ewondo:      3965\n",
      "\n",
      "INFO - 05/30/20 14:58:28 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(9061, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=9061, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/30/20 14:58:28 - 0:00:01 - Number of parameters (model): 85393253\n",
      "INFO - 05/30/20 14:58:34 - 0:00:06 - Found 0 memories.\n",
      "INFO - 05/30/20 14:58:34 - 0:00:06 - Found 6 FFN.\n",
      "INFO - 05/30/20 14:58:34 - 0:00:06 - Found 102 parameters in model.\n",
      "INFO - 05/30/20 14:58:34 - 0:00:06 - Optimizers: model\n",
      "WARNING - 05/30/20 14:58:34 - 0:00:06 - Reloading checkpoint from /home/jupyter/models/africa/cluster1/mlm_tlm_BuluMKPAMAN_AMVOE_Ewondo/maml/checkpoint.pth ...\n",
      "WARNING - 05/30/20 14:58:34 - 0:00:07 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/30/20 14:58:34 - 0:00:07 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/30/20 14:58:34 - 0:00:07 - Checkpoint reloaded. Resuming at epoch 32 / iteration 8480 ...\n",
      "INFO - 05/30/20 14:58:37 - 0:00:10 - epoch -> 32.000000\n",
      "INFO - 05/30/20 14:58:37 - 0:00:10 - valid_Bulu_mlm_ppl -> 4814.468511\n",
      "INFO - 05/30/20 14:58:37 - 0:00:10 - valid_Bulu_mlm_acc -> 9.326425\n",
      "INFO - 05/30/20 14:58:37 - 0:00:10 - valid_MKPAMAN_AMVOE_Ewondo_mlm_ppl -> 37184.981011\n",
      "INFO - 05/30/20 14:58:37 - 0:00:10 - valid_MKPAMAN_AMVOE_Ewondo_mlm_acc -> 6.735751\n",
      "INFO - 05/30/20 14:58:37 - 0:00:10 - valid_mlm_ppl -> 20999.724761\n",
      "INFO - 05/30/20 14:58:37 - 0:00:10 - valid_mlm_acc -> 8.031088\n",
      "INFO - 05/30/20 14:58:37 - 0:00:10 - test_Bulu_mlm_ppl -> 12173.249973\n",
      "INFO - 05/30/20 14:58:37 - 0:00:10 - test_Bulu_mlm_acc -> 3.367876\n",
      "INFO - 05/30/20 14:58:37 - 0:00:10 - test_MKPAMAN_AMVOE_Ewondo_mlm_ppl -> 25144.632980\n",
      "INFO - 05/30/20 14:58:37 - 0:00:10 - test_MKPAMAN_AMVOE_Ewondo_mlm_acc -> 11.139896\n",
      "INFO - 05/30/20 14:58:37 - 0:00:10 - test_mlm_ppl -> 18658.941477\n",
      "INFO - 05/30/20 14:58:37 - 0:00:10 - test_mlm_acc -> 7.253886\n",
      "INFO - 05/30/20 14:58:37 - 0:00:10 - __log__:{\"epoch\": 32, \"valid_Bulu_mlm_ppl\": 4814.468510972942, \"valid_Bulu_mlm_acc\": 9.32642487046632, \"valid_MKPAMAN_AMVOE_Ewondo_mlm_ppl\": 37184.9810113367, \"valid_MKPAMAN_AMVOE_Ewondo_mlm_acc\": 6.7357512953367875, \"valid_mlm_ppl\": 20999.72476115482, \"valid_mlm_acc\": 8.031088082901555, \"test_Bulu_mlm_ppl\": 12173.249973407077, \"test_Bulu_mlm_acc\": 3.3678756476683938, \"test_MKPAMAN_AMVOE_Ewondo_mlm_ppl\": 25144.632979821115, \"test_MKPAMAN_AMVOE_Ewondo_mlm_acc\": 11.139896373056995, \"test_mlm_ppl\": 18658.941476614098, \"test_mlm_acc\": 7.253886010362694}\n",
      "=====================\n",
      "Bulu to Limbum\n",
      "MKPAMAN_AMVOE_Ewondo to Ngiemboon\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "! ../copy_rename.sh $src_path $tgt_path Limbum-Ngiemboon $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : put the rename file on place\n",
    "! ../copy_rename.sh $src_path $tgt_path $tgt_pair Limbum-Ngiemboon "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ghomala_Limbum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: dump_path=/home/jupyter/models/africa/cluster3\n",
      "env: exp_name=mlm_tlm_GhomalaLimbum\n",
      "env: data_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum\n",
      "env: lgs=Ghomala-Limbum\n",
      "env: mlm_steps=Ghomala,Limbum\n",
      "env: tgt_pair=Ghomala-Limbum\n",
      "env: src_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum\n",
      "env: tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum\n"
     ]
    }
   ],
   "source": [
    "%env dump_path=/home/jupyter/models/africa/cluster3\n",
    "%env exp_name=mlm_tlm_GhomalaLimbum\n",
    "%env data_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum\n",
    "%env lgs=Ghomala-Limbum\n",
    "%env mlm_steps=Ghomala,Limbum\n",
    "%env tgt_pair=Ghomala-Limbum\n",
    "%env src_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum\n",
    "%env tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum\n",
    "\n",
    "# creation of the dummy files so that the experiment does not bug\n",
    "! touch /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/train.Ghomala.pth\n",
    "! touch /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/train.Limbum.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ghomala_Limbum vs Ngiemboon and Bafia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Ngiemboon to Ghomala\n",
      "Bafia to Limbum\n",
      "=====================\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/30/20 14:59:11 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/30/20 14:59:11 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaLimbum --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum' --lgs 'Ghomala-Limbum' --clm_steps '' --mlm_steps 'Ghomala,Limbum' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_GhomalaLimbum\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Ghomala', 1: 'Limbum'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Ghomala': 0, 'Limbum': 1}\n",
      "                                     langs: ['Ghomala', 'Limbum']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Ghomala-Limbum']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Ghomala', None), ('Limbum', None)]\n",
      "                                     mono_dataset: {'Ghomala': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/train.Ghomala.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Ghomala.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Ghomala.pth'}, 'Limbum': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/train.Limbum.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Limbum.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Limbum.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': False, 'valid': False, 'test': False}\n",
      "                                     remove_long_sentences_test: False\n",
      "                                     remove_long_sentences_train: False\n",
      "                                     remove_long_sentences_valid: False\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/30/20 14:59:11 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml\n",
      "                                     \n",
      "INFO - 05/30/20 14:59:11 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaLimbum --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum' --lgs 'Ghomala-Limbum' --clm_steps '' --mlm_steps 'Ghomala,Limbum' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/30/20 14:59:11 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/30/20 14:59:11 - 0:00:00 - ============ langs: Ghomala, Limbum\n",
      "INFO - 05/30/20 14:59:11 - 0:00:00 - ============ Monolingual data (Ghomala)\n",
      "INFO - 05/30/20 14:59:11 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Ghomala.pth ...\n",
      "INFO - 05/30/20 14:59:11 - 0:00:00 - 400875 words (5057 unique) in 3965 sentences. 161875 unknown words (278 unique) covering 40.38% of the data.\n",
      "INFO - 05/30/20 14:59:11 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/30/20 14:59:11 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Ghomala.pth ...\n",
      "INFO - 05/30/20 14:59:11 - 0:00:00 - 406640 words (5057 unique) in 3965 sentences. 163784 unknown words (277 unique) covering 40.28% of the data.\n",
      "INFO - 05/30/20 14:59:11 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/30/20 14:59:11 - 0:00:00 - ============ Monolingual data (Limbum)\n",
      "INFO - 05/30/20 14:59:11 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Limbum.pth ...\n",
      "INFO - 05/30/20 14:59:11 - 0:00:00 - 275494 words (5057 unique) in 3975 sentences. 84987 unknown words (333 unique) covering 30.85% of the data.\n",
      "INFO - 05/30/20 14:59:11 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/30/20 14:59:11 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Limbum.pth ...\n",
      "INFO - 05/30/20 14:59:11 - 0:00:00 - 277454 words (5057 unique) in 3975 sentences. 86097 unknown words (322 unique) covering 31.03% of the data.\n",
      "INFO - 05/30/20 14:59:11 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/30/20 14:59:11 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/30/20 14:59:11 - 0:00:00 - Monolingual data   - valid -      Ghomala:      3965\n",
      "INFO - 05/30/20 14:59:11 - 0:00:00 - Monolingual data   -  test -      Ghomala:      3965\n",
      "INFO - 05/30/20 14:59:11 - 0:00:00 - Monolingual data   - valid -       Limbum:      3975\n",
      "INFO - 05/30/20 14:59:11 - 0:00:00 - Monolingual data   -  test -       Limbum:      3975\n",
      "\n",
      "INFO - 05/30/20 14:59:12 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(5057, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=5057, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/30/20 14:59:12 - 0:00:01 - Number of parameters (model): 81289153\n",
      "INFO - 05/30/20 14:59:18 - 0:00:07 - Found 0 memories.\n",
      "INFO - 05/30/20 14:59:18 - 0:00:07 - Found 6 FFN.\n",
      "INFO - 05/30/20 14:59:18 - 0:00:07 - Found 102 parameters in model.\n",
      "INFO - 05/30/20 14:59:18 - 0:00:07 - Optimizers: model\n",
      "WARNING - 05/30/20 14:59:18 - 0:00:07 - Reloading checkpoint from /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml/checkpoint.pth ...\n",
      "WARNING - 05/30/20 14:59:19 - 0:00:07 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/30/20 14:59:19 - 0:00:07 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/30/20 14:59:19 - 0:00:07 - Checkpoint reloaded. Resuming at epoch 52 / iteration 13728 ...\n",
      "INFO - 05/30/20 14:59:21 - 0:00:10 - epoch -> 52.000000\n",
      "INFO - 05/30/20 14:59:21 - 0:00:10 - valid_Ghomala_mlm_ppl -> 28911.900525\n",
      "INFO - 05/30/20 14:59:21 - 0:00:10 - valid_Ghomala_mlm_acc -> 8.808290\n",
      "INFO - 05/30/20 14:59:21 - 0:00:10 - valid_Limbum_mlm_ppl -> 31835.773778\n",
      "INFO - 05/30/20 14:59:21 - 0:00:10 - valid_Limbum_mlm_acc -> 6.217617\n",
      "INFO - 05/30/20 14:59:21 - 0:00:10 - valid_mlm_ppl -> 30373.837152\n",
      "INFO - 05/30/20 14:59:21 - 0:00:10 - valid_mlm_acc -> 7.512953\n",
      "INFO - 05/30/20 14:59:21 - 0:00:10 - test_Ghomala_mlm_ppl -> 48240.166124\n",
      "INFO - 05/30/20 14:59:21 - 0:00:10 - test_Ghomala_mlm_acc -> 2.590674\n",
      "INFO - 05/30/20 14:59:21 - 0:00:10 - test_Limbum_mlm_ppl -> 33169.420261\n",
      "INFO - 05/30/20 14:59:21 - 0:00:10 - test_Limbum_mlm_acc -> 9.326425\n",
      "INFO - 05/30/20 14:59:21 - 0:00:10 - test_mlm_ppl -> 40704.793192\n",
      "INFO - 05/30/20 14:59:21 - 0:00:10 - test_mlm_acc -> 5.958549\n",
      "INFO - 05/30/20 14:59:21 - 0:00:10 - __log__:{\"epoch\": 52, \"valid_Ghomala_mlm_ppl\": 28911.900525130626, \"valid_Ghomala_mlm_acc\": 8.808290155440414, \"valid_Limbum_mlm_ppl\": 31835.77377814693, \"valid_Limbum_mlm_acc\": 6.217616580310881, \"valid_mlm_ppl\": 30373.837151638778, \"valid_mlm_acc\": 7.512953367875648, \"test_Ghomala_mlm_ppl\": 48240.16612396391, \"test_Ghomala_mlm_acc\": 2.5906735751295336, \"test_Limbum_mlm_ppl\": 33169.42026098916, \"test_Limbum_mlm_acc\": 9.32642487046632, \"test_mlm_ppl\": 40704.79319247653, \"test_mlm_acc\": 5.958549222797927}\n",
      "=====================\n",
      "Ghomala to Ngiemboon\n",
      "Limbum to Bafia\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "! ../copy_rename.sh $src_path $tgt_path Ngiemboon-Bafia $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : put the rename file on place\n",
    "! ../copy_rename.sh $src_path $tgt_path $tgt_pair Ngiemboon-Bafia "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ghomala_Limbum vs Bulu and Ewondo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Bulu to Ghomala\n",
      "MKPAMAN_AMVOE_Ewondo to Limbum\n",
      "=====================\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/30/20 14:59:44 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/30/20 14:59:44 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaLimbum --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum' --lgs 'Ghomala-Limbum' --clm_steps '' --mlm_steps 'Ghomala,Limbum' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_GhomalaLimbum\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Ghomala', 1: 'Limbum'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Ghomala': 0, 'Limbum': 1}\n",
      "                                     langs: ['Ghomala', 'Limbum']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Ghomala-Limbum']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Ghomala', None), ('Limbum', None)]\n",
      "                                     mono_dataset: {'Ghomala': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/train.Ghomala.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Ghomala.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Ghomala.pth'}, 'Limbum': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/train.Limbum.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Limbum.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Limbum.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': False, 'valid': False, 'test': False}\n",
      "                                     remove_long_sentences_test: False\n",
      "                                     remove_long_sentences_train: False\n",
      "                                     remove_long_sentences_valid: False\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/30/20 14:59:44 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml\n",
      "                                     \n",
      "INFO - 05/30/20 14:59:44 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaLimbum --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum' --lgs 'Ghomala-Limbum' --clm_steps '' --mlm_steps 'Ghomala,Limbum' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/30/20 14:59:44 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/30/20 14:59:44 - 0:00:00 - ============ langs: Ghomala, Limbum\n",
      "INFO - 05/30/20 14:59:44 - 0:00:00 - ============ Monolingual data (Ghomala)\n",
      "INFO - 05/30/20 14:59:44 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Ghomala.pth ...\n",
      "INFO - 05/30/20 14:59:44 - 0:00:00 - 189585 words (5057 unique) in 3973 sentences. 30222 unknown words (339 unique) covering 15.94% of the data.\n",
      "INFO - 05/30/20 14:59:44 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/30/20 14:59:44 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Ghomala.pth ...\n",
      "INFO - 05/30/20 14:59:44 - 0:00:00 - 189586 words (5057 unique) in 3973 sentences. 30074 unknown words (328 unique) covering 15.86% of the data.\n",
      "INFO - 05/30/20 14:59:44 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/30/20 14:59:44 - 0:00:00 - ============ Monolingual data (Limbum)\n",
      "INFO - 05/30/20 14:59:44 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/valid.Limbum.pth ...\n",
      "INFO - 05/30/20 14:59:44 - 0:00:00 - 175753 words (5057 unique) in 3972 sentences. 31273 unknown words (349 unique) covering 17.79% of the data.\n",
      "INFO - 05/30/20 14:59:44 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/30/20 14:59:44 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Limbum/test.Limbum.pth ...\n",
      "INFO - 05/30/20 14:59:44 - 0:00:00 - 175999 words (5057 unique) in 3972 sentences. 31429 unknown words (346 unique) covering 17.86% of the data.\n",
      "INFO - 05/30/20 14:59:44 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/30/20 14:59:44 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/30/20 14:59:44 - 0:00:00 - Monolingual data   - valid -      Ghomala:      3973\n",
      "INFO - 05/30/20 14:59:44 - 0:00:00 - Monolingual data   -  test -      Ghomala:      3973\n",
      "INFO - 05/30/20 14:59:44 - 0:00:00 - Monolingual data   - valid -       Limbum:      3972\n",
      "INFO - 05/30/20 14:59:44 - 0:00:00 - Monolingual data   -  test -       Limbum:      3972\n",
      "\n",
      "INFO - 05/30/20 14:59:45 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(5057, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=5057, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/30/20 14:59:45 - 0:00:01 - Number of parameters (model): 81289153\n",
      "INFO - 05/30/20 14:59:51 - 0:00:07 - Found 0 memories.\n",
      "INFO - 05/30/20 14:59:51 - 0:00:07 - Found 6 FFN.\n",
      "INFO - 05/30/20 14:59:51 - 0:00:07 - Found 102 parameters in model.\n",
      "INFO - 05/30/20 14:59:51 - 0:00:07 - Optimizers: model\n",
      "WARNING - 05/30/20 14:59:51 - 0:00:07 - Reloading checkpoint from /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaLimbum/maml/checkpoint.pth ...\n",
      "WARNING - 05/30/20 14:59:52 - 0:00:08 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/30/20 14:59:52 - 0:00:08 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/30/20 14:59:52 - 0:00:08 - Checkpoint reloaded. Resuming at epoch 52 / iteration 13728 ...\n",
      "INFO - 05/30/20 14:59:54 - 0:00:10 - epoch -> 52.000000\n",
      "INFO - 05/30/20 14:59:54 - 0:00:10 - valid_Ghomala_mlm_ppl -> 11265.730122\n",
      "INFO - 05/30/20 14:59:54 - 0:00:10 - valid_Ghomala_mlm_acc -> 12.694301\n",
      "INFO - 05/30/20 14:59:54 - 0:00:10 - valid_Limbum_mlm_ppl -> 8646.258685\n",
      "INFO - 05/30/20 14:59:54 - 0:00:10 - valid_Limbum_mlm_acc -> 12.176166\n",
      "INFO - 05/30/20 14:59:54 - 0:00:10 - valid_mlm_ppl -> 9955.994404\n",
      "INFO - 05/30/20 14:59:54 - 0:00:10 - valid_mlm_acc -> 12.435233\n",
      "INFO - 05/30/20 14:59:54 - 0:00:10 - test_Ghomala_mlm_ppl -> 11263.244021\n",
      "INFO - 05/30/20 14:59:54 - 0:00:10 - test_Ghomala_mlm_acc -> 13.471503\n",
      "INFO - 05/30/20 14:59:54 - 0:00:10 - test_Limbum_mlm_ppl -> 10013.829030\n",
      "INFO - 05/30/20 14:59:54 - 0:00:10 - test_Limbum_mlm_acc -> 13.989637\n",
      "INFO - 05/30/20 14:59:54 - 0:00:10 - test_mlm_ppl -> 10638.536525\n",
      "INFO - 05/30/20 14:59:54 - 0:00:10 - test_mlm_acc -> 13.730570\n",
      "INFO - 05/30/20 14:59:54 - 0:00:10 - __log__:{\"epoch\": 52, \"valid_Ghomala_mlm_ppl\": 11265.730121833452, \"valid_Ghomala_mlm_acc\": 12.694300518134716, \"valid_Limbum_mlm_ppl\": 8646.258685328365, \"valid_Limbum_mlm_acc\": 12.176165803108809, \"valid_mlm_ppl\": 9955.99440358091, \"valid_mlm_acc\": 12.435233160621763, \"test_Ghomala_mlm_ppl\": 11263.244020649063, \"test_Ghomala_mlm_acc\": 13.471502590673575, \"test_Limbum_mlm_ppl\": 10013.8290296135, \"test_Limbum_mlm_acc\": 13.989637305699482, \"test_mlm_ppl\": 10638.536525131281, \"test_mlm_acc\": 13.730569948186528}\n",
      "=====================\n",
      "Ghomala to Bulu\n",
      "Limbum to MKPAMAN_AMVOE_Ewondo\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "! ../copy_rename.sh $src_path $tgt_path Bulu-MKPAMAN_AMVOE_Ewondo $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : put the rename file on place\n",
    "! ../copy_rename.sh $src_path $tgt_path $tgt_pair Bulu-MKPAMAN_AMVOE_Ewondo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ghomala_Ngiemboon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: dump_path=/home/jupyter/models/africa/cluster3\n",
      "env: exp_name=mlm_tlm_GhomalaNgiemboon\n",
      "env: data_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon\n",
      "env: lgs=Ghomala-Ngiemboon\n",
      "env: mlm_steps=Ghomala,Ngiemboon\n",
      "env: tgt_pair=Ghomala-Ngiemboon\n",
      "env: src_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon\n",
      "env: tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon\n"
     ]
    }
   ],
   "source": [
    "%env dump_path=/home/jupyter/models/africa/cluster3\n",
    "%env exp_name=mlm_tlm_GhomalaNgiemboon\n",
    "%env data_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon\n",
    "%env lgs=Ghomala-Ngiemboon\n",
    "%env mlm_steps=Ghomala,Ngiemboon\n",
    "%env tgt_pair=Ghomala-Ngiemboon\n",
    "%env src_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon\n",
    "%env tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon\n",
    "\n",
    "# creation of the dummy files so that the experiment does not bug\n",
    "! touch /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/train.Ghomala.pth\n",
    "! touch /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/train.Ngiemboon.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ghomala_Ngiemboon vs Limbum and Bafia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Limbum to Ghomala\n",
      "Bafia to Ngiemboon\n",
      "=====================\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/30/20 15:00:16 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/30/20 15:00:16 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon' --lgs 'Ghomala-Ngiemboon' --clm_steps '' --mlm_steps 'Ghomala,Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_GhomalaNgiemboon\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Ghomala', 1: 'Ngiemboon'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Ghomala': 0, 'Ngiemboon': 1}\n",
      "                                     langs: ['Ghomala', 'Ngiemboon']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Ghomala-Ngiemboon']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Ghomala', None), ('Ngiemboon', None)]\n",
      "                                     mono_dataset: {'Ghomala': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/train.Ghomala.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ghomala.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ghomala.pth'}, 'Ngiemboon': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/train.Ngiemboon.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ngiemboon.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ngiemboon.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': False, 'valid': False, 'test': False}\n",
      "                                     remove_long_sentences_test: False\n",
      "                                     remove_long_sentences_train: False\n",
      "                                     remove_long_sentences_valid: False\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/30/20 15:00:16 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml\n",
      "                                     \n",
      "INFO - 05/30/20 15:00:16 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon' --lgs 'Ghomala-Ngiemboon' --clm_steps '' --mlm_steps 'Ghomala,Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/30/20 15:00:16 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/30/20 15:00:16 - 0:00:00 - ============ langs: Ghomala, Ngiemboon\n",
      "INFO - 05/30/20 15:00:16 - 0:00:00 - ============ Monolingual data (Ghomala)\n",
      "INFO - 05/30/20 15:00:16 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ghomala.pth ...\n",
      "INFO - 05/30/20 15:00:16 - 0:00:00 - 290647 words (6715 unique) in 3960 sentences. 1408 unknown words (39 unique) covering 0.48% of the data.\n",
      "INFO - 05/30/20 15:00:16 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/30/20 15:00:16 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ghomala.pth ...\n",
      "INFO - 05/30/20 15:00:16 - 0:00:00 - 287818 words (6715 unique) in 3960 sentences. 1440 unknown words (42 unique) covering 0.50% of the data.\n",
      "INFO - 05/30/20 15:00:16 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/30/20 15:00:16 - 0:00:00 - ============ Monolingual data (Ngiemboon)\n",
      "INFO - 05/30/20 15:00:16 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ngiemboon.pth ...\n",
      "INFO - 05/30/20 15:00:16 - 0:00:00 - 305709 words (6715 unique) in 3975 sentences. 40441 unknown words (62 unique) covering 13.23% of the data.\n",
      "INFO - 05/30/20 15:00:16 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/30/20 15:00:16 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ngiemboon.pth ...\n",
      "INFO - 05/30/20 15:00:16 - 0:00:00 - 308014 words (6715 unique) in 3975 sentences. 41032 unknown words (63 unique) covering 13.32% of the data.\n",
      "INFO - 05/30/20 15:00:16 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/30/20 15:00:16 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/30/20 15:00:16 - 0:00:00 - Monolingual data   - valid -      Ghomala:      3960\n",
      "INFO - 05/30/20 15:00:16 - 0:00:00 - Monolingual data   -  test -      Ghomala:      3960\n",
      "INFO - 05/30/20 15:00:16 - 0:00:00 - Monolingual data   - valid -    Ngiemboon:      3975\n",
      "INFO - 05/30/20 15:00:16 - 0:00:00 - Monolingual data   -  test -    Ngiemboon:      3975\n",
      "\n",
      "INFO - 05/30/20 15:00:16 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(6715, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=6715, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/30/20 15:00:16 - 0:00:01 - Number of parameters (model): 82988603\n",
      "INFO - 05/30/20 15:00:22 - 0:00:06 - Found 0 memories.\n",
      "INFO - 05/30/20 15:00:22 - 0:00:06 - Found 6 FFN.\n",
      "INFO - 05/30/20 15:00:22 - 0:00:06 - Found 102 parameters in model.\n",
      "INFO - 05/30/20 15:00:22 - 0:00:06 - Optimizers: model\n",
      "WARNING - 05/30/20 15:00:22 - 0:00:06 - Reloading checkpoint from /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml/checkpoint.pth ...\n",
      "WARNING - 05/30/20 15:00:23 - 0:00:07 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/30/20 15:00:23 - 0:00:07 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/30/20 15:00:23 - 0:00:07 - Checkpoint reloaded. Resuming at epoch 40 / iteration 10600 ...\n",
      "INFO - 05/30/20 15:00:25 - 0:00:09 - epoch -> 40.000000\n",
      "INFO - 05/30/20 15:00:25 - 0:00:09 - valid_Ghomala_mlm_ppl -> 5323.958039\n",
      "INFO - 05/30/20 15:00:25 - 0:00:09 - valid_Ghomala_mlm_acc -> 5.440415\n",
      "INFO - 05/30/20 15:00:25 - 0:00:09 - valid_Ngiemboon_mlm_ppl -> 19833.161065\n",
      "INFO - 05/30/20 15:00:25 - 0:00:09 - valid_Ngiemboon_mlm_acc -> 5.440415\n",
      "INFO - 05/30/20 15:00:25 - 0:00:09 - valid_mlm_ppl -> 12578.559552\n",
      "INFO - 05/30/20 15:00:25 - 0:00:09 - valid_mlm_acc -> 5.440415\n",
      "INFO - 05/30/20 15:00:25 - 0:00:09 - test_Ghomala_mlm_ppl -> 5812.576298\n",
      "INFO - 05/30/20 15:00:25 - 0:00:09 - test_Ghomala_mlm_acc -> 4.922280\n",
      "INFO - 05/30/20 15:00:25 - 0:00:09 - test_Ngiemboon_mlm_ppl -> 19895.904959\n",
      "INFO - 05/30/20 15:00:25 - 0:00:09 - test_Ngiemboon_mlm_acc -> 4.663212\n",
      "INFO - 05/30/20 15:00:25 - 0:00:09 - test_mlm_ppl -> 12854.240629\n",
      "INFO - 05/30/20 15:00:25 - 0:00:09 - test_mlm_acc -> 4.792746\n",
      "INFO - 05/30/20 15:00:25 - 0:00:09 - __log__:{\"epoch\": 40, \"valid_Ghomala_mlm_ppl\": 5323.95803916111, \"valid_Ghomala_mlm_acc\": 5.4404145077720205, \"valid_Ngiemboon_mlm_ppl\": 19833.161065005585, \"valid_Ngiemboon_mlm_acc\": 5.4404145077720205, \"valid_mlm_ppl\": 12578.559552083347, \"valid_mlm_acc\": 5.4404145077720205, \"test_Ghomala_mlm_ppl\": 5812.576297995843, \"test_Ghomala_mlm_acc\": 4.922279792746114, \"test_Ngiemboon_mlm_ppl\": 19895.904959392265, \"test_Ngiemboon_mlm_acc\": 4.66321243523316, \"test_mlm_ppl\": 12854.240628694053, \"test_mlm_acc\": 4.792746113989637}\n",
      "=====================\n",
      "Ghomala to Limbum\n",
      "Ngiemboon to Bafia\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "! ../copy_rename.sh $src_path $tgt_path Limbum-Bafia $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : put the rename file on place\n",
    "! ../copy_rename.sh $src_path $tgt_path $tgt_pair Limbum-Bafia "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ghomala_Ngiemboon vs Bulu and Ewondo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Bulu to Ghomala\n",
      "MKPAMAN_AMVOE_Ewondo to Ngiemboon\n",
      "=====================\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/30/20 15:00:44 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/30/20 15:00:44 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon' --lgs 'Ghomala-Ngiemboon' --clm_steps '' --mlm_steps 'Ghomala,Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_GhomalaNgiemboon\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Ghomala', 1: 'Ngiemboon'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Ghomala': 0, 'Ngiemboon': 1}\n",
      "                                     langs: ['Ghomala', 'Ngiemboon']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Ghomala-Ngiemboon']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Ghomala', None), ('Ngiemboon', None)]\n",
      "                                     mono_dataset: {'Ghomala': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/train.Ghomala.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ghomala.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ghomala.pth'}, 'Ngiemboon': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/train.Ngiemboon.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ngiemboon.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ngiemboon.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': False, 'valid': False, 'test': False}\n",
      "                                     remove_long_sentences_test: False\n",
      "                                     remove_long_sentences_train: False\n",
      "                                     remove_long_sentences_valid: False\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/30/20 15:00:44 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml\n",
      "                                     \n",
      "INFO - 05/30/20 15:00:44 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_GhomalaNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon' --lgs 'Ghomala-Ngiemboon' --clm_steps '' --mlm_steps 'Ghomala,Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/30/20 15:00:44 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/30/20 15:00:44 - 0:00:00 - ============ langs: Ghomala, Ngiemboon\n",
      "INFO - 05/30/20 15:00:44 - 0:00:00 - ============ Monolingual data (Ghomala)\n",
      "INFO - 05/30/20 15:00:44 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ghomala.pth ...\n",
      "INFO - 05/30/20 15:00:44 - 0:00:00 - 215224 words (6715 unique) in 3973 sentences. 7894 unknown words (73 unique) covering 3.67% of the data.\n",
      "INFO - 05/30/20 15:00:44 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/30/20 15:00:44 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ghomala.pth ...\n",
      "INFO - 05/30/20 15:00:44 - 0:00:00 - 215027 words (6715 unique) in 3973 sentences. 7866 unknown words (77 unique) covering 3.66% of the data.\n",
      "INFO - 05/30/20 15:00:45 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/30/20 15:00:45 - 0:00:00 - ============ Monolingual data (Ngiemboon)\n",
      "INFO - 05/30/20 15:00:45 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/valid.Ngiemboon.pth ...\n",
      "INFO - 05/30/20 15:00:45 - 0:00:00 - 192457 words (6715 unique) in 3972 sentences. 5763 unknown words (66 unique) covering 2.99% of the data.\n",
      "INFO - 05/30/20 15:00:45 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/30/20 15:00:45 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Ghomala_Ngiemboon/test.Ngiemboon.pth ...\n",
      "INFO - 05/30/20 15:00:45 - 0:00:00 - 192845 words (6715 unique) in 3972 sentences. 5780 unknown words (63 unique) covering 3.00% of the data.\n",
      "INFO - 05/30/20 15:00:45 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/30/20 15:00:45 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/30/20 15:00:45 - 0:00:00 - Monolingual data   - valid -      Ghomala:      3973\n",
      "INFO - 05/30/20 15:00:45 - 0:00:00 - Monolingual data   -  test -      Ghomala:      3973\n",
      "INFO - 05/30/20 15:00:45 - 0:00:00 - Monolingual data   - valid -    Ngiemboon:      3972\n",
      "INFO - 05/30/20 15:00:45 - 0:00:00 - Monolingual data   -  test -    Ngiemboon:      3972\n",
      "\n",
      "INFO - 05/30/20 15:00:45 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(6715, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=6715, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/30/20 15:00:45 - 0:00:01 - Number of parameters (model): 82988603\n",
      "INFO - 05/30/20 15:00:51 - 0:00:07 - Found 0 memories.\n",
      "INFO - 05/30/20 15:00:51 - 0:00:07 - Found 6 FFN.\n",
      "INFO - 05/30/20 15:00:51 - 0:00:07 - Found 102 parameters in model.\n",
      "INFO - 05/30/20 15:00:51 - 0:00:07 - Optimizers: model\n",
      "WARNING - 05/30/20 15:00:51 - 0:00:07 - Reloading checkpoint from /home/jupyter/models/africa/cluster3/mlm_tlm_GhomalaNgiemboon/maml/checkpoint.pth ...\n",
      "WARNING - 05/30/20 15:00:52 - 0:00:07 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/30/20 15:00:52 - 0:00:07 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/30/20 15:00:52 - 0:00:07 - Checkpoint reloaded. Resuming at epoch 40 / iteration 10600 ...\n",
      "INFO - 05/30/20 15:00:54 - 0:00:09 - epoch -> 40.000000\n",
      "INFO - 05/30/20 15:00:54 - 0:00:09 - valid_Ghomala_mlm_ppl -> 2452.371424\n",
      "INFO - 05/30/20 15:00:54 - 0:00:09 - valid_Ghomala_mlm_acc -> 11.658031\n",
      "INFO - 05/30/20 15:00:54 - 0:00:09 - valid_Ngiemboon_mlm_ppl -> 3062.536426\n",
      "INFO - 05/30/20 15:00:54 - 0:00:09 - valid_Ngiemboon_mlm_acc -> 11.658031\n",
      "INFO - 05/30/20 15:00:54 - 0:00:09 - valid_mlm_ppl -> 2757.453925\n",
      "INFO - 05/30/20 15:00:54 - 0:00:09 - valid_mlm_acc -> 11.658031\n",
      "INFO - 05/30/20 15:00:54 - 0:00:09 - test_Ghomala_mlm_ppl -> 2490.247399\n",
      "INFO - 05/30/20 15:00:54 - 0:00:09 - test_Ghomala_mlm_acc -> 9.067358\n",
      "INFO - 05/30/20 15:00:54 - 0:00:09 - test_Ngiemboon_mlm_ppl -> 3777.740258\n",
      "INFO - 05/30/20 15:00:54 - 0:00:09 - test_Ngiemboon_mlm_acc -> 3.626943\n",
      "INFO - 05/30/20 15:00:54 - 0:00:09 - test_mlm_ppl -> 3133.993828\n",
      "INFO - 05/30/20 15:00:54 - 0:00:09 - test_mlm_acc -> 6.347150\n",
      "INFO - 05/30/20 15:00:54 - 0:00:09 - __log__:{\"epoch\": 40, \"valid_Ghomala_mlm_ppl\": 2452.3714241939615, \"valid_Ghomala_mlm_acc\": 11.658031088082902, \"valid_Ngiemboon_mlm_ppl\": 3062.53642619827, \"valid_Ngiemboon_mlm_acc\": 11.658031088082902, \"valid_mlm_ppl\": 2757.453925196116, \"valid_mlm_acc\": 11.658031088082902, \"test_Ghomala_mlm_ppl\": 2490.2473986368227, \"test_Ghomala_mlm_acc\": 9.067357512953368, \"test_Ngiemboon_mlm_ppl\": 3777.740257591441, \"test_Ngiemboon_mlm_acc\": 3.626943005181347, \"test_mlm_ppl\": 3133.993828114132, \"test_mlm_acc\": 6.347150259067358}\n",
      "=====================\n",
      "Ghomala to Bulu\n",
      "Ngiemboon to MKPAMAN_AMVOE_Ewondo\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "! ../copy_rename.sh $src_path $tgt_path Bulu-MKPAMAN_AMVOE_Ewondo $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : put the rename file on place\n",
    "! ../copy_rename.sh $src_path $tgt_path $tgt_pair Bulu-MKPAMAN_AMVOE_Ewondo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limbum_Ngiemboon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: dump_path=/home/jupyter/models/africa/cluster3\n",
      "env: exp_name=mlm_tlm_LimbumNgiemboon\n",
      "env: data_path=/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon\n",
      "env: lgs=Limbum-Ngiemboon\n",
      "env: mlm_steps=Limbum,Ngiemboon\n",
      "env: tgt_pair=Limbum-Ngiemboon\n",
      "env: src_path=/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon\n",
      "env: tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon\n"
     ]
    }
   ],
   "source": [
    "%env dump_path=/home/jupyter/models/africa/cluster3\n",
    "%env exp_name=mlm_tlm_LimbumNgiemboon\n",
    "%env data_path=/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon\n",
    "%env lgs=Limbum-Ngiemboon\n",
    "%env mlm_steps=Limbum,Ngiemboon\n",
    "%env tgt_pair=Limbum-Ngiemboon\n",
    "%env src_path=/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon\n",
    "%env tgt_path=/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon\n",
    "\n",
    "# creation of the dummy files so that the experiment does not bug\n",
    "! touch /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/train.Limbum.pth\n",
    "! touch /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/train.Ngiemboon.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Limbum_Ngiemboon vs Ghomala and Bafia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Ghomala to Limbum\n",
      "Bafia to Ngiemboon\n",
      "=====================\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/30/20 15:01:15 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/30/20 15:01:15 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_LimbumNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon' --lgs 'Limbum-Ngiemboon' --clm_steps '' --mlm_steps 'Limbum,Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_LimbumNgiemboon\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Limbum', 1: 'Ngiemboon'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Limbum': 0, 'Ngiemboon': 1}\n",
      "                                     langs: ['Limbum', 'Ngiemboon']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Limbum-Ngiemboon']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Limbum', None), ('Ngiemboon', None)]\n",
      "                                     mono_dataset: {'Limbum': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/train.Limbum.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Limbum.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Limbum.pth'}, 'Ngiemboon': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/train.Ngiemboon.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Ngiemboon.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Ngiemboon.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': False, 'valid': False, 'test': False}\n",
      "                                     remove_long_sentences_test: False\n",
      "                                     remove_long_sentences_train: False\n",
      "                                     remove_long_sentences_valid: False\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/30/20 15:01:15 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml\n",
      "                                     \n",
      "INFO - 05/30/20 15:01:15 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_LimbumNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon' --lgs 'Limbum-Ngiemboon' --clm_steps '' --mlm_steps 'Limbum,Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/30/20 15:01:15 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/30/20 15:01:15 - 0:00:00 - ============ langs: Limbum, Ngiemboon\n",
      "INFO - 05/30/20 15:01:15 - 0:00:00 - ============ Monolingual data (Limbum)\n",
      "INFO - 05/30/20 15:01:15 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Limbum.pth ...\n",
      "INFO - 05/30/20 15:01:15 - 0:00:00 - 251161 words (6667 unique) in 3971 sentences. 79982 unknown words (52 unique) covering 31.84% of the data.\n",
      "INFO - 05/30/20 15:01:15 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/30/20 15:01:15 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Limbum.pth ...\n",
      "INFO - 05/30/20 15:01:15 - 0:00:00 - 249379 words (6667 unique) in 3971 sentences. 79534 unknown words (54 unique) covering 31.89% of the data.\n",
      "INFO - 05/30/20 15:01:15 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/30/20 15:01:15 - 0:00:00 - ============ Monolingual data (Ngiemboon)\n",
      "INFO - 05/30/20 15:01:15 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Ngiemboon.pth ...\n",
      "INFO - 05/30/20 15:01:15 - 0:00:00 - 311407 words (6667 unique) in 3975 sentences. 102253 unknown words (48 unique) covering 32.84% of the data.\n",
      "INFO - 05/30/20 15:01:15 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/30/20 15:01:15 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Ngiemboon.pth ...\n",
      "INFO - 05/30/20 15:01:15 - 0:00:00 - 313817 words (6667 unique) in 3975 sentences. 104021 unknown words (41 unique) covering 33.15% of the data.\n",
      "INFO - 05/30/20 15:01:15 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/30/20 15:01:15 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/30/20 15:01:15 - 0:00:00 - Monolingual data   - valid -       Limbum:      3971\n",
      "INFO - 05/30/20 15:01:15 - 0:00:00 - Monolingual data   -  test -       Limbum:      3971\n",
      "INFO - 05/30/20 15:01:15 - 0:00:00 - Monolingual data   - valid -    Ngiemboon:      3975\n",
      "INFO - 05/30/20 15:01:15 - 0:00:00 - Monolingual data   -  test -    Ngiemboon:      3975\n",
      "\n",
      "INFO - 05/30/20 15:01:16 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(6667, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=6667, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/30/20 15:01:16 - 0:00:01 - Number of parameters (model): 82939403\n",
      "INFO - 05/30/20 15:01:22 - 0:00:06 - Found 0 memories.\n",
      "INFO - 05/30/20 15:01:22 - 0:00:06 - Found 6 FFN.\n",
      "INFO - 05/30/20 15:01:22 - 0:00:06 - Found 102 parameters in model.\n",
      "INFO - 05/30/20 15:01:22 - 0:00:06 - Optimizers: model\n",
      "WARNING - 05/30/20 15:01:22 - 0:00:06 - Reloading checkpoint from /home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml/checkpoint.pth ...\n",
      "WARNING - 05/30/20 15:01:22 - 0:00:07 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/30/20 15:01:22 - 0:00:07 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/30/20 15:01:22 - 0:00:07 - Checkpoint reloaded. Resuming at epoch 60 / iteration 15840 ...\n",
      "INFO - 05/30/20 15:01:25 - 0:00:10 - epoch -> 60.000000\n",
      "INFO - 05/30/20 15:01:25 - 0:00:10 - valid_Limbum_mlm_ppl -> 20831.719003\n",
      "INFO - 05/30/20 15:01:25 - 0:00:10 - valid_Limbum_mlm_acc -> 7.253886\n",
      "INFO - 05/30/20 15:01:25 - 0:00:10 - valid_Ngiemboon_mlm_ppl -> 77542.528616\n",
      "INFO - 05/30/20 15:01:25 - 0:00:10 - valid_Ngiemboon_mlm_acc -> 3.108808\n",
      "INFO - 05/30/20 15:01:25 - 0:00:10 - valid_mlm_ppl -> 49187.123809\n",
      "INFO - 05/30/20 15:01:25 - 0:00:10 - valid_mlm_acc -> 5.181347\n",
      "INFO - 05/30/20 15:01:25 - 0:00:10 - test_Limbum_mlm_ppl -> 46476.915815\n",
      "INFO - 05/30/20 15:01:25 - 0:00:10 - test_Limbum_mlm_acc -> 6.476684\n",
      "INFO - 05/30/20 15:01:25 - 0:00:10 - test_Ngiemboon_mlm_ppl -> 43218.523445\n",
      "INFO - 05/30/20 15:01:25 - 0:00:10 - test_Ngiemboon_mlm_acc -> 8.290155\n",
      "INFO - 05/30/20 15:01:25 - 0:00:10 - test_mlm_ppl -> 44847.719630\n",
      "INFO - 05/30/20 15:01:25 - 0:00:10 - test_mlm_acc -> 7.383420\n",
      "INFO - 05/30/20 15:01:25 - 0:00:10 - __log__:{\"epoch\": 60, \"valid_Limbum_mlm_ppl\": 20831.719002581605, \"valid_Limbum_mlm_acc\": 7.253886010362694, \"valid_Ngiemboon_mlm_ppl\": 77542.52861597764, \"valid_Ngiemboon_mlm_acc\": 3.1088082901554404, \"valid_mlm_ppl\": 49187.12380927963, \"valid_mlm_acc\": 5.181347150259067, \"test_Limbum_mlm_ppl\": 46476.91581527921, \"test_Limbum_mlm_acc\": 6.476683937823834, \"test_Ngiemboon_mlm_ppl\": 43218.523445358376, \"test_Ngiemboon_mlm_acc\": 8.290155440414507, \"test_mlm_ppl\": 44847.719630318796, \"test_mlm_acc\": 7.383419689119171}\n",
      "=====================\n",
      "Limbum to Ghomala\n",
      "Ngiemboon to Bafia\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "! ../copy_rename.sh $src_path $tgt_path Ghomala-Bafia $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : put the rename file on place\n",
    "! ../copy_rename.sh $src_path $tgt_path $tgt_pair Ghomala-Bafia "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Limbum_Ngiemboon vs Bulu and Ewondo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Bulu to Limbum\n",
      "MKPAMAN_AMVOE_Ewondo to Ngiemboon\n",
      "=====================\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm-bis-vm\n",
      "INFO - 05/30/20 15:01:52 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/30/20 15:01:52 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 32\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --eval_only True --exp_name mlm_tlm_LimbumNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon' --lgs 'Limbum-Ngiemboon' --clm_steps '' --mlm_steps 'Limbum,Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: /home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 5000\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: True\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_tlm_LimbumNgiemboon\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'Limbum', 1: 'Ngiemboon'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'Limbum': 0, 'Ngiemboon': 1}\n",
      "                                     langs: ['Limbum', 'Ngiemboon']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['Limbum-Ngiemboon']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 100\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('Limbum', None), ('Ngiemboon', None)]\n",
      "                                     mono_dataset: {'Limbum': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/train.Limbum.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Limbum.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Limbum.pth'}, 'Ngiemboon': {'train': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/train.Ngiemboon.pth', 'valid': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Ngiemboon.pth', 'test': '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Ngiemboon.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     n_task: 1\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': False, 'valid': False, 'test': False}\n",
      "                                     remove_long_sentences_test: False\n",
      "                                     remove_long_sentences_train: False\n",
      "                                     remove_long_sentences_valid: False\n",
      "                                     same_data_path: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/30/20 15:01:52 - 0:00:00 - The experiment will be stored in /home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml\n",
      "                                     \n",
      "INFO - 05/30/20 15:01:52 - 0:00:00 - Running command: python train.py --eval_only True --exp_name mlm_tlm_LimbumNgiemboon --exp_id maml --dump_path '/home/jupyter/models/africa/cluster3' --data_path '/home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon' --lgs 'Limbum-Ngiemboon' --clm_steps '' --mlm_steps 'Limbum,Ngiemboon' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 32 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 5000 --max_epoch 100 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu False --remove_long_sentences_train False --remove_long_sentences_valid False --remove_long_sentences_test False --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1'\n",
      "\n",
      "WARNING - 05/30/20 15:01:52 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/30/20 15:01:52 - 0:00:00 - ============ langs: Limbum, Ngiemboon\n",
      "INFO - 05/30/20 15:01:52 - 0:00:00 - ============ Monolingual data (Limbum)\n",
      "INFO - 05/30/20 15:01:52 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Limbum.pth ...\n",
      "INFO - 05/30/20 15:01:52 - 0:00:00 - 200114 words (6667 unique) in 3973 sentences. 7647 unknown words (52 unique) covering 3.82% of the data.\n",
      "INFO - 05/30/20 15:01:52 - 0:00:00 - ========================== debug : 1\n",
      "\n",
      "INFO - 05/30/20 15:01:52 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Limbum.pth ...\n",
      "INFO - 05/30/20 15:01:52 - 0:00:00 - 199901 words (6667 unique) in 3973 sentences. 7721 unknown words (58 unique) covering 3.86% of the data.\n",
      "INFO - 05/30/20 15:01:52 - 0:00:00 - ========================== debug : 2\n",
      "\n",
      "INFO - 05/30/20 15:01:52 - 0:00:00 - ============ Monolingual data (Ngiemboon)\n",
      "INFO - 05/30/20 15:01:52 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/valid.Ngiemboon.pth ...\n",
      "INFO - 05/30/20 15:01:52 - 0:00:00 - 191892 words (6667 unique) in 3972 sentences. 1732 unknown words (49 unique) covering 0.90% of the data.\n",
      "INFO - 05/30/20 15:01:52 - 0:00:00 - ========================== debug : 3\n",
      "\n",
      "INFO - 05/30/20 15:01:52 - 0:00:00 - Loading data from /home/jupyter/models/africa/evaluation_hypothesis/Limbum_Ngiemboon/test.Ngiemboon.pth ...\n",
      "INFO - 05/30/20 15:01:52 - 0:00:00 - 192496 words (6667 unique) in 3972 sentences. 1642 unknown words (46 unique) covering 0.85% of the data.\n",
      "INFO - 05/30/20 15:01:52 - 0:00:00 - ========================== debug : 4\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/30/20 15:01:52 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/30/20 15:01:52 - 0:00:00 - Monolingual data   - valid -       Limbum:      3973\n",
      "INFO - 05/30/20 15:01:52 - 0:00:00 - Monolingual data   -  test -       Limbum:      3973\n",
      "INFO - 05/30/20 15:01:52 - 0:00:00 - Monolingual data   - valid -    Ngiemboon:      3972\n",
      "INFO - 05/30/20 15:01:52 - 0:00:00 - Monolingual data   -  test -    Ngiemboon:      3972\n",
      "\n",
      "INFO - 05/30/20 15:01:53 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(6667, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=6667, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/30/20 15:01:53 - 0:00:01 - Number of parameters (model): 82939403\n",
      "INFO - 05/30/20 15:01:59 - 0:00:07 - Found 0 memories.\n",
      "INFO - 05/30/20 15:01:59 - 0:00:07 - Found 6 FFN.\n",
      "INFO - 05/30/20 15:01:59 - 0:00:07 - Found 102 parameters in model.\n",
      "INFO - 05/30/20 15:01:59 - 0:00:07 - Optimizers: model\n",
      "WARNING - 05/30/20 15:01:59 - 0:00:07 - Reloading checkpoint from /home/jupyter/models/africa/cluster3/mlm_tlm_LimbumNgiemboon/maml/checkpoint.pth ...\n",
      "WARNING - 05/30/20 15:02:00 - 0:00:07 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/30/20 15:02:00 - 0:00:07 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/30/20 15:02:00 - 0:00:07 - Checkpoint reloaded. Resuming at epoch 60 / iteration 15840 ...\n",
      "INFO - 05/30/20 15:02:02 - 0:00:10 - epoch -> 60.000000\n",
      "INFO - 05/30/20 15:02:02 - 0:00:10 - valid_Limbum_mlm_ppl -> 6876.349871\n",
      "INFO - 05/30/20 15:02:02 - 0:00:10 - valid_Limbum_mlm_acc -> 11.658031\n",
      "INFO - 05/30/20 15:02:02 - 0:00:10 - valid_Ngiemboon_mlm_ppl -> 3263.703318\n",
      "INFO - 05/30/20 15:02:02 - 0:00:10 - valid_Ngiemboon_mlm_acc -> 12.435233\n",
      "INFO - 05/30/20 15:02:02 - 0:00:10 - valid_mlm_ppl -> 5070.026594\n",
      "INFO - 05/30/20 15:02:02 - 0:00:10 - valid_mlm_acc -> 12.046632\n",
      "INFO - 05/30/20 15:02:02 - 0:00:10 - test_Limbum_mlm_ppl -> 9514.259468\n",
      "INFO - 05/30/20 15:02:02 - 0:00:10 - test_Limbum_mlm_acc -> 12.953368\n",
      "INFO - 05/30/20 15:02:02 - 0:00:10 - test_Ngiemboon_mlm_ppl -> 4330.209592\n",
      "INFO - 05/30/20 15:02:02 - 0:00:10 - test_Ngiemboon_mlm_acc -> 10.362694\n",
      "INFO - 05/30/20 15:02:02 - 0:00:10 - test_mlm_ppl -> 6922.234530\n",
      "INFO - 05/30/20 15:02:02 - 0:00:10 - test_mlm_acc -> 11.658031\n",
      "INFO - 05/30/20 15:02:02 - 0:00:10 - __log__:{\"epoch\": 60, \"valid_Limbum_mlm_ppl\": 6876.3498705630955, \"valid_Limbum_mlm_acc\": 11.658031088082902, \"valid_Ngiemboon_mlm_ppl\": 3263.703317839215, \"valid_Ngiemboon_mlm_acc\": 12.435233160621761, \"valid_mlm_ppl\": 5070.026594201156, \"valid_mlm_acc\": 12.046632124352332, \"test_Limbum_mlm_ppl\": 9514.259468208895, \"test_Limbum_mlm_acc\": 12.953367875647668, \"test_Ngiemboon_mlm_ppl\": 4330.209591503126, \"test_Ngiemboon_mlm_acc\": 10.362694300518134, \"test_mlm_ppl\": 6922.23452985601, \"test_mlm_acc\": 11.6580310880829}\n",
      "=====================\n",
      "Limbum to Bulu\n",
      "Ngiemboon to MKPAMAN_AMVOE_Ewondo\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "! ../copy_rename.sh $src_path $tgt_path Bulu-MKPAMAN_AMVOE_Ewondo $tgt_pair\n",
    "! ../evaluate.sh\n",
    "#because the same dir : put the rename file on place\n",
    "! ../copy_rename.sh $src_path $tgt_path $tgt_pair Bulu-MKPAMAN_AMVOE_Ewondo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
