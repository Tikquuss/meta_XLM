{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data : Monolingual data (MLM) and/or Parallel data (TLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(bookmark:HOME) -> /home/jupyter/meta_XLM/XLM\n",
      "/home/jupyter/meta_XLM/XLM\n"
     ]
    }
   ],
   "source": [
    "%bookmark HOME \"/home/jupyter/meta_XLM/XLM\" \n",
    "%cd -b HOME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PARA=True\n",
      "env: MONO=True\n",
      "env: PARA_PATH=data/enfrde/para\n",
      "env: MONO_PATH=data/enfrde/mono\n",
      "env: SAME_VOCAB=True\n",
      "env: nCodes=10\n",
      "env: shuf_n_samples=1\n",
      "env: threads_for_tokenizer=16\n",
      "env: test_size=10\n",
      "env: val_size=10\n",
      "env: TOKENIZE=tools/tokenizer_our.sh\n",
      "env: LOWER_REMOVE_ACCENT=tools/lowercase_and_remove_accent.py\n",
      "env: FASTBPE=tools/fastBPE/fast\n",
      "env: OUTPATH=data/enfrde/processed\n",
      "env: n_samples=-1\n",
      "env: sub_task=en-fr:10,en-de:-1,de-fr:-1\n"
     ]
    }
   ],
   "source": [
    "%env PARA=True          \n",
    "%env MONO=True           \n",
    "                   \n",
    "%env PARA_PATH=data/enfrde/para      \n",
    "%env MONO_PATH=data/enfrde/mono    \n",
    "%env SAME_VOCAB=True    \n",
    "%env nCodes=10\n",
    "%env shuf_n_samples=1\n",
    "%env threads_for_tokenizer=16\n",
    "%env test_size=10       \n",
    "%env val_size=10        \n",
    "\n",
    "# tools paths\n",
    "%env TOKENIZE=tools/tokenizer_our.sh\n",
    "%env LOWER_REMOVE_ACCENT=tools/lowercase_and_remove_accent.py\n",
    "%env FASTBPE=tools/fastBPE/fast\n",
    "\n",
    "\n",
    "%env OUTPATH=data/enfrde/processed \n",
    "# create output path\n",
    "! mkdir -p $OUTPATH\n",
    "\n",
    "! chmod +x $FASTBPE\n",
    "! chmod +x ../build_meta_data.sh\n",
    "! chmod +x tools/mosesdecoder/scripts/tokenizer/*.perl\n",
    "\n",
    "%env n_samples=-1\n",
    "\n",
    "%env sub_task=en-fr:10,en-de:-1,de-fr:-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir data/enfrde/processed/fine_tune already exists\n",
      "params ok !\n",
      "*** Cleaning and tokenizing en-fr data ... ***\n",
      "file data/enfrde/para/en-fr.en.all already exists\n",
      "file data/enfrde/para/en-fr.fr.all already exists\n",
      "*** Cleaning and tokenizing en-de data ... ***\n",
      "file data/enfrde/para/en-de.en.all already exists\n",
      "file data/enfrde/para/en-de.de.all already exists\n",
      "*** Cleaning and tokenizing de-fr data ... ***\n",
      "file data/enfrde/para/de-fr.de.all already exists\n",
      "file data/enfrde/para/de-fr.fr.all already exists\n",
      "file data/enfrde/mono/en.all already exists\n",
      "file data/enfrde/mono/fr.all already exists\n",
      "file data/enfrde/mono/en.all already exists\n",
      "file data/enfrde/mono/de.all already exists\n",
      "file data/enfrde/mono/de.all already exists\n",
      "file data/enfrde/mono/fr.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "\n",
      "\n",
      "\n",
      "***build the training set for BPE tokenization (10 codes)***\n",
      "\n",
      "\n",
      "***shuf ... Generating 1 random permutations of train data and store result in data/enfrde/processed/de-fr/bpe.train***\n",
      "file data/enfrde/processed/bpe.train already exists\n",
      "\n",
      "\n",
      "***Learn the BPE vocabulary on the training set : data/enfrde/processed/bpe.train ...***\n",
      "file data/enfrde/processed/codes already exists\n",
      "***Learn 10 BPE code on the bpe.train file***\n",
      "\n",
      "\n",
      "***Get the post-BPE vocab***\n",
      "file data/enfrde/processed/train already exists\n",
      "file data/enfrde/processed/vocab already exists\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the parallel corpora.***\n",
      "Loading codes from data/enfrde/processed/codes ...\n",
      "Read 0 codes from the codes file.\n",
      "Loading vocabulary from data/enfrde/para/en-fr.en.train ...\n",
      "Read 111998 words (10147 unique) from text file.\n",
      "Applying BPE to data/enfrde/para/en-fr.en.train ...\n",
      "Modified 111998 words from text file.\n",
      "Loading codes from data/enfrde/processed/codes ...\n",
      "Read 0 codes from the codes file.\n",
      "Loading vocabulary from data/enfrde/para/en-fr.en.valid ...\n",
      "Read 13526 words (3243 unique) from text file.\n",
      "Applying BPE to data/enfrde/para/en-fr.en.valid ...\n",
      "Modified 13526 words from text file.\n",
      "Loading codes from data/enfrde/processed/codes ...\n",
      "Read 0 codes from the codes file.\n",
      "Loading vocabulary from data/enfrde/para/en-fr.en.test ...\n",
      "Read 14027 words (3244 unique) from text file.\n",
      "Applying BPE to data/enfrde/para/en-fr.en.test ...\n",
      "Modified 14027 words from text file.\n",
      "Loading codes from data/enfrde/processed/codes ...\n",
      "Read 0 codes from the codes file.\n",
      "Loading vocabulary from data/enfrde/para/en-fr.fr.train ...\n",
      "Read 139524 words (11621 unique) from text file.\n",
      "Applying BPE to data/enfrde/para/en-fr.fr.train ...\n",
      "Modified 139524 words from text file.\n",
      "Loading codes from data/enfrde/processed/codes ...\n",
      "Read 0 codes from the codes file.\n",
      "Loading vocabulary from data/enfrde/para/en-fr.fr.valid ...\n",
      "Read 16291 words (3599 unique) from text file.\n",
      "Applying BPE to data/enfrde/para/en-fr.fr.valid ...\n",
      "Modified 16291 words from text file.\n",
      "Loading codes from data/enfrde/processed/codes ...\n",
      "Read 0 codes from the codes file.\n",
      "Loading vocabulary from data/enfrde/para/en-fr.fr.test ...\n",
      "Read 16858 words (3525 unique) from text file.\n",
      "Applying BPE to data/enfrde/para/en-fr.fr.test ...\n",
      "Modified 16858 words from text file.\n",
      "Loading codes from data/enfrde/processed/codes ...\n",
      "Read 0 codes from the codes file.\n",
      "Loading vocabulary from data/enfrde/para/en-de.en.train ...\n",
      "Read 111998 words (10147 unique) from text file.\n",
      "Applying BPE to data/enfrde/para/en-de.en.train ...\n",
      "Modified 111998 words from text file.\n",
      "Loading codes from data/enfrde/processed/codes ...\n",
      "Read 0 codes from the codes file.\n",
      "Loading vocabulary from data/enfrde/para/en-de.en.valid ...\n",
      "Read 13526 words (3243 unique) from text file.\n",
      "Applying BPE to data/enfrde/para/en-de.en.valid ...\n",
      "Modified 13526 words from text file.\n",
      "Loading codes from data/enfrde/processed/codes ...\n",
      "Read 0 codes from the codes file.\n",
      "Loading vocabulary from data/enfrde/para/en-de.en.test ...\n",
      "Read 14027 words (3244 unique) from text file.\n",
      "Applying BPE to data/enfrde/para/en-de.en.test ...\n",
      "Modified 14027 words from text file.\n",
      "Loading codes from data/enfrde/processed/codes ...\n",
      "Read 0 codes from the codes file.\n",
      "Loading vocabulary from data/enfrde/para/en-de.de.train ...\n",
      "Read 139547 words (11620 unique) from text file.\n",
      "Applying BPE to data/enfrde/para/en-de.de.train ...\n",
      "Modified 139547 words from text file.\n",
      "Loading codes from data/enfrde/processed/codes ...\n",
      "Read 0 codes from the codes file.\n",
      "Loading vocabulary from data/enfrde/para/en-de.de.valid ...\n",
      "Read 16299 words (3598 unique) from text file.\n",
      "Applying BPE to data/enfrde/para/en-de.de.valid ...\n",
      "Modified 16299 words from text file.\n",
      "Loading codes from data/enfrde/processed/codes ...\n",
      "Read 0 codes from the codes file.\n",
      "Loading vocabulary from data/enfrde/para/en-de.de.test ...\n",
      "Read 16859 words (3524 unique) from text file.\n",
      "Applying BPE to data/enfrde/para/en-de.de.test ...\n",
      "Modified 16859 words from text file.\n",
      "Loading codes from data/enfrde/processed/codes ...\n",
      "Read 0 codes from the codes file.\n",
      "Loading vocabulary from data/enfrde/para/de-fr.de.train ...\n",
      "Read 139547 words (11620 unique) from text file.\n",
      "Applying BPE to data/enfrde/para/de-fr.de.train ...\n",
      "Modified 139547 words from text file.\n",
      "Loading codes from data/enfrde/processed/codes ...\n",
      "Read 0 codes from the codes file.\n",
      "Loading vocabulary from data/enfrde/para/de-fr.de.valid ...\n",
      "Read 16299 words (3598 unique) from text file.\n",
      "Applying BPE to data/enfrde/para/de-fr.de.valid ...\n",
      "Modified 16299 words from text file.\n",
      "Loading codes from data/enfrde/processed/codes ...\n",
      "Read 0 codes from the codes file.\n",
      "Loading vocabulary from data/enfrde/para/de-fr.de.test ...\n",
      "Read 16859 words (3524 unique) from text file.\n",
      "Applying BPE to data/enfrde/para/de-fr.de.test ...\n",
      "Modified 16859 words from text file.\n",
      "Loading codes from data/enfrde/processed/codes ...\n",
      "Read 0 codes from the codes file.\n",
      "Loading vocabulary from data/enfrde/para/de-fr.fr.train ...\n",
      "Read 139524 words (11621 unique) from text file.\n",
      "Applying BPE to data/enfrde/para/de-fr.fr.train ...\n",
      "Modified 139524 words from text file.\n",
      "Loading codes from data/enfrde/processed/codes ...\n",
      "Read 0 codes from the codes file.\n",
      "Loading vocabulary from data/enfrde/para/de-fr.fr.valid ...\n",
      "Read 16291 words (3599 unique) from text file.\n",
      "Applying BPE to data/enfrde/para/de-fr.fr.valid ...\n",
      "Modified 16291 words from text file.\n",
      "Loading codes from data/enfrde/processed/codes ...\n",
      "Read 0 codes from the codes file.\n",
      "Loading vocabulary from data/enfrde/para/de-fr.fr.test ...\n",
      "Read 16858 words (3525 unique) from text file.\n",
      "Applying BPE to data/enfrde/para/de-fr.fr.test ...\n",
      "Modified 16858 words from text file.\n",
      "Loading codes from data/enfrde/processed/codes ...\n",
      "Read 0 codes from the codes file.\n",
      "Loading vocabulary from data/enfrde/mono/en.train ...\n",
      "Read 111998 words (10147 unique) from text file.\n",
      "Applying BPE to data/enfrde/mono/en.train ...\n",
      "Modified 111998 words from text file.\n",
      "Loading codes from data/enfrde/processed/codes ...\n",
      "Read 0 codes from the codes file.\n",
      "Loading vocabulary from data/enfrde/mono/en.valid ...\n",
      "Read 13526 words (3243 unique) from text file.\n",
      "Applying BPE to data/enfrde/mono/en.valid ...\n",
      "Modified 13526 words from text file.\n",
      "Loading codes from data/enfrde/processed/codes ...\n",
      "Read 0 codes from the codes file.\n",
      "Loading vocabulary from data/enfrde/mono/en.test ...\n",
      "Read 14027 words (3244 unique) from text file.\n",
      "Applying BPE to data/enfrde/mono/en.test ...\n",
      "Modified 14027 words from text file.\n",
      "Loading codes from data/enfrde/processed/codes ...\n",
      "Read 0 codes from the codes file.\n",
      "Loading vocabulary from data/enfrde/mono/fr.train ...\n",
      "Read 139524 words (11621 unique) from text file.\n",
      "Applying BPE to data/enfrde/mono/fr.train ...\n",
      "Modified 139524 words from text file.\n",
      "Loading codes from data/enfrde/processed/codes ...\n",
      "Read 0 codes from the codes file.\n",
      "Loading vocabulary from data/enfrde/mono/fr.valid ...\n",
      "Read 16291 words (3599 unique) from text file.\n",
      "Applying BPE to data/enfrde/mono/fr.valid ...\n",
      "Modified 16291 words from text file.\n",
      "Loading codes from data/enfrde/processed/codes ...\n",
      "Read 0 codes from the codes file.\n",
      "Loading vocabulary from data/enfrde/mono/fr.test ...\n",
      "Read 16858 words (3525 unique) from text file.\n",
      "Applying BPE to data/enfrde/mono/fr.test ...\n",
      "Modified 16858 words from text file.\n",
      "Loading codes from data/enfrde/processed/codes ...\n",
      "Read 0 codes from the codes file.\n",
      "Loading vocabulary from data/enfrde/mono/en.train ...\n",
      "Read 111998 words (10147 unique) from text file.\n",
      "Applying BPE to data/enfrde/mono/en.train ...\n",
      "Modified 111998 words from text file.\n",
      "Loading codes from data/enfrde/processed/codes ...\n",
      "Read 0 codes from the codes file.\n",
      "Loading vocabulary from data/enfrde/mono/en.valid ...\n",
      "Read 13526 words (3243 unique) from text file.\n",
      "Applying BPE to data/enfrde/mono/en.valid ...\n",
      "Modified 13526 words from text file.\n",
      "Loading codes from data/enfrde/processed/codes ...\n",
      "Read 0 codes from the codes file.\n",
      "Loading vocabulary from data/enfrde/mono/en.test ...\n",
      "Read 14027 words (3244 unique) from text file.\n",
      "Applying BPE to data/enfrde/mono/en.test ...\n",
      "Modified 14027 words from text file.\n",
      "Loading codes from data/enfrde/processed/codes ...\n",
      "Read 0 codes from the codes file.\n",
      "Loading vocabulary from data/enfrde/mono/de.train ...\n",
      "Read 139547 words (11620 unique) from text file.\n",
      "Applying BPE to data/enfrde/mono/de.train ...\n",
      "Modified 139547 words from text file.\n",
      "Loading codes from data/enfrde/processed/codes ...\n",
      "Read 0 codes from the codes file.\n",
      "Loading vocabulary from data/enfrde/mono/de.valid ...\n",
      "Read 16299 words (3598 unique) from text file.\n",
      "Applying BPE to data/enfrde/mono/de.valid ...\n",
      "Modified 16299 words from text file.\n",
      "Loading codes from data/enfrde/processed/codes ...\n",
      "Read 0 codes from the codes file.\n",
      "Loading vocabulary from data/enfrde/mono/de.test ...\n",
      "Read 16859 words (3524 unique) from text file.\n",
      "Applying BPE to data/enfrde/mono/de.test ...\n",
      "Modified 16859 words from text file.\n",
      "Loading codes from data/enfrde/processed/codes ...\n",
      "Read 0 codes from the codes file.\n",
      "Loading vocabulary from data/enfrde/mono/de.train ...\n",
      "Read 139547 words (11620 unique) from text file.\n",
      "Applying BPE to data/enfrde/mono/de.train ...\n",
      "Modified 139547 words from text file.\n",
      "Loading codes from data/enfrde/processed/codes ...\n",
      "Read 0 codes from the codes file.\n",
      "Loading vocabulary from data/enfrde/mono/de.valid ...\n",
      "Read 16299 words (3598 unique) from text file.\n",
      "Applying BPE to data/enfrde/mono/de.valid ...\n",
      "Modified 16299 words from text file.\n",
      "Loading codes from data/enfrde/processed/codes ...\n",
      "Read 0 codes from the codes file.\n",
      "Loading vocabulary from data/enfrde/mono/de.test ...\n",
      "Read 16859 words (3524 unique) from text file.\n",
      "Applying BPE to data/enfrde/mono/de.test ...\n",
      "Modified 16859 words from text file.\n",
      "Loading codes from data/enfrde/processed/codes ...\n",
      "Read 0 codes from the codes file.\n",
      "Loading vocabulary from data/enfrde/mono/fr.train ...\n",
      "Read 139524 words (11621 unique) from text file.\n",
      "Applying BPE to data/enfrde/mono/fr.train ...\n",
      "Modified 139524 words from text file.\n",
      "Loading codes from data/enfrde/processed/codes ...\n",
      "Read 0 codes from the codes file.\n",
      "Loading vocabulary from data/enfrde/mono/fr.valid ...\n",
      "Read 16291 words (3599 unique) from text file.\n",
      "Applying BPE to data/enfrde/mono/fr.valid ...\n",
      "Modified 16291 words from text file.\n",
      "Loading codes from data/enfrde/processed/codes ...\n",
      "Read 0 codes from the codes file.\n",
      "Loading vocabulary from data/enfrde/mono/fr.test ...\n",
      "Read 16858 words (3525 unique) from text file.\n",
      "Applying BPE to data/enfrde/mono/fr.test ...\n",
      "Modified 16858 words from text file.\n",
      "\n",
      "\n",
      "***Build fine_tune data***\n",
      "data/enfrde/processed/en-fr.en.train\n",
      "../build_meta_data.sh: line 315: 4001 data/enfrde/processed/en-fr.en.train+1: syntax error in expression (error token is \"data/enfrde/processed/en-fr.en.train+1\")\n",
      "\n",
      "\n",
      "***Binarize everything using preprocess.py.***\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Loading data from data/enfrde/processed/en-fr.en.train.pth ...\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - 544682 words (102 unique) in 4001 sentences.\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - 295 unknown words (28 unique), covering 0.05% of the data.\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - v: 89\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - $: 81\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - ?: 19\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - =: 14\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - _: 13\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - {: 10\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - j: 9\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - }: 9\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - @: 8\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - ®: 7\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - &: 5\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - >: 3\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - <: 3\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - −: 3\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - +: 3\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - μ@@: 2\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - #: 2\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - £: 2\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - ≤: 2\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - ·: 2\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - €: 2\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - ×: 1\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - º@@: 1\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - ≥: 1\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - §: 1\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - ∗: 1\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - '@@: 1\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - ±: 1\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Loading data from data/enfrde/processed/en-fr.en.valid.pth ...\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - 66317 words (102 unique) in 500 sentences.\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - 27 unknown words (12 unique), covering 0.04% of the data.\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - $: 9\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - v: 5\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - ?: 3\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - _: 2\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - >: 1\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - <: 1\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - ∗: 1\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - j: 1\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - ø@@: 1\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - €: 1\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - °: 1\n",
      "INFO - 05/14/20 01:46:15 - 0:00:00 - +: 1\n",
      "INFO - 05/14/20 01:46:16 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Loading data from data/enfrde/processed/en-fr.en.test.pth ...\n",
      "INFO - 05/14/20 01:46:16 - 0:00:00 - 67592 words (102 unique) in 500 sentences.\n",
      "INFO - 05/14/20 01:46:16 - 0:00:00 - 29 unknown words (7 unique), covering 0.04% of the data.\n",
      "INFO - 05/14/20 01:46:16 - 0:00:00 - v: 11\n",
      "INFO - 05/14/20 01:46:16 - 0:00:00 - $: 10\n",
      "INFO - 05/14/20 01:46:16 - 0:00:00 - <: 2\n",
      "INFO - 05/14/20 01:46:16 - 0:00:00 - @: 2\n",
      "INFO - 05/14/20 01:46:16 - 0:00:00 - =: 2\n",
      "INFO - 05/14/20 01:46:16 - 0:00:00 - γ: 1\n",
      "INFO - 05/14/20 01:46:16 - 0:00:00 - >: 1\n",
      "INFO - 05/14/20 01:46:17 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Loading data from data/enfrde/processed/en-fr.fr.train.pth ...\n",
      "INFO - 05/14/20 01:46:17 - 0:00:00 - 627614 words (102 unique) in 4001 sentences.\n",
      "INFO - 05/14/20 01:46:17 - 0:00:00 - 301 unknown words (34 unique), covering 0.05% of the data.\n",
      "INFO - 05/14/20 01:46:17 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Loading data from data/enfrde/processed/en-fr.fr.valid.pth ...\n",
      "INFO - 05/14/20 01:46:17 - 0:00:00 - 74507 words (102 unique) in 500 sentences.\n",
      "INFO - 05/14/20 01:46:17 - 0:00:00 - 30 unknown words (11 unique), covering 0.04% of the data.\n",
      "INFO - 05/14/20 01:46:17 - 0:00:00 - œ@@: 13\n",
      "INFO - 05/14/20 01:46:17 - 0:00:00 - −: 4\n",
      "INFO - 05/14/20 01:46:17 - 0:00:00 - ?: 3\n",
      "INFO - 05/14/20 01:46:17 - 0:00:00 - _: 2\n",
      "INFO - 05/14/20 01:46:17 - 0:00:00 - v: 2\n",
      "INFO - 05/14/20 01:46:17 - 0:00:00 - >: 1\n",
      "INFO - 05/14/20 01:46:17 - 0:00:00 - <: 1\n",
      "INFO - 05/14/20 01:46:17 - 0:00:00 - j: 1\n",
      "INFO - 05/14/20 01:46:17 - 0:00:00 - ø@@: 1\n",
      "INFO - 05/14/20 01:46:17 - 0:00:00 - °: 1\n",
      "INFO - 05/14/20 01:46:17 - 0:00:00 - +: 1\n",
      "INFO - 05/14/20 01:46:18 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Loading data from data/enfrde/processed/en-fr.fr.test.pth ...\n",
      "INFO - 05/14/20 01:46:18 - 0:00:00 - 75485 words (102 unique) in 500 sentences.\n",
      "INFO - 05/14/20 01:46:18 - 0:00:00 - 31 unknown words (10 unique), covering 0.04% of the data.\n",
      "INFO - 05/14/20 01:46:18 - 0:00:00 - œ@@: 11\n",
      "INFO - 05/14/20 01:46:18 - 0:00:00 - v: 7\n",
      "INFO - 05/14/20 01:46:18 - 0:00:00 - j: 3\n",
      "INFO - 05/14/20 01:46:18 - 0:00:00 - °: 2\n",
      "INFO - 05/14/20 01:46:18 - 0:00:00 - @: 2\n",
      "INFO - 05/14/20 01:46:18 - 0:00:00 - −: 2\n",
      "INFO - 05/14/20 01:46:18 - 0:00:00 - γ: 1\n",
      "INFO - 05/14/20 01:46:18 - 0:00:00 - >: 1\n",
      "INFO - 05/14/20 01:46:18 - 0:00:00 - <: 1\n",
      "INFO - 05/14/20 01:46:18 - 0:00:00 - ?: 1\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Loading data from data/enfrde/processed/en-de.en.train.pth ...\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - 544682 words (102 unique) in 4001 sentences.\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - 295 unknown words (28 unique), covering 0.05% of the data.\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - v: 89\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - $: 81\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - ?: 19\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - =: 14\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - _: 13\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - {: 10\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - j: 9\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - }: 9\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - @: 8\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - ®: 7\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - &: 5\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - >: 3\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - <: 3\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - −: 3\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - +: 3\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - μ@@: 2\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - #: 2\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - £: 2\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - ≤: 2\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - ·: 2\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - €: 2\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - ×: 1\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - º@@: 1\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - ≥: 1\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - §: 1\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - ∗: 1\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - '@@: 1\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - ±: 1\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Loading data from data/enfrde/processed/en-de.en.valid.pth ...\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - 66317 words (102 unique) in 500 sentences.\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - 27 unknown words (12 unique), covering 0.04% of the data.\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - $: 9\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - v: 5\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - ?: 3\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - _: 2\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - >: 1\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - <: 1\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - ∗: 1\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - j: 1\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - ø@@: 1\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - €: 1\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - °: 1\n",
      "INFO - 05/14/20 01:46:19 - 0:00:00 - +: 1\n",
      "INFO - 05/14/20 01:46:20 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Loading data from data/enfrde/processed/en-de.en.test.pth ...\n",
      "INFO - 05/14/20 01:46:20 - 0:00:00 - 67592 words (102 unique) in 500 sentences.\n",
      "INFO - 05/14/20 01:46:20 - 0:00:00 - 29 unknown words (7 unique), covering 0.04% of the data.\n",
      "INFO - 05/14/20 01:46:20 - 0:00:00 - v: 11\n",
      "INFO - 05/14/20 01:46:20 - 0:00:00 - $: 10\n",
      "INFO - 05/14/20 01:46:20 - 0:00:00 - <: 2\n",
      "INFO - 05/14/20 01:46:20 - 0:00:00 - @: 2\n",
      "INFO - 05/14/20 01:46:20 - 0:00:00 - =: 2\n",
      "INFO - 05/14/20 01:46:20 - 0:00:00 - γ: 1\n",
      "INFO - 05/14/20 01:46:20 - 0:00:00 - >: 1\n",
      "INFO - 05/14/20 01:46:21 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Loading data from data/enfrde/processed/en-de.de.train.pth ...\n",
      "INFO - 05/14/20 01:46:21 - 0:00:00 - 627614 words (102 unique) in 4001 sentences.\n",
      "INFO - 05/14/20 01:46:21 - 0:00:00 - 301 unknown words (34 unique), covering 0.05% of the data.\n",
      "INFO - 05/14/20 01:46:21 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Loading data from data/enfrde/processed/en-de.de.valid.pth ...\n",
      "INFO - 05/14/20 01:46:21 - 0:00:00 - 74507 words (102 unique) in 500 sentences.\n",
      "INFO - 05/14/20 01:46:21 - 0:00:00 - 30 unknown words (11 unique), covering 0.04% of the data.\n",
      "INFO - 05/14/20 01:46:21 - 0:00:00 - œ@@: 13\n",
      "INFO - 05/14/20 01:46:21 - 0:00:00 - −: 4\n",
      "INFO - 05/14/20 01:46:21 - 0:00:00 - ?: 3\n",
      "INFO - 05/14/20 01:46:21 - 0:00:00 - _: 2\n",
      "INFO - 05/14/20 01:46:21 - 0:00:00 - v: 2\n",
      "INFO - 05/14/20 01:46:21 - 0:00:00 - >: 1\n",
      "INFO - 05/14/20 01:46:21 - 0:00:00 - <: 1\n",
      "INFO - 05/14/20 01:46:21 - 0:00:00 - j: 1\n",
      "INFO - 05/14/20 01:46:21 - 0:00:00 - ø@@: 1\n",
      "INFO - 05/14/20 01:46:21 - 0:00:00 - °: 1\n",
      "INFO - 05/14/20 01:46:21 - 0:00:00 - +: 1\n",
      "INFO - 05/14/20 01:46:22 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Loading data from data/enfrde/processed/en-de.de.test.pth ...\n",
      "INFO - 05/14/20 01:46:22 - 0:00:00 - 75485 words (102 unique) in 500 sentences.\n",
      "INFO - 05/14/20 01:46:22 - 0:00:00 - 31 unknown words (10 unique), covering 0.04% of the data.\n",
      "INFO - 05/14/20 01:46:22 - 0:00:00 - œ@@: 11\n",
      "INFO - 05/14/20 01:46:22 - 0:00:00 - v: 7\n",
      "INFO - 05/14/20 01:46:22 - 0:00:00 - j: 3\n",
      "INFO - 05/14/20 01:46:22 - 0:00:00 - °: 2\n",
      "INFO - 05/14/20 01:46:22 - 0:00:00 - @: 2\n",
      "INFO - 05/14/20 01:46:22 - 0:00:00 - −: 2\n",
      "INFO - 05/14/20 01:46:22 - 0:00:00 - γ: 1\n",
      "INFO - 05/14/20 01:46:22 - 0:00:00 - >: 1\n",
      "INFO - 05/14/20 01:46:22 - 0:00:00 - <: 1\n",
      "INFO - 05/14/20 01:46:22 - 0:00:00 - ?: 1\n",
      "INFO - 05/14/20 01:46:23 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Loading data from data/enfrde/processed/de-fr.de.train.pth ...\n",
      "INFO - 05/14/20 01:46:23 - 0:00:00 - 627614 words (102 unique) in 4001 sentences.\n",
      "INFO - 05/14/20 01:46:23 - 0:00:00 - 301 unknown words (34 unique), covering 0.05% of the data.\n",
      "INFO - 05/14/20 01:46:23 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Loading data from data/enfrde/processed/de-fr.de.valid.pth ...\n",
      "INFO - 05/14/20 01:46:23 - 0:00:00 - 74507 words (102 unique) in 500 sentences.\n",
      "INFO - 05/14/20 01:46:23 - 0:00:00 - 30 unknown words (11 unique), covering 0.04% of the data.\n",
      "INFO - 05/14/20 01:46:23 - 0:00:00 - œ@@: 13\n",
      "INFO - 05/14/20 01:46:23 - 0:00:00 - −: 4\n",
      "INFO - 05/14/20 01:46:23 - 0:00:00 - ?: 3\n",
      "INFO - 05/14/20 01:46:23 - 0:00:00 - _: 2\n",
      "INFO - 05/14/20 01:46:23 - 0:00:00 - v: 2\n",
      "INFO - 05/14/20 01:46:23 - 0:00:00 - >: 1\n",
      "INFO - 05/14/20 01:46:23 - 0:00:00 - <: 1\n",
      "INFO - 05/14/20 01:46:23 - 0:00:00 - j: 1\n",
      "INFO - 05/14/20 01:46:23 - 0:00:00 - ø@@: 1\n",
      "INFO - 05/14/20 01:46:23 - 0:00:00 - °: 1\n",
      "INFO - 05/14/20 01:46:23 - 0:00:00 - +: 1\n",
      "INFO - 05/14/20 01:46:24 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Loading data from data/enfrde/processed/de-fr.de.test.pth ...\n",
      "INFO - 05/14/20 01:46:24 - 0:00:00 - 75485 words (102 unique) in 500 sentences.\n",
      "INFO - 05/14/20 01:46:24 - 0:00:00 - 31 unknown words (10 unique), covering 0.04% of the data.\n",
      "INFO - 05/14/20 01:46:24 - 0:00:00 - œ@@: 11\n",
      "INFO - 05/14/20 01:46:24 - 0:00:00 - v: 7\n",
      "INFO - 05/14/20 01:46:24 - 0:00:00 - j: 3\n",
      "INFO - 05/14/20 01:46:24 - 0:00:00 - °: 2\n",
      "INFO - 05/14/20 01:46:24 - 0:00:00 - @: 2\n",
      "INFO - 05/14/20 01:46:24 - 0:00:00 - −: 2\n",
      "INFO - 05/14/20 01:46:24 - 0:00:00 - γ: 1\n",
      "INFO - 05/14/20 01:46:24 - 0:00:00 - >: 1\n",
      "INFO - 05/14/20 01:46:24 - 0:00:00 - <: 1\n",
      "INFO - 05/14/20 01:46:24 - 0:00:00 - ?: 1\n",
      "INFO - 05/14/20 01:46:25 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Loading data from data/enfrde/processed/de-fr.fr.train.pth ...\n",
      "INFO - 05/14/20 01:46:25 - 0:00:00 - 627614 words (102 unique) in 4001 sentences.\n",
      "INFO - 05/14/20 01:46:25 - 0:00:00 - 301 unknown words (34 unique), covering 0.05% of the data.\n",
      "INFO - 05/14/20 01:46:25 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Loading data from data/enfrde/processed/de-fr.fr.valid.pth ...\n",
      "INFO - 05/14/20 01:46:25 - 0:00:00 - 74507 words (102 unique) in 500 sentences.\n",
      "INFO - 05/14/20 01:46:25 - 0:00:00 - 30 unknown words (11 unique), covering 0.04% of the data.\n",
      "INFO - 05/14/20 01:46:25 - 0:00:00 - œ@@: 13\n",
      "INFO - 05/14/20 01:46:25 - 0:00:00 - −: 4\n",
      "INFO - 05/14/20 01:46:25 - 0:00:00 - ?: 3\n",
      "INFO - 05/14/20 01:46:25 - 0:00:00 - _: 2\n",
      "INFO - 05/14/20 01:46:25 - 0:00:00 - v: 2\n",
      "INFO - 05/14/20 01:46:25 - 0:00:00 - >: 1\n",
      "INFO - 05/14/20 01:46:25 - 0:00:00 - <: 1\n",
      "INFO - 05/14/20 01:46:25 - 0:00:00 - j: 1\n",
      "INFO - 05/14/20 01:46:25 - 0:00:00 - ø@@: 1\n",
      "INFO - 05/14/20 01:46:25 - 0:00:00 - °: 1\n",
      "INFO - 05/14/20 01:46:25 - 0:00:00 - +: 1\n",
      "INFO - 05/14/20 01:46:26 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Loading data from data/enfrde/processed/de-fr.fr.test.pth ...\n",
      "INFO - 05/14/20 01:46:26 - 0:00:00 - 75485 words (102 unique) in 500 sentences.\n",
      "INFO - 05/14/20 01:46:26 - 0:00:00 - 31 unknown words (10 unique), covering 0.04% of the data.\n",
      "INFO - 05/14/20 01:46:26 - 0:00:00 - œ@@: 11\n",
      "INFO - 05/14/20 01:46:26 - 0:00:00 - v: 7\n",
      "INFO - 05/14/20 01:46:26 - 0:00:00 - j: 3\n",
      "INFO - 05/14/20 01:46:26 - 0:00:00 - °: 2\n",
      "INFO - 05/14/20 01:46:26 - 0:00:00 - @: 2\n",
      "INFO - 05/14/20 01:46:26 - 0:00:00 - −: 2\n",
      "INFO - 05/14/20 01:46:26 - 0:00:00 - γ: 1\n",
      "INFO - 05/14/20 01:46:26 - 0:00:00 - >: 1\n",
      "INFO - 05/14/20 01:46:26 - 0:00:00 - <: 1\n",
      "INFO - 05/14/20 01:46:26 - 0:00:00 - ?: 1\n",
      "INFO - 05/14/20 01:46:26 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Loading data from data/enfrde/processed/train.en.pth ...\n",
      "INFO - 05/14/20 01:46:26 - 0:00:00 - 2000879 words (102 unique) in 14003 sentences.\n",
      "INFO - 05/14/20 01:46:26 - 0:00:00 - 1008 unknown words (44 unique), covering 0.05% of the data.\n",
      "INFO - 05/14/20 01:46:27 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Loading data from data/enfrde/processed/valid.en.pth ...\n",
      "INFO - 05/14/20 01:46:27 - 0:00:00 - 1522514 words (102 unique) in 10502 sentences.\n",
      "INFO - 05/14/20 01:46:27 - 0:00:00 - 740 unknown words (44 unique), covering 0.05% of the data.\n",
      "INFO - 05/14/20 01:46:28 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Loading data from data/enfrde/processed/test.en.pth ...\n",
      "INFO - 05/14/20 01:46:28 - 0:00:00 - 1523789 words (102 unique) in 10502 sentences.\n",
      "INFO - 05/14/20 01:46:28 - 0:00:00 - 742 unknown words (44 unique), covering 0.05% of the data.\n",
      "INFO - 05/14/20 01:46:28 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Loading data from data/enfrde/processed/train.fr.pth ...\n",
      "INFO - 05/14/20 01:46:28 - 0:00:00 - 2182826 words (102 unique) in 14003 sentences.\n",
      "INFO - 05/14/20 01:46:28 - 0:00:00 - 1025 unknown words (36 unique), covering 0.05% of the data.\n",
      "INFO - 05/14/20 01:46:29 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Loading data from data/enfrde/processed/valid.fr.pth ...\n",
      "INFO - 05/14/20 01:46:29 - 0:00:00 - 1629719 words (102 unique) in 10502 sentences.\n",
      "INFO - 05/14/20 01:46:29 - 0:00:00 - 754 unknown words (36 unique), covering 0.05% of the data.\n",
      "INFO - 05/14/20 01:46:30 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Loading data from data/enfrde/processed/test.fr.pth ...\n",
      "INFO - 05/14/20 01:46:30 - 0:00:00 - 1630697 words (102 unique) in 10502 sentences.\n",
      "INFO - 05/14/20 01:46:30 - 0:00:00 - 755 unknown words (36 unique), covering 0.05% of the data.\n",
      "INFO - 05/14/20 01:46:30 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Loading data from data/enfrde/processed/train.en.pth ...\n",
      "INFO - 05/14/20 01:46:30 - 0:00:00 - 2000879 words (102 unique) in 14003 sentences.\n",
      "INFO - 05/14/20 01:46:30 - 0:00:00 - 1008 unknown words (44 unique), covering 0.05% of the data.\n",
      "INFO - 05/14/20 01:46:31 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Loading data from data/enfrde/processed/valid.en.pth ...\n",
      "INFO - 05/14/20 01:46:31 - 0:00:00 - 1522514 words (102 unique) in 10502 sentences.\n",
      "INFO - 05/14/20 01:46:31 - 0:00:00 - 740 unknown words (44 unique), covering 0.05% of the data.\n",
      "INFO - 05/14/20 01:46:32 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Loading data from data/enfrde/processed/test.en.pth ...\n",
      "INFO - 05/14/20 01:46:32 - 0:00:00 - 1523789 words (102 unique) in 10502 sentences.\n",
      "INFO - 05/14/20 01:46:32 - 0:00:00 - 742 unknown words (44 unique), covering 0.05% of the data.\n",
      "INFO - 05/14/20 01:46:32 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Loading data from data/enfrde/processed/train.de.pth ...\n",
      "INFO - 05/14/20 01:46:32 - 0:00:00 - 2182826 words (102 unique) in 14003 sentences.\n",
      "INFO - 05/14/20 01:46:32 - 0:00:00 - 1025 unknown words (36 unique), covering 0.05% of the data.\n",
      "INFO - 05/14/20 01:46:33 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Loading data from data/enfrde/processed/valid.de.pth ...\n",
      "INFO - 05/14/20 01:46:33 - 0:00:00 - 1629719 words (102 unique) in 10502 sentences.\n",
      "INFO - 05/14/20 01:46:33 - 0:00:00 - 754 unknown words (36 unique), covering 0.05% of the data.\n",
      "INFO - 05/14/20 01:46:33 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Loading data from data/enfrde/processed/test.de.pth ...\n",
      "INFO - 05/14/20 01:46:33 - 0:00:00 - 1630697 words (102 unique) in 10502 sentences.\n",
      "INFO - 05/14/20 01:46:33 - 0:00:00 - 755 unknown words (36 unique), covering 0.05% of the data.\n",
      "INFO - 05/14/20 01:46:34 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Loading data from data/enfrde/processed/train.de.pth ...\n",
      "INFO - 05/14/20 01:46:34 - 0:00:00 - 2182826 words (102 unique) in 14003 sentences.\n",
      "INFO - 05/14/20 01:46:34 - 0:00:00 - 1025 unknown words (36 unique), covering 0.05% of the data.\n",
      "INFO - 05/14/20 01:46:35 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Loading data from data/enfrde/processed/valid.de.pth ...\n",
      "INFO - 05/14/20 01:46:35 - 0:00:00 - 1629719 words (102 unique) in 10502 sentences.\n",
      "INFO - 05/14/20 01:46:35 - 0:00:00 - 754 unknown words (36 unique), covering 0.05% of the data.\n",
      "INFO - 05/14/20 01:46:35 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Loading data from data/enfrde/processed/test.de.pth ...\n",
      "INFO - 05/14/20 01:46:35 - 0:00:00 - 1630697 words (102 unique) in 10502 sentences.\n",
      "INFO - 05/14/20 01:46:35 - 0:00:00 - 755 unknown words (36 unique), covering 0.05% of the data.\n",
      "INFO - 05/14/20 01:46:36 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Loading data from data/enfrde/processed/train.fr.pth ...\n",
      "INFO - 05/14/20 01:46:36 - 0:00:00 - 2182826 words (102 unique) in 14003 sentences.\n",
      "INFO - 05/14/20 01:46:36 - 0:00:00 - 1025 unknown words (36 unique), covering 0.05% of the data.\n",
      "INFO - 05/14/20 01:46:37 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Loading data from data/enfrde/processed/valid.fr.pth ...\n",
      "INFO - 05/14/20 01:46:37 - 0:00:00 - 1629719 words (102 unique) in 10502 sentences.\n",
      "INFO - 05/14/20 01:46:37 - 0:00:00 - 754 unknown words (36 unique), covering 0.05% of the data.\n",
      "INFO - 05/14/20 01:46:37 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Loading data from data/enfrde/processed/test.fr.pth ...\n",
      "INFO - 05/14/20 01:46:37 - 0:00:00 - 1630697 words (102 unique) in 10502 sentences.\n",
      "INFO - 05/14/20 01:46:37 - 0:00:00 - 755 unknown words (36 unique), covering 0.05% of the data.\n",
      "\n",
      "\n",
      "***Creat the file to train the XLM model with MLM+TLM objective***\n",
      "\n",
      "\n",
      "*** build data with succes : dir data/enfrde/processed ***\n"
     ]
    }
   ],
   "source": [
    "! ../build_meta_data.sh $sub_task $n_samples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: exp_id=maml\n",
      "env: lgs=en-fr|en-de|de-fr\n",
      "env: batch_size=2\n",
      "env: max_epoch=1\n",
      "env: epoch_size=100\n",
      "env: train_n_samples=80\n",
      "env: valid_n_samples=10\n",
      "env: test_n_samples=10\n"
     ]
    }
   ],
   "source": [
    "%env exp_id=maml\n",
    "%env lgs=en-fr|en-de|de-fr\n",
    "\n",
    "%env batch_size=2\n",
    "%env max_epoch=1\n",
    "%env epoch_size=100\n",
    "\n",
    "# limit the number of examples (-1 by default for non limitation)\n",
    "%env train_n_samples=80\n",
    "%env valid_n_samples=10\n",
    "%env test_n_samples=10\n",
    "#--train_n_samples $train_n_samples --valid_n_samples $valid_n_samples --test_n_samples $test_n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain a language model (MLM + TLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: stopping_criterion=_valid_mlm_ppl,10\n",
      "env: eval_bleu=false\n"
     ]
    }
   ],
   "source": [
    "# stopping criterion (if criterion does not improve 10 times)\n",
    "%env stopping_criterion=_valid_mlm_ppl,10\n",
    "%env eval_bleu false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/meta_XLM/XLM\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: mlm_steps=en,fr,en-fr|en,de|de,fr,de-fr\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm\n",
      "INFO - 05/14/20 03:03:43 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/14/20 03:03:43 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: [[], [], []]\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 2\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: [[], [], []]\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: [[], [], []]\n",
      "                                     command: python train.py --exp_name mlm_enfrde --exp_id maml --dump_path './dumped/' --data_path 'data/enfrde/processed' --lgs 'en-fr|en-de|de-fr' --clm_steps '' --mlm_steps 'en,fr,en-fr|en,de|de,fr,de-fr' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 2 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 100 --max_epoch 1 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu false --train_n_samples 80 --valid_n_samples 10 --test_n_samples 10 --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: data/enfrde/processed\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: ./dumped/mlm_enfrde/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 100\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: False\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_enfrde\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'de', 1: 'fr'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'de': 0, 'fr': 1}\n",
      "                                     langs: [['en', 'fr'], ['en', 'de'], ['de', 'fr']]\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['en-fr', 'en-de', 'de-fr']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 1\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: True\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [[('en', None), ('fr', None), ('en', 'fr')], [('en', None), ('de', None)], [('de', None), ('fr', None), ('de', 'fr')]]\n",
      "                                     mono_dataset: {'de': {'train': 'data/enfrde/processed/train.de.pth', 'valid': 'data/enfrde/processed/valid.de.pth', 'test': 'data/enfrde/processed/test.de.pth'}, 'fr': {'train': 'data/enfrde/processed/train.fr.pth', 'valid': 'data/enfrde/processed/valid.fr.pth', 'test': 'data/enfrde/processed/test.fr.pth'}}\n",
      "                                     mt_steps: [[], [], []]\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': 80, 'valid': 10, 'test': 10}\n",
      "                                     n_task: 3\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {('de', 'fr'): {'train': ('data/enfrde/processed/train.de-fr.de.pth', 'data/enfrde/processed/train.de-fr.fr.pth'), 'valid': ('data/enfrde/processed/valid.de-fr.de.pth', 'data/enfrde/processed/valid.de-fr.fr.pth'), 'test': ('data/enfrde/processed/test.de-fr.de.pth', 'data/enfrde/processed/test.de-fr.fr.pth')}}\n",
      "                                     pc_steps: [[], [], []]\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': True, 'valid': False, 'test': False}\n",
      "                                     remove_long_sentences_test: False\n",
      "                                     remove_long_sentences_train: True\n",
      "                                     remove_long_sentences_valid: False\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: 10\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: 80\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: 10\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/14/20 03:03:43 - 0:00:00 - The experiment will be stored in ./dumped/mlm_enfrde/maml\n",
      "                                     \n",
      "INFO - 05/14/20 03:03:43 - 0:00:00 - Running command: python train.py --exp_name mlm_enfrde --exp_id maml --dump_path './dumped/' --data_path 'data/enfrde/processed' --lgs 'en-fr|en-de|de-fr' --clm_steps '' --mlm_steps 'en,fr,en-fr|en,de|de,fr,de-fr' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 2 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 100 --max_epoch 1 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu false --train_n_samples 80 --valid_n_samples 10 --test_n_samples 10\n",
      "\n",
      "WARNING - 05/14/20 03:03:43 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/14/20 03:03:43 - 0:00:00 - ============ langs: en, fr\n",
      "INFO - 05/14/20 03:03:43 - 0:00:00 - ============ Monolingual data (en)\n",
      "INFO - 05/14/20 03:03:43 - 0:00:00 - Loading data from data/enfrde/processed/train.en.pth ...\n",
      "INFO - 05/14/20 03:03:43 - 0:00:00 - 2000879 words (102 unique) in 14003 sentences. 1008 unknown words (44 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 03:03:43 - 0:00:00 - Selecting batches from 0 to 22 ...\n",
      "\n",
      "INFO - 05/14/20 03:03:43 - 0:00:00 - Loading data from data/enfrde/processed/valid.en.pth ...\n",
      "INFO - 05/14/20 03:03:43 - 0:00:00 - 1522514 words (102 unique) in 10502 sentences. 740 unknown words (44 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 03:03:43 - 0:00:00 - Selecting batches from 0 to 5 ...\n",
      "\n",
      "INFO - 05/14/20 03:03:43 - 0:00:00 - Loading data from data/enfrde/processed/test.en.pth ...\n",
      "INFO - 05/14/20 03:03:43 - 0:00:00 - 1523789 words (102 unique) in 10502 sentences. 742 unknown words (44 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 03:03:43 - 0:00:00 - Selecting batches from 0 to 5 ...\n",
      "\n",
      "INFO - 05/14/20 03:03:43 - 0:00:00 - ============ Monolingual data (fr)\n",
      "INFO - 05/14/20 03:03:43 - 0:00:00 - Loading data from data/enfrde/processed/train.fr.pth ...\n",
      "INFO - 05/14/20 03:03:43 - 0:00:00 - 2182826 words (102 unique) in 14003 sentences. 1025 unknown words (36 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 03:03:43 - 0:00:00 - Selecting batches from 0 to 24 ...\n",
      "\n",
      "INFO - 05/14/20 03:03:43 - 0:00:00 - Loading data from data/enfrde/processed/valid.fr.pth ...\n",
      "INFO - 05/14/20 03:03:43 - 0:00:00 - 1629719 words (102 unique) in 10502 sentences. 754 unknown words (36 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 03:03:43 - 0:00:00 - Selecting batches from 0 to 6 ...\n",
      "\n",
      "INFO - 05/14/20 03:03:43 - 0:00:00 - Loading data from data/enfrde/processed/test.fr.pth ...\n",
      "INFO - 05/14/20 03:03:43 - 0:00:00 - 1630697 words (102 unique) in 10502 sentences. 755 unknown words (36 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 03:03:43 - 0:00:00 - Selecting batches from 0 to 6 ...\n",
      "\n",
      "\n",
      "INFO - 05/14/20 03:03:43 - 0:00:00 - ============ Parallel data (en-fr)\n",
      "INFO - 05/14/20 03:03:43 - 0:00:00 - Loading data from data/enfrde/processed/train.en-fr.en.pth ...\n",
      "INFO - 05/14/20 03:03:43 - 0:00:00 - 544682 words (102 unique) in 4001 sentences. 295 unknown words (28 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 03:03:43 - 0:00:00 - Loading data from data/enfrde/processed/train.en-fr.fr.pth ...\n",
      "INFO - 05/14/20 03:03:43 - 0:00:00 - 627614 words (102 unique) in 4001 sentences. 301 unknown words (34 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 03:03:43 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/14/20 03:03:43 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/14/20 03:03:43 - 0:00:00 - Removed 2520 too long sentences.\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Selecting sentences from 0 to 80 ...\n",
      "\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Loading data from data/enfrde/processed/valid.en-fr.en.pth ...\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - 66317 words (102 unique) in 500 sentences. 27 unknown words (12 unique) covering 0.04% of the data.\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Loading data from data/enfrde/processed/valid.en-fr.fr.pth ...\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - 74507 words (102 unique) in 500 sentences. 30 unknown words (11 unique) covering 0.04% of the data.\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Selecting sentences from 0 to 10 ...\n",
      "\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Loading data from data/enfrde/processed/test.en-fr.en.pth ...\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - 67592 words (102 unique) in 500 sentences. 29 unknown words (7 unique) covering 0.04% of the data.\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Loading data from data/enfrde/processed/test.en-fr.fr.pth ...\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - 75485 words (102 unique) in 500 sentences. 31 unknown words (10 unique) covering 0.04% of the data.\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Selecting sentences from 0 to 10 ...\n",
      "\n",
      "\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Monolingual data   - train -           en:       428\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Monolingual data   - valid -           en:       174\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Monolingual data   -  test -           en:       180\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Monolingual data   - train -           fr:       201\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Monolingual data   - valid -           fr:       234\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Monolingual data   -  test -           fr:        25\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Parallel data      - train -        en-fr:        80\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Parallel data      - valid -        en-fr:        10\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Parallel data      -  test -        en-fr:        10\n",
      "\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - ============ langs: en, de\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - ============ Monolingual data (en)\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Loading data from data/enfrde/processed/train.en.pth ...\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - 2000879 words (102 unique) in 14003 sentences. 1008 unknown words (44 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Selecting batches from 0 to 22 ...\n",
      "\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Loading data from data/enfrde/processed/valid.en.pth ...\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - 1522514 words (102 unique) in 10502 sentences. 740 unknown words (44 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Selecting batches from 0 to 5 ...\n",
      "\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Loading data from data/enfrde/processed/test.en.pth ...\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - 1523789 words (102 unique) in 10502 sentences. 742 unknown words (44 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Selecting batches from 0 to 5 ...\n",
      "\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - ============ Monolingual data (de)\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Loading data from data/enfrde/processed/train.de.pth ...\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - 2182826 words (102 unique) in 14003 sentences. 1025 unknown words (36 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Selecting batches from 0 to 24 ...\n",
      "\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Loading data from data/enfrde/processed/valid.de.pth ...\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - 1629719 words (102 unique) in 10502 sentences. 754 unknown words (36 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Selecting batches from 0 to 6 ...\n",
      "\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Loading data from data/enfrde/processed/test.de.pth ...\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - 1630697 words (102 unique) in 10502 sentences. 755 unknown words (36 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Selecting batches from 0 to 6 ...\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Monolingual data   - train -           en:       428\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Monolingual data   - valid -           en:       174\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Monolingual data   -  test -           en:       180\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Monolingual data   - train -           de:       201\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Monolingual data   - valid -           de:       234\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Monolingual data   -  test -           de:        25\n",
      "\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - ============ langs: de, fr\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - ============ Monolingual data (de)\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Loading data from data/enfrde/processed/train.de.pth ...\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - 2182826 words (102 unique) in 14003 sentences. 1025 unknown words (36 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Selecting batches from 0 to 24 ...\n",
      "\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Loading data from data/enfrde/processed/valid.de.pth ...\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - 1629719 words (102 unique) in 10502 sentences. 754 unknown words (36 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Selecting batches from 0 to 6 ...\n",
      "\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Loading data from data/enfrde/processed/test.de.pth ...\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - 1630697 words (102 unique) in 10502 sentences. 755 unknown words (36 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Selecting batches from 0 to 6 ...\n",
      "\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - ============ Monolingual data (fr)\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Loading data from data/enfrde/processed/train.fr.pth ...\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - 2182826 words (102 unique) in 14003 sentences. 1025 unknown words (36 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Selecting batches from 0 to 24 ...\n",
      "\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Loading data from data/enfrde/processed/valid.fr.pth ...\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - 1629719 words (102 unique) in 10502 sentences. 754 unknown words (36 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Selecting batches from 0 to 6 ...\n",
      "\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Loading data from data/enfrde/processed/test.fr.pth ...\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - 1630697 words (102 unique) in 10502 sentences. 755 unknown words (36 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Selecting batches from 0 to 6 ...\n",
      "\n",
      "\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - ============ Parallel data (de-fr)\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Loading data from data/enfrde/processed/train.de-fr.de.pth ...\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - 627614 words (102 unique) in 4001 sentences. 301 unknown words (34 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Loading data from data/enfrde/processed/train.de-fr.fr.pth ...\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - 627614 words (102 unique) in 4001 sentences. 301 unknown words (34 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Removed 2473 too long sentences.\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Selecting sentences from 0 to 80 ...\n",
      "\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Loading data from data/enfrde/processed/valid.de-fr.de.pth ...\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - 74507 words (102 unique) in 500 sentences. 30 unknown words (11 unique) covering 0.04% of the data.\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Loading data from data/enfrde/processed/valid.de-fr.fr.pth ...\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - 74507 words (102 unique) in 500 sentences. 30 unknown words (11 unique) covering 0.04% of the data.\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Selecting sentences from 0 to 10 ...\n",
      "\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Loading data from data/enfrde/processed/test.de-fr.de.pth ...\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - 75485 words (102 unique) in 500 sentences. 31 unknown words (10 unique) covering 0.04% of the data.\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Loading data from data/enfrde/processed/test.de-fr.fr.pth ...\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - 75485 words (102 unique) in 500 sentences. 31 unknown words (10 unique) covering 0.04% of the data.\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Selecting sentences from 0 to 10 ...\n",
      "\n",
      "\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Monolingual data   - train -           de:       201\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Monolingual data   - valid -           de:       234\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Monolingual data   -  test -           de:        25\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Monolingual data   - train -           fr:       201\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Monolingual data   - valid -           fr:       234\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Monolingual data   -  test -           fr:        25\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Parallel data      - train -        de-fr:        80\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Parallel data      - valid -        de-fr:        10\n",
      "INFO - 05/14/20 03:03:44 - 0:00:00 - Parallel data      -  test -        de-fr:        10\n",
      "\n",
      "INFO - 05/14/20 03:03:44 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(102, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=102, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/14/20 03:03:44 - 0:00:01 - Number of parameters (model): 76210278\n",
      "INFO - 05/14/20 03:03:48 - 0:00:04 - Found 0 memories.\n",
      "INFO - 05/14/20 03:03:48 - 0:00:04 - Found 6 FFN.\n",
      "INFO - 05/14/20 03:03:48 - 0:00:04 - Found 102 parameters in model.\n",
      "INFO - 05/14/20 03:03:48 - 0:00:04 - Optimizers: model\n",
      "INFO - 05/14/20 03:03:48 - 0:00:04 - ============ Starting epoch 0 ... ============\n",
      "INFO - 05/14/20 03:03:48 - 0:00:04 - Creating new training data iterator (pred,fr) ...\n",
      "INFO - 05/14/20 03:03:48 - 0:00:05 - Creating new training data iterator (pred,en,fr) ...\n",
      "INFO - 05/14/20 03:03:48 - 0:00:05 - Creating new training data iterator (pred,en) ...\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 539, in <module>\n",
      "    main(params)\n",
      "  File \"train.py\", line 396, in main\n",
      "    flag = flag or trainer.mlm_step(lang1_dic['mlm_step'] , lang2_dic['mlm_step'], params.lambda_mlm)\n",
      "  File \"/home/jupyter/meta_XLM/XLM/src/trainer.py\", line 932, in mlm_step\n",
      "    x, lengths, positions, langs, _ = self.generate_batch(lang1, lang2, 'pred', data_key = data_key)\n",
      "  File \"/home/jupyter/meta_XLM/XLM/src/trainer.py\", line 592, in generate_batch\n",
      "    lang1_id = params.lang2id[lang1]\n",
      "KeyError: 'de'\n"
     ]
    }
   ],
   "source": [
    "%env mlm_steps=en,fr,en-fr|en,de|de,fr,de-fr\n",
    "\n",
    "! python train.py --exp_name mlm_enfrde --exp_id $exp_id --dump_path ./dumped/ --data_path $OUTPATH --lgs $lgs --clm_steps '' --mlm_steps $mlm_steps --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout 0.1 --attention_dropout 0.1 --gelu_activation true --batch_size $batch_size --bptt 256 --optimizer adam,lr=0.0001 --epoch_size $epoch_size --max_epoch $max_epoch --validation_metrics _valid_mlm_ppl --stopping_criterion $stopping_criterion --eval_bleu $eval_bleu --train_n_samples $train_n_samples --valid_n_samples $valid_n_samples --test_n_samples $test_n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
