{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data : Monolingual data (MLM) and/or Parallel data (TLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(bookmark:HOME) -> /home/jupyter/meta_XLM/XLM\n",
      "/home/jupyter/meta_XLM/XLM\n"
     ]
    }
   ],
   "source": [
    "%bookmark HOME \"/home/jupyter/meta_XLM/XLM\" \n",
    "%cd -b HOME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PARA=True\n",
      "env: MONO=True\n",
      "env: PARA_PATH=data/enfrde/para\n",
      "env: MONO_PATH=data/enfrde/mono\n",
      "env: SAME_VOCAB=True\n",
      "env: nCodes=10\n",
      "env: shuf_n_samples=1\n",
      "env: threads_for_tokenizer=16\n",
      "env: test_size=10\n",
      "env: val_size=10\n",
      "env: TOKENIZE=tools/tokenizer_our.sh\n",
      "env: LOWER_REMOVE_ACCENT=tools/lowercase_and_remove_accent.py\n",
      "env: FASTBPE=tools/fastBPE/fast\n",
      "env: OUTPATH=data/enfrde/processed\n",
      "env: n_samples=-1\n",
      "env: sub_task=en-fr:10,en-de:-1,de-fr:-1\n"
     ]
    }
   ],
   "source": [
    "%env PARA=True          \n",
    "%env MONO=True           \n",
    "                   \n",
    "%env PARA_PATH=data/enfrde/para      \n",
    "%env MONO_PATH=data/enfrde/mono    \n",
    "%env SAME_VOCAB=True    \n",
    "%env nCodes=10\n",
    "%env shuf_n_samples=1\n",
    "%env threads_for_tokenizer=16\n",
    "%env test_size=10       \n",
    "%env val_size=10        \n",
    "\n",
    "# tools paths\n",
    "%env TOKENIZE=tools/tokenizer_our.sh\n",
    "%env LOWER_REMOVE_ACCENT=tools/lowercase_and_remove_accent.py\n",
    "%env FASTBPE=tools/fastBPE/fast\n",
    "\n",
    "\n",
    "%env OUTPATH=data/enfrde/processed \n",
    "# create output path\n",
    "! mkdir -p $OUTPATH\n",
    "\n",
    "! chmod +x $FASTBPE\n",
    "! chmod +x ../build_meta_data.sh\n",
    "! chmod +x tools/mosesdecoder/scripts/tokenizer/*.perl\n",
    "\n",
    "%env n_samples=-1\n",
    "\n",
    "%env sub_task=en-fr:10,en-de:-1,de-fr:-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params ok !\n",
      "*** Cleaning and tokenizing en-fr data ... ***\n",
      "file data/enfrde/para/en-fr.en.all already exists\n",
      "file data/enfrde/para/en-fr.fr.all already exists\n",
      "*** Cleaning and tokenizing en-de data ... ***\n",
      "file data/enfrde/para/en-de.en.all already exists\n",
      "file data/enfrde/para/en-de.de.all already exists\n",
      "*** Cleaning and tokenizing de-fr data ... ***\n",
      "file data/enfrde/para/de-fr.de.all already exists\n",
      "file data/enfrde/para/de-fr.fr.all already exists\n",
      "file data/enfrde/mono/en.all already exists\n",
      "file data/enfrde/mono/fr.all already exists\n",
      "file data/enfrde/mono/en.all already exists\n",
      "file data/enfrde/mono/de.all already exists\n",
      "file data/enfrde/mono/de.all already exists\n",
      "file data/enfrde/mono/fr.all already exists\n",
      "\n",
      "\n",
      "*** split into train / valid / test ***\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "\n",
      "\n",
      "\n",
      "***build the training set for BPE tokenization (10 codes)***\n",
      "\n",
      "\n",
      "***shuf ... Generating 1 random permutations of training data and store result in data/enfrde/processed/de-fr/bpe.train***\n",
      "file data/enfrde/processed/bpe.train already exists\n",
      "\n",
      "\n",
      "***Learn the BPE vocabulary on the training set : data/enfrde/processed/bpe.train ...***\n",
      "file data/enfrde/processed/codes already exists\n",
      "***Learn 10 BPE code on the bpe.train file***\n",
      "\n",
      "\n",
      "***Get the post-BPE vocab***\n",
      "file data/enfrde/processed/train already exists\n",
      "file data/enfrde/processed/vocab already exists\n",
      "\n",
      "\n",
      "***Apply BPE tokenization on the corpora.***\n",
      "file data/enfrde/processed/en-fr.en.train already exists\n",
      "file data/enfrde/processed/en-fr.en.valid already exists\n",
      "file data/enfrde/processed/en-fr.en.test already exists\n",
      "file data/enfrde/processed/en-fr.fr.train already exists\n",
      "file data/enfrde/processed/en-fr.fr.valid already exists\n",
      "file data/enfrde/processed/en-fr.fr.test already exists\n",
      "file data/enfrde/processed/en-de.en.train already exists\n",
      "file data/enfrde/processed/en-de.en.valid already exists\n",
      "file data/enfrde/processed/en-de.en.test already exists\n",
      "file data/enfrde/processed/en-de.de.train already exists\n",
      "file data/enfrde/processed/en-de.de.valid already exists\n",
      "file data/enfrde/processed/en-de.de.test already exists\n",
      "file data/enfrde/processed/de-fr.de.train already exists\n",
      "file data/enfrde/processed/de-fr.de.valid already exists\n",
      "file data/enfrde/processed/de-fr.de.test already exists\n",
      "file data/enfrde/processed/de-fr.fr.train already exists\n",
      "file data/enfrde/processed/de-fr.fr.valid already exists\n",
      "file data/enfrde/processed/de-fr.fr.test already exists\n",
      "file data/enfrde/processed/train.en already exists\n",
      "file data/enfrde/processed/valid.en already exists\n",
      "file data/enfrde/processed/test.en already exists\n",
      "file data/enfrde/processed/train.fr already exists\n",
      "file data/enfrde/processed/valid.fr already exists\n",
      "file data/enfrde/processed/test.fr already exists\n",
      "file data/enfrde/processed/train.en already exists\n",
      "file data/enfrde/processed/valid.en already exists\n",
      "file data/enfrde/processed/test.en already exists\n",
      "file data/enfrde/processed/train.de already exists\n",
      "file data/enfrde/processed/valid.de already exists\n",
      "file data/enfrde/processed/test.de already exists\n",
      "file data/enfrde/processed/train.de already exists\n",
      "file data/enfrde/processed/valid.de already exists\n",
      "file data/enfrde/processed/test.de already exists\n",
      "file data/enfrde/processed/train.fr already exists\n",
      "file data/enfrde/processed/valid.fr already exists\n",
      "file data/enfrde/processed/test.fr already exists\n",
      "\n",
      "\n",
      "***Build fine_tune data***\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "shuf: write error: Broken pipe\n",
      "shuf: write error\n",
      "\n",
      "\n",
      "***Binarize everything using preprocess.py.***\n",
      "file data/enfrde/processed/en-fr.en.train.pth already exists\n",
      "INFO - 05/14/20 13:33:11 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Saving the data to data/enfrde/processed/fine_tune/en-fr.en.train.pth ...\n",
      "INFO - 05/14/20 13:33:11 - 0:00:00 - 20371 words (102 unique) in 155 sentences.\n",
      "INFO - 05/14/20 13:33:11 - 0:00:00 - 15 unknown words (7 unique), covering 0.07% of the data.\n",
      "INFO - 05/14/20 13:33:11 - 0:00:00 - }: 3\n",
      "INFO - 05/14/20 13:33:11 - 0:00:00 - {: 3\n",
      "INFO - 05/14/20 13:33:11 - 0:00:00 - $: 3\n",
      "INFO - 05/14/20 13:33:11 - 0:00:00 - v: 3\n",
      "INFO - 05/14/20 13:33:11 - 0:00:00 - =: 1\n",
      "INFO - 05/14/20 13:33:11 - 0:00:00 - ?: 1\n",
      "INFO - 05/14/20 13:33:11 - 0:00:00 - j: 1\n",
      "file data/enfrde/processed/en-fr.en.valid.pth already exists\n",
      "INFO - 05/14/20 13:33:11 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Saving the data to data/enfrde/processed/fine_tune/en-fr.en.valid.pth ...\n",
      "INFO - 05/14/20 13:33:11 - 0:00:00 - 4193 words (102 unique) in 30 sentences.\n",
      "INFO - 05/14/20 13:33:11 - 0:00:00 - 2 unknown words (1 unique), covering 0.05% of the data.\n",
      "INFO - 05/14/20 13:33:11 - 0:00:00 - v: 2\n",
      "file data/enfrde/processed/en-fr.en.test.pth already exists\n",
      "INFO - 05/14/20 13:33:12 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Saving the data to data/enfrde/processed/fine_tune/en-fr.en.test.pth ...\n",
      "INFO - 05/14/20 13:33:12 - 0:00:00 - 3944 words (102 unique) in 30 sentences.\n",
      "file data/enfrde/processed/en-fr.fr.train.pth already exists\n",
      "INFO - 05/14/20 13:33:13 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Saving the data to data/enfrde/processed/fine_tune/en-fr.fr.train.pth ...\n",
      "INFO - 05/14/20 13:33:13 - 0:00:00 - 36299 words (102 unique) in 236 sentences.\n",
      "INFO - 05/14/20 13:33:13 - 0:00:00 - 17 unknown words (11 unique), covering 0.05% of the data.\n",
      "INFO - 05/14/20 13:33:13 - 0:00:00 - œ@@: 3\n",
      "INFO - 05/14/20 13:33:13 - 0:00:00 - т@@: 2\n",
      "INFO - 05/14/20 13:33:13 - 0:00:00 - е@@: 2\n",
      "INFO - 05/14/20 13:33:13 - 0:00:00 - μ@@: 2\n",
      "INFO - 05/14/20 13:33:13 - 0:00:00 - −: 2\n",
      "INFO - 05/14/20 13:33:13 - 0:00:00 - j: 1\n",
      "INFO - 05/14/20 13:33:13 - 0:00:00 - а@@: 1\n",
      "INFO - 05/14/20 13:33:13 - 0:00:00 - т: 1\n",
      "INFO - 05/14/20 13:33:13 - 0:00:00 - з: 1\n",
      "INFO - 05/14/20 13:33:13 - 0:00:00 - º@@: 1\n",
      "INFO - 05/14/20 13:33:13 - 0:00:00 - ?: 1\n",
      "file data/enfrde/processed/en-fr.fr.valid.pth already exists\n",
      "INFO - 05/14/20 13:33:13 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Saving the data to data/enfrde/processed/fine_tune/en-fr.fr.valid.pth ...\n",
      "INFO - 05/14/20 13:33:13 - 0:00:00 - 4747 words (102 unique) in 30 sentences.\n",
      "INFO - 05/14/20 13:33:13 - 0:00:00 - 2 unknown words (1 unique), covering 0.04% of the data.\n",
      "INFO - 05/14/20 13:33:13 - 0:00:00 - œ@@: 2\n",
      "file data/enfrde/processed/en-fr.fr.test.pth already exists\n",
      "INFO - 05/14/20 13:33:14 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Saving the data to data/enfrde/processed/fine_tune/en-fr.fr.test.pth ...\n",
      "INFO - 05/14/20 13:33:14 - 0:00:00 - 4531 words (102 unique) in 30 sentences.\n",
      "INFO - 05/14/20 13:33:14 - 0:00:00 - 1 unknown words (1 unique), covering 0.02% of the data.\n",
      "INFO - 05/14/20 13:33:14 - 0:00:00 - œ@@: 1\n",
      "file data/enfrde/processed/en-de.en.train.pth already exists\n",
      "file data/enfrde/processed/en-de.en.valid.pth already exists\n",
      "file data/enfrde/processed/en-de.en.test.pth already exists\n",
      "file data/enfrde/processed/en-de.de.train.pth already exists\n",
      "file data/enfrde/processed/en-de.de.valid.pth already exists\n",
      "file data/enfrde/processed/en-de.de.test.pth already exists\n",
      "file data/enfrde/processed/de-fr.de.train.pth already exists\n",
      "file data/enfrde/processed/de-fr.de.valid.pth already exists\n",
      "file data/enfrde/processed/de-fr.de.test.pth already exists\n",
      "file data/enfrde/processed/de-fr.fr.train.pth already exists\n",
      "file data/enfrde/processed/de-fr.fr.valid.pth already exists\n",
      "file data/enfrde/processed/de-fr.fr.test.pth already exists\n",
      "file data/enfrde/processed/train.en.pth already exists\n",
      "INFO - 05/14/20 13:33:14 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Saving the data to data/enfrde/processed/fine_tune/train.en.pth ...\n",
      "INFO - 05/14/20 13:33:14 - 0:00:00 - 90524 words (102 unique) in 618 sentences.\n",
      "INFO - 05/14/20 13:33:14 - 0:00:00 - 48 unknown words (11 unique), covering 0.05% of the data.\n",
      "INFO - 05/14/20 13:33:14 - 0:00:00 - $: 17\n",
      "INFO - 05/14/20 13:33:14 - 0:00:00 - v: 11\n",
      "INFO - 05/14/20 13:33:14 - 0:00:00 - =: 6\n",
      "INFO - 05/14/20 13:33:14 - 0:00:00 - j: 4\n",
      "INFO - 05/14/20 13:33:14 - 0:00:00 - @: 2\n",
      "INFO - 05/14/20 13:33:14 - 0:00:00 - ?: 2\n",
      "INFO - 05/14/20 13:33:14 - 0:00:00 - œ@@: 2\n",
      "INFO - 05/14/20 13:33:14 - 0:00:00 - ∗: 1\n",
      "INFO - 05/14/20 13:33:14 - 0:00:00 - −: 1\n",
      "INFO - 05/14/20 13:33:14 - 0:00:00 - >: 1\n",
      "INFO - 05/14/20 13:33:14 - 0:00:00 - <: 1\n",
      "file data/enfrde/processed/valid.en.pth already exists\n",
      "INFO - 05/14/20 13:33:15 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Saving the data to data/enfrde/processed/fine_tune/valid.en.pth ...\n",
      "INFO - 05/14/20 13:33:15 - 0:00:00 - 77104 words (102 unique) in 558 sentences.\n",
      "INFO - 05/14/20 13:33:15 - 0:00:00 - 41 unknown words (11 unique), covering 0.05% of the data.\n",
      "INFO - 05/14/20 13:33:15 - 0:00:00 - $: 11\n",
      "INFO - 05/14/20 13:33:15 - 0:00:00 - œ@@: 7\n",
      "INFO - 05/14/20 13:33:15 - 0:00:00 - v: 6\n",
      "INFO - 05/14/20 13:33:15 - 0:00:00 - ?: 4\n",
      "INFO - 05/14/20 13:33:15 - 0:00:00 - °: 4\n",
      "INFO - 05/14/20 13:33:15 - 0:00:00 - >: 2\n",
      "INFO - 05/14/20 13:33:15 - 0:00:00 - <: 2\n",
      "INFO - 05/14/20 13:33:15 - 0:00:00 - μ@@: 2\n",
      "INFO - 05/14/20 13:33:15 - 0:00:00 - €: 1\n",
      "INFO - 05/14/20 13:33:15 - 0:00:00 - @: 1\n",
      "INFO - 05/14/20 13:33:15 - 0:00:00 - =: 1\n",
      "file data/enfrde/processed/test.en.pth already exists\n",
      "INFO - 05/14/20 13:33:16 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Saving the data to data/enfrde/processed/fine_tune/test.en.pth ...\n",
      "INFO - 05/14/20 13:33:16 - 0:00:00 - 77014 words (102 unique) in 558 sentences.\n",
      "INFO - 05/14/20 13:33:16 - 0:00:00 - 40 unknown words (11 unique), covering 0.05% of the data.\n",
      "INFO - 05/14/20 13:33:16 - 0:00:00 - $: 10\n",
      "INFO - 05/14/20 13:33:16 - 0:00:00 - œ@@: 7\n",
      "INFO - 05/14/20 13:33:16 - 0:00:00 - v: 6\n",
      "INFO - 05/14/20 13:33:16 - 0:00:00 - ?: 4\n",
      "INFO - 05/14/20 13:33:16 - 0:00:00 - °: 4\n",
      "INFO - 05/14/20 13:33:16 - 0:00:00 - >: 2\n",
      "INFO - 05/14/20 13:33:16 - 0:00:00 - <: 2\n",
      "INFO - 05/14/20 13:33:16 - 0:00:00 - μ@@: 2\n",
      "INFO - 05/14/20 13:33:16 - 0:00:00 - €: 1\n",
      "INFO - 05/14/20 13:33:16 - 0:00:00 - @: 1\n",
      "INFO - 05/14/20 13:33:16 - 0:00:00 - =: 1\n",
      "file data/enfrde/processed/train.fr.pth already exists\n",
      "INFO - 05/14/20 13:33:16 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Saving the data to data/enfrde/processed/fine_tune/train.fr.pth ...\n",
      "INFO - 05/14/20 13:33:16 - 0:00:00 - 120570 words (102 unique) in 827 sentences.\n",
      "INFO - 05/14/20 13:33:16 - 0:00:00 - 68 unknown words (18 unique), covering 0.06% of the data.\n",
      "INFO - 05/14/20 13:33:16 - 0:00:00 - œ@@: 18\n",
      "INFO - 05/14/20 13:33:16 - 0:00:00 - v: 8\n",
      "INFO - 05/14/20 13:33:16 - 0:00:00 - и@@: 6\n",
      "INFO - 05/14/20 13:33:16 - 0:00:00 - ?: 6\n",
      "INFO - 05/14/20 13:33:16 - 0:00:00 - @: 5\n",
      "INFO - 05/14/20 13:33:16 - 0:00:00 - °: 3\n",
      "INFO - 05/14/20 13:33:16 - 0:00:00 - {: 3\n",
      "INFO - 05/14/20 13:33:16 - 0:00:00 - =: 3\n",
      "INFO - 05/14/20 13:33:16 - 0:00:00 - −: 2\n",
      "INFO - 05/14/20 13:33:16 - 0:00:00 - }: 2\n",
      "INFO - 05/14/20 13:33:16 - 0:00:00 - >: 2\n",
      "INFO - 05/14/20 13:33:16 - 0:00:00 - <: 2\n",
      "INFO - 05/14/20 13:33:16 - 0:00:00 - º: 2\n",
      "INFO - 05/14/20 13:33:16 - 0:00:00 - j: 2\n",
      "INFO - 05/14/20 13:33:16 - 0:00:00 - и: 1\n",
      "INFO - 05/14/20 13:33:16 - 0:00:00 - а: 1\n",
      "INFO - 05/14/20 13:33:16 - 0:00:00 - º@@: 1\n",
      "INFO - 05/14/20 13:33:16 - 0:00:00 - ·: 1\n",
      "file data/enfrde/processed/valid.fr.pth already exists\n",
      "INFO - 05/14/20 13:33:17 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Saving the data to data/enfrde/processed/fine_tune/valid.fr.pth ...\n",
      "INFO - 05/14/20 13:33:17 - 0:00:00 - 93708 words (102 unique) in 620 sentences.\n",
      "INFO - 05/14/20 13:33:17 - 0:00:00 - 34 unknown words (9 unique), covering 0.04% of the data.\n",
      "INFO - 05/14/20 13:33:17 - 0:00:00 - ®: 7\n",
      "INFO - 05/14/20 13:33:17 - 0:00:00 - œ@@: 7\n",
      "INFO - 05/14/20 13:33:17 - 0:00:00 - v: 7\n",
      "INFO - 05/14/20 13:33:17 - 0:00:00 - −: 5\n",
      "INFO - 05/14/20 13:33:17 - 0:00:00 - ?: 3\n",
      "INFO - 05/14/20 13:33:17 - 0:00:00 - j: 2\n",
      "INFO - 05/14/20 13:33:17 - 0:00:00 - >: 1\n",
      "INFO - 05/14/20 13:33:17 - 0:00:00 - <: 1\n",
      "INFO - 05/14/20 13:33:17 - 0:00:00 - @: 1\n",
      "file data/enfrde/processed/test.fr.pth already exists\n",
      "INFO - 05/14/20 13:33:18 - 0:00:00 - Read 102 words from the vocabulary file.\n",
      "\n",
      "Saving the data to data/enfrde/processed/fine_tune/test.fr.pth ...\n",
      "INFO - 05/14/20 13:33:18 - 0:00:00 - 93080 words (102 unique) in 620 sentences.\n",
      "INFO - 05/14/20 13:33:18 - 0:00:00 - 39 unknown words (9 unique), covering 0.04% of the data.\n",
      "INFO - 05/14/20 13:33:18 - 0:00:00 - œ@@: 8\n",
      "INFO - 05/14/20 13:33:18 - 0:00:00 - ®: 7\n",
      "INFO - 05/14/20 13:33:18 - 0:00:00 - v: 7\n",
      "INFO - 05/14/20 13:33:18 - 0:00:00 - −: 5\n",
      "INFO - 05/14/20 13:33:18 - 0:00:00 - @: 3\n",
      "INFO - 05/14/20 13:33:18 - 0:00:00 - ?: 3\n",
      "INFO - 05/14/20 13:33:18 - 0:00:00 - j: 2\n",
      "INFO - 05/14/20 13:33:18 - 0:00:00 - >: 2\n",
      "INFO - 05/14/20 13:33:18 - 0:00:00 - <: 2\n",
      "file data/enfrde/processed/train.en.pth already exists\n",
      "file data/enfrde/processed/fine_tune/train.en.pth already exists\n",
      "file data/enfrde/processed/valid.en.pth already exists\n",
      "file data/enfrde/processed/fine_tune/valid.en.pth already exists\n",
      "file data/enfrde/processed/test.en.pth already exists\n",
      "file data/enfrde/processed/fine_tune/test.en.pth already exists\n",
      "file data/enfrde/processed/train.de.pth already exists\n",
      "file data/enfrde/processed/valid.de.pth already exists\n",
      "file data/enfrde/processed/test.de.pth already exists\n",
      "file data/enfrde/processed/train.de.pth already exists\n",
      "file data/enfrde/processed/valid.de.pth already exists\n",
      "file data/enfrde/processed/test.de.pth already exists\n",
      "file data/enfrde/processed/train.fr.pth already exists\n",
      "file data/enfrde/processed/fine_tune/train.fr.pth already exists\n",
      "file data/enfrde/processed/valid.fr.pth already exists\n",
      "file data/enfrde/processed/fine_tune/valid.fr.pth already exists\n",
      "file data/enfrde/processed/test.fr.pth already exists\n",
      "file data/enfrde/processed/fine_tune/test.fr.pth already exists\n",
      "\n",
      "\n",
      "***Creat the file to train the XLM model with MLM+TLM objective***\n",
      "file data/enfrde/processed/train.en-fr.en.pth already exists\n",
      "file data/enfrde/processed/valid.en-fr.en.pth already exists\n",
      "file data/enfrde/processed/test.en-fr.en.pth already exists\n",
      "file data/enfrde/processed/train.en-fr.fr.pth already exists\n",
      "file data/enfrde/processed/valid.en-fr.fr.pth already exists\n",
      "file data/enfrde/processed/test.en-fr.fr.pth already exists\n",
      "file data/enfrde/processed/train.en-de.en.pth already exists\n",
      "file data/enfrde/processed/valid.en-de.en.pth already exists\n",
      "file data/enfrde/processed/test.en-de.en.pth already exists\n",
      "file data/enfrde/processed/train.en-de.de.pth already exists\n",
      "file data/enfrde/processed/valid.en-de.de.pth already exists\n",
      "file data/enfrde/processed/test.en-de.de.pth already exists\n",
      "file data/enfrde/processed/train.de-fr.de.pth already exists\n",
      "file data/enfrde/processed/valid.de-fr.de.pth already exists\n",
      "file data/enfrde/processed/test.de-fr.de.pth already exists\n",
      "file data/enfrde/processed/train.de-fr.fr.pth already exists\n",
      "file data/enfrde/processed/valid.de-fr.fr.pth already exists\n",
      "file data/enfrde/processed/test.de-fr.fr.pth already exists\n",
      "\n",
      "\n",
      "*** build data with succes : dir data/enfrde/processed ***\n"
     ]
    }
   ],
   "source": [
    "! ../build_meta_data.sh $sub_task $n_samples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: exp_id=maml\n",
      "env: lgs=en-fr|de-fr\n",
      "env: batch_size=2\n",
      "env: max_epoch=1\n",
      "env: epoch_size=100\n",
      "env: train_n_samples=80\n",
      "env: valid_n_samples=10\n",
      "env: test_n_samples=10\n"
     ]
    }
   ],
   "source": [
    "%env exp_id=maml\n",
    "#%env lgs=en-fr|en-de|de-fr\n",
    "%env lgs=en-fr|de-fr\n",
    "\n",
    "%env batch_size=2\n",
    "%env max_epoch=1\n",
    "%env epoch_size=100\n",
    "\n",
    "# limit the number of examples (-1 by default for non limitation)\n",
    "%env train_n_samples=80\n",
    "%env valid_n_samples=10\n",
    "%env test_n_samples=10\n",
    "#--train_n_samples $train_n_samples --valid_n_samples $valid_n_samples --test_n_samples $test_n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: remove_long_sentences_train=True\n",
      "env: remove_long_sentences_valid=True\n",
      "env: remove_long_sentences_test=True\n"
     ]
    }
   ],
   "source": [
    "# If you don't have enough RAM or swap memory, leave these three parameters to True, otherwise you may get an error like this when evaluating \n",
    "# RuntimeError: copy_if failed to synchronize: cudaErrorAssert: device-side assert triggered\n",
    "%env remove_long_sentences_train=True\n",
    "%env remove_long_sentences_valid=True\n",
    "%env remove_long_sentences_test=True\n",
    "#--remove_long_sentences_train $remove_long_sentences_train --remove_long_sentences_valid $remove_long_sentences_valid --remove_long_sentences_test $remove_long_sentences_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain a language meta-model (MLM + TLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: stopping_criterion=_valid_mlm_ppl,10\n",
      "env: eval_bleu=false\n"
     ]
    }
   ],
   "source": [
    "# stopping criterion (if criterion does not improve 10 times)\n",
    "%env stopping_criterion=_valid_mlm_ppl,10\n",
    "%env eval_bleu false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: mlm_steps=en,fr,en-fr|de,fr,de-fr\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: [[], []]\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 2\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: [[], []]\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: [[], []]\n",
      "                                     command: python train.py --exp_name mlm_enfrde --exp_id maml --dump_path './dumped/' --data_path 'data/enfrde/processed' --lgs 'en-fr|de-fr' --clm_steps '' --mlm_steps 'en,fr,en-fr|de,fr,de-fr' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 2 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 100 --max_epoch 1 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu false --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --train_n_samples 80 --valid_n_samples 10 --test_n_samples 10 --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: data/enfrde/processed\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: ./dumped/mlm_enfrde/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 100\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: False\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: mlm_enfrde\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'de', 1: 'fr'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'de': 0, 'fr': 1}\n",
      "                                     langs: [['en', 'fr'], ['de', 'fr']]\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['en-fr', 'de-fr']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 1\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: True\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [[('en', None), ('fr', None), ('en', 'fr')], [('de', None), ('fr', None), ('de', 'fr')]]\n",
      "                                     mono_dataset: {'de': {'train': 'data/enfrde/processed/train.de.pth', 'valid': 'data/enfrde/processed/valid.de.pth', 'test': 'data/enfrde/processed/test.de.pth'}, 'fr': {'train': 'data/enfrde/processed/train.fr.pth', 'valid': 'data/enfrde/processed/valid.fr.pth', 'test': 'data/enfrde/processed/test.fr.pth'}}\n",
      "                                     mt_steps: [[], []]\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': 80, 'valid': 10, 'test': 10}\n",
      "                                     n_task: 2\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {('de', 'fr'): {'train': ('data/enfrde/processed/train.de-fr.de.pth', 'data/enfrde/processed/train.de-fr.fr.pth'), 'valid': ('data/enfrde/processed/valid.de-fr.de.pth', 'data/enfrde/processed/valid.de-fr.fr.pth'), 'test': ('data/enfrde/processed/test.de-fr.de.pth', 'data/enfrde/processed/test.de-fr.fr.pth')}}\n",
      "                                     pc_steps: [[], []]\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': True, 'valid': True, 'test': True}\n",
      "                                     remove_long_sentences_test: True\n",
      "                                     remove_long_sentences_train: True\n",
      "                                     remove_long_sentences_valid: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: 10\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: 80\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: 10\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - The experiment will be stored in ./dumped/mlm_enfrde/maml\n",
      "                                     \n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Running command: python train.py --exp_name mlm_enfrde --exp_id maml --dump_path './dumped/' --data_path 'data/enfrde/processed' --lgs 'en-fr|de-fr' --clm_steps '' --mlm_steps 'en,fr,en-fr|de,fr,de-fr' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 2 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 100 --max_epoch 1 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu false --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --train_n_samples 80 --valid_n_samples 10 --test_n_samples 10\n",
      "\n",
      "WARNING - 05/14/20 14:44:12 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - ============ langs: en, fr\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - ============ Monolingual data (en)\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Loading data from data/enfrde/processed/train.en.pth ...\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - 2000879 words (102 unique) in 14003 sentences. 1008 unknown words (44 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Selecting batches from 0 to 22 ...\n",
      "\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Loading data from data/enfrde/processed/valid.en.pth ...\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - 1522514 words (102 unique) in 10502 sentences. 740 unknown words (44 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Selecting batches from 0 to 5 ...\n",
      "\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Loading data from data/enfrde/processed/test.en.pth ...\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - 1523789 words (102 unique) in 10502 sentences. 742 unknown words (44 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Selecting batches from 0 to 5 ...\n",
      "\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - ============ Monolingual data (fr)\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Loading data from data/enfrde/processed/train.fr.pth ...\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - 2182826 words (102 unique) in 14003 sentences. 1025 unknown words (36 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Selecting batches from 0 to 24 ...\n",
      "\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Loading data from data/enfrde/processed/valid.fr.pth ...\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - 1629719 words (102 unique) in 10502 sentences. 754 unknown words (36 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Selecting batches from 0 to 6 ...\n",
      "\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Loading data from data/enfrde/processed/test.fr.pth ...\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - 1630697 words (102 unique) in 10502 sentences. 755 unknown words (36 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Selecting batches from 0 to 6 ...\n",
      "\n",
      "\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - ============ Parallel data (en-fr)\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Loading data from data/enfrde/processed/train.en-fr.en.pth ...\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - 544682 words (102 unique) in 4001 sentences. 295 unknown words (28 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Loading data from data/enfrde/processed/train.en-fr.fr.pth ...\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - 627614 words (102 unique) in 4001 sentences. 301 unknown words (34 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Removed 2520 too long sentences.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Selecting sentences from 0 to 80 ...\n",
      "\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Loading data from data/enfrde/processed/valid.en-fr.en.pth ...\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - 66317 words (102 unique) in 500 sentences. 27 unknown words (12 unique) covering 0.04% of the data.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Loading data from data/enfrde/processed/valid.en-fr.fr.pth ...\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - 74507 words (102 unique) in 500 sentences. 30 unknown words (11 unique) covering 0.04% of the data.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Removed 310 too long sentences.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Selecting sentences from 0 to 10 ...\n",
      "\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Loading data from data/enfrde/processed/test.en-fr.en.pth ...\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - 67592 words (102 unique) in 500 sentences. 29 unknown words (7 unique) covering 0.04% of the data.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Loading data from data/enfrde/processed/test.en-fr.fr.pth ...\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - 75485 words (102 unique) in 500 sentences. 31 unknown words (10 unique) covering 0.04% of the data.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Removed 325 too long sentences.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Selecting sentences from 0 to 10 ...\n",
      "\n",
      "\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Monolingual data   - train -           en:       428\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Monolingual data   - valid -           en:       174\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Monolingual data   -  test -           en:       180\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Monolingual data   - train -           fr:       201\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Monolingual data   - valid -           fr:       234\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Monolingual data   -  test -           fr:        25\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Parallel data      - train -        en-fr:        80\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Parallel data      - valid -        en-fr:        10\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Parallel data      -  test -        en-fr:        10\n",
      "\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - ============ langs: de, fr\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - ============ Monolingual data (de)\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Loading data from data/enfrde/processed/train.de.pth ...\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - 2182826 words (102 unique) in 14003 sentences. 1025 unknown words (36 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Selecting batches from 0 to 24 ...\n",
      "\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Loading data from data/enfrde/processed/valid.de.pth ...\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - 1629719 words (102 unique) in 10502 sentences. 754 unknown words (36 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Selecting batches from 0 to 6 ...\n",
      "\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Loading data from data/enfrde/processed/test.de.pth ...\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - 1630697 words (102 unique) in 10502 sentences. 755 unknown words (36 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Selecting batches from 0 to 6 ...\n",
      "\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - ============ Monolingual data (fr)\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Loading data from data/enfrde/processed/train.fr.pth ...\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - 2182826 words (102 unique) in 14003 sentences. 1025 unknown words (36 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Selecting batches from 0 to 24 ...\n",
      "\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Loading data from data/enfrde/processed/valid.fr.pth ...\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - 1629719 words (102 unique) in 10502 sentences. 754 unknown words (36 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Selecting batches from 0 to 6 ...\n",
      "\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Loading data from data/enfrde/processed/test.fr.pth ...\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - 1630697 words (102 unique) in 10502 sentences. 755 unknown words (36 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Selecting batches from 0 to 6 ...\n",
      "\n",
      "\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - ============ Parallel data (de-fr)\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Loading data from data/enfrde/processed/train.de-fr.de.pth ...\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - 627614 words (102 unique) in 4001 sentences. 301 unknown words (34 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Loading data from data/enfrde/processed/train.de-fr.fr.pth ...\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - 627614 words (102 unique) in 4001 sentences. 301 unknown words (34 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Removed 2473 too long sentences.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Selecting sentences from 0 to 80 ...\n",
      "\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Loading data from data/enfrde/processed/valid.de-fr.de.pth ...\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - 74507 words (102 unique) in 500 sentences. 30 unknown words (11 unique) covering 0.04% of the data.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Loading data from data/enfrde/processed/valid.de-fr.fr.pth ...\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - 74507 words (102 unique) in 500 sentences. 30 unknown words (11 unique) covering 0.04% of the data.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Removed 304 too long sentences.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Selecting sentences from 0 to 10 ...\n",
      "\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Loading data from data/enfrde/processed/test.de-fr.de.pth ...\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - 75485 words (102 unique) in 500 sentences. 31 unknown words (10 unique) covering 0.04% of the data.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Loading data from data/enfrde/processed/test.de-fr.fr.pth ...\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - 75485 words (102 unique) in 500 sentences. 31 unknown words (10 unique) covering 0.04% of the data.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Removed 318 too long sentences.\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Selecting sentences from 0 to 10 ...\n",
      "\n",
      "\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Monolingual data   - train -           de:       201\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Monolingual data   - valid -           de:       234\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Monolingual data   -  test -           de:        25\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Monolingual data   - train -           fr:       201\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Monolingual data   - valid -           fr:       234\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Monolingual data   -  test -           fr:        25\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Parallel data      - train -        de-fr:        80\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Parallel data      - valid -        de-fr:        10\n",
      "INFO - 05/14/20 14:44:12 - 0:00:00 - Parallel data      -  test -        de-fr:        10\n",
      "\n",
      "INFO - 05/14/20 14:44:13 - 0:00:01 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(102, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=102, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/14/20 14:44:13 - 0:00:01 - Number of parameters (model): 76210278\n",
      "INFO - 05/14/20 14:44:16 - 0:00:04 - Found 0 memories.\n",
      "INFO - 05/14/20 14:44:16 - 0:00:04 - Found 6 FFN.\n",
      "INFO - 05/14/20 14:44:16 - 0:00:04 - Found 102 parameters in model.\n",
      "INFO - 05/14/20 14:44:16 - 0:00:04 - Optimizers: model\n",
      "INFO - 05/14/20 14:44:16 - 0:00:04 - ============ Starting epoch 0 ... ============\n",
      "INFO - 05/14/20 14:44:16 - 0:00:04 - Creating new training data iterator (pred,en) ...\n",
      "INFO - 05/14/20 14:44:16 - 0:00:04 - Creating new training data iterator (pred,fr) ...\n",
      "INFO - 05/14/20 14:44:17 - 0:00:05 - Creating new training data iterator (pred,en,fr) ...\n",
      "INFO - 05/14/20 14:44:17 - 0:00:05 - Creating new training data iterator (pred,de,fr) ...\n",
      "INFO - 05/14/20 14:44:21 - 0:00:09 - task : en-fr ||      5 -    6.13 sent/s -   195.06 words/s - MLM-en:  5.6747 || MLM-fr:  4.3771 || MLM-en-fr:  4.3225 -  - model LR: 1.0000e-04\n",
      "INFO - 05/14/20 14:44:21 - 0:00:09 - task : de-fr ||      5 - 39211.32 sent/s - 1271753.75 words/s - MLM-fr:  3.8795 || MLM-de-fr:  3.8948 -  - model LR: 1.0000e-04\n",
      "INFO - 05/14/20 14:44:23 - 0:00:11 - Creating new training data iterator (pred,fr) ...\n",
      "INFO - 05/14/20 14:44:25 - 0:00:13 - task : en-fr ||     10 -    7.17 sent/s -   456.71 words/s - MLM-en:  3.8105 || MLM-fr:  3.5568 || MLM-en-fr:  3.7671 -  - model LR: 1.0000e-04\n",
      "INFO - 05/14/20 14:44:25 - 0:00:13 - task : de-fr ||     10 - 43003.80 sent/s - 2775178.59 words/s - MLM-fr:  3.7775 || MLM-de-fr:  3.7411 -  - model LR: 1.0000e-04\n",
      "INFO - 05/14/20 14:44:29 - 0:00:17 - task : en-fr ||     15 -    7.34 sent/s -   697.92 words/s - MLM-en:  3.5278 || MLM-fr:  3.6276 || MLM-en-fr:  3.6407 -  - model LR: 1.0000e-04\n",
      "INFO - 05/14/20 14:44:29 - 0:00:17 - task : de-fr ||     15 - 43965.45 sent/s - 4052149.04 words/s - MLM-fr:  3.4819 || MLM-de-fr:  4.0262 -  - model LR: 1.0000e-04\n",
      "INFO - 05/14/20 14:44:30 - 0:00:18 - Creating new training data iterator (pred,fr) ...\n",
      "INFO - 05/14/20 14:44:30 - 0:00:18 - ============ End of epoch 0 ============\n",
      "INFO - 05/14/20 14:44:31 - 0:00:19 - epoch -> 0.000000\n",
      "INFO - 05/14/20 14:44:31 - 0:00:19 - valid_en_mlm_ppl -> 38.925891\n",
      "INFO - 05/14/20 14:44:31 - 0:00:19 - valid_en_mlm_acc -> 7.500000\n",
      "INFO - 05/14/20 14:44:31 - 0:00:19 - valid_fr_mlm_ppl -> 38.737265\n",
      "INFO - 05/14/20 14:44:31 - 0:00:19 - valid_fr_mlm_acc -> 6.666667\n",
      "INFO - 05/14/20 14:44:31 - 0:00:19 - valid_en_fr_mlm_ppl -> 55.573276\n",
      "INFO - 05/14/20 14:44:31 - 0:00:19 - valid_en_fr_mlm_acc -> 7.831325\n",
      "INFO - 05/14/20 14:44:31 - 0:00:19 - valid_mlm_ppl -> 38.740314\n",
      "INFO - 05/14/20 14:44:31 - 0:00:19 - valid_mlm_acc -> 6.666667\n",
      "INFO - 05/14/20 14:44:31 - 0:00:19 - valid_de_mlm_ppl -> 38.743363\n",
      "INFO - 05/14/20 14:44:31 - 0:00:19 - valid_de_mlm_acc -> 6.666667\n",
      "INFO - 05/14/20 14:44:31 - 0:00:19 - valid_de_fr_mlm_ppl -> 46.391135\n",
      "INFO - 05/14/20 14:44:31 - 0:00:19 - valid_de_fr_mlm_acc -> 10.555556\n",
      "INFO - 05/14/20 14:44:31 - 0:00:19 - test_en_mlm_ppl -> 56.061920\n",
      "INFO - 05/14/20 14:44:31 - 0:00:19 - test_en_mlm_acc -> 6.500000\n",
      "INFO - 05/14/20 14:44:31 - 0:00:19 - test_fr_mlm_ppl -> 37.006949\n",
      "INFO - 05/14/20 14:44:31 - 0:00:19 - test_fr_mlm_acc -> 10.000000\n",
      "INFO - 05/14/20 14:44:31 - 0:00:19 - test_en_fr_mlm_ppl -> 42.601091\n",
      "INFO - 05/14/20 14:44:31 - 0:00:19 - test_en_fr_mlm_acc -> 7.100592\n",
      "INFO - 05/14/20 14:44:31 - 0:00:19 - test_mlm_ppl -> 37.105886\n",
      "INFO - 05/14/20 14:44:31 - 0:00:19 - test_mlm_acc -> 10.000000\n",
      "INFO - 05/14/20 14:44:31 - 0:00:19 - test_de_mlm_ppl -> 37.204823\n",
      "INFO - 05/14/20 14:44:31 - 0:00:19 - test_de_mlm_acc -> 10.000000\n",
      "INFO - 05/14/20 14:44:31 - 0:00:19 - test_de_fr_mlm_ppl -> 35.537298\n",
      "INFO - 05/14/20 14:44:31 - 0:00:19 - test_de_fr_mlm_acc -> 10.447761\n",
      "INFO - 05/14/20 14:44:31 - 0:00:19 - __log__:{\"epoch\": 0, \"valid_en_mlm_ppl\": 38.925890685133204, \"valid_en_mlm_acc\": 7.5, \"valid_fr_mlm_ppl\": 38.73726544670293, \"valid_fr_mlm_acc\": 6.666666666666667, \"valid_en_fr_mlm_ppl\": 55.573275629736216, \"valid_en_fr_mlm_acc\": 7.831325301204819, \"valid_mlm_ppl\": 38.74031421267563, \"valid_mlm_acc\": 6.666666666666667, \"valid_de_mlm_ppl\": 38.74336297864832, \"valid_de_mlm_acc\": 6.666666666666667, \"valid_de_fr_mlm_ppl\": 46.39113533593849, \"valid_de_fr_mlm_acc\": 10.555555555555555, \"test_en_mlm_ppl\": 56.06192029926272, \"test_en_mlm_acc\": 6.5, \"test_fr_mlm_ppl\": 37.00694931163074, \"test_fr_mlm_acc\": 10.0, \"test_en_fr_mlm_ppl\": 42.601091038955836, \"test_en_fr_mlm_acc\": 7.100591715976331, \"test_mlm_ppl\": 37.1058860137954, \"test_mlm_acc\": 10.0, \"test_de_mlm_ppl\": 37.204822715960056, \"test_de_mlm_acc\": 10.0, \"test_de_fr_mlm_ppl\": 35.537298052891096, \"test_de_fr_mlm_acc\": 10.447761194029852}\n",
      "INFO - 05/14/20 14:44:31 - 0:00:19 - New best score for valid_mlm_ppl: 38.740314\n",
      "INFO - 05/14/20 14:44:31 - 0:00:19 - Saving best-valid_mlm_ppl to ./dumped/mlm_enfrde/maml/best-valid_mlm_ppl.pth ...\n",
      "WARNING - 05/14/20 14:44:31 - 0:00:19 - Saving model parameters ...\n",
      "INFO - 05/14/20 14:44:32 - 0:00:20 - New best validation score: 38.740314\n",
      "INFO - 05/14/20 14:44:32 - 0:00:20 - Saving checkpoint to ./dumped/mlm_enfrde/maml/checkpoint.pth ...\n",
      "WARNING - 05/14/20 14:44:32 - 0:00:20 - Saving model parameters ...\n",
      "WARNING - 05/14/20 14:44:32 - 0:00:20 - Saving model optimizer ...\n",
      "INFO - 05/14/20 14:44:32 - 0:00:20 - ============= Collecting 0 ...\n"
     ]
    }
   ],
   "source": [
    "# MLM\n",
    "#%env mlm_steps=en,fr|en,de|de,fr\n",
    "# MLM + TML\n",
    "#%env mlm_steps=en,fr,en-fr|en,de,en-de|de,fr,de-fr\n",
    "%env mlm_steps=en,fr,en-fr|de,fr,de-fr\n",
    "\n",
    "! python train.py --exp_name mlm_enfrde --exp_id $exp_id --dump_path ./dumped/ --data_path $OUTPATH --lgs $lgs --clm_steps '' --mlm_steps $mlm_steps --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout 0.1 --attention_dropout 0.1 --gelu_activation true --batch_size $batch_size --bptt 256 --optimizer adam,lr=0.0001 --epoch_size $epoch_size --max_epoch $max_epoch --validation_metrics _valid_mlm_ppl --stopping_criterion $stopping_criterion --eval_bleu $eval_bleu --remove_long_sentences_train $remove_long_sentences_train --remove_long_sentences_valid $remove_long_sentences_valid --remove_long_sentences_test $remove_long_sentences_test --train_n_samples $train_n_samples --valid_n_samples $valid_n_samples --test_n_samples $test_n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a (unsupervised/supervised) MT from a pretrained meta-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: lgs=en-fr|de-fr\n",
      "env: eval_bleu=true\n"
     ]
    }
   ],
   "source": [
    "#%env batch_size=...\n",
    "#%env epoch_size=...\n",
    "\n",
    "%env lgs=en-fr|de-fr\n",
    "\n",
    "%env eval_bleu=true\n",
    "! chmod +x src/evaluation/multi-bleu.perl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: stopping_criterion=valid_en-fr_mt_bleu,5\n",
      "env: validation_metrics=valid_en-fr_mt_bleu\n",
      "env: reload_model=dumped/mlm_enfrde/maml/best-valid_mlm_ppl.pth,dumped/mlm_enfrde/maml/best-valid_mlm_ppl.pth\n",
      "env: ae_steps=en,fr|de,fr\n",
      "env: bt_steps=en-fr-en,fr-en-fr|de-fr-de,fr-de-fr\n"
     ]
    }
   ],
   "source": [
    "%env stopping_criterion=valid_en-fr_mt_bleu,5\n",
    "%env validation_metrics=valid_en-fr_mt_bleu\n",
    "%env reload_model=dumped/mlm_enfrde/maml/best-valid_mlm_ppl.pth,dumped/mlm_enfrde/maml/best-valid_mlm_ppl.pth\n",
    "%env ae_steps=en,fr|de,fr\n",
    "%env bt_steps=en-fr-en,fr-en-fr|de-fr-de,fr-de-fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: train_n_samples=8\n",
      "env: valid_n_samples=2\n",
      "env: test_n_samples=2\n",
      "env: max_epoch=1\n",
      "env: batch_size=1\n",
      "env: epoch_size=20\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: [['en', 'fr'], ['de', 'fr']]\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 1\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: ['de', 'fr']\n",
      "                                     bt_steps: [[('en', 'fr', 'en'), ('fr', 'en', 'fr')], [('de', 'fr', 'de'), ('fr', 'de', 'fr')]]\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: [[], []]\n",
      "                                     command: python train.py --exp_name meta_MT_enfrde --exp_id maml --dump_path './dumped/' --reload_model 'dumped/mlm_enfrde/maml/best-valid_mlm_ppl.pth,dumped/mlm_enfrde/maml/best-valid_mlm_ppl.pth' --data_path 'data/enfrde/processed' --lgs 'en-fr|de-fr' --ae_steps 'en,fr|de,fr' --bt_steps 'en-fr-en,fr-en-fr|de-fr-de,fr-de-fr' --word_shuffle 3 --word_dropout '0.1' --word_blank '0.1' --lambda_ae '0:1,100000:0.1,300000:0' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --tokens_per_batch 2000 --batch_size 1 --bptt 256 --optimizer 'adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001' --epoch_size 20 --max_epoch 1 --eval_bleu true --stopping_criterion 'valid_en-fr_mt_bleu,5' --validation_metrics 'valid_en-fr_mt_bleu' --train_n_samples 8 --valid_n_samples 2 --test_n_samples 2 --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --exp_id \"maml\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: data/enfrde/processed\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: ./dumped/meta_MT_enfrde/maml\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: False\n",
      "                                     epoch_size: 20\n",
      "                                     eval_bleu: True\n",
      "                                     eval_only: False\n",
      "                                     exp_id: maml\n",
      "                                     exp_name: meta_MT_enfrde\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'de', 1: 'fr'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 0:1,100000:0.1,300000:0\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'de': 0, 'fr': 1}\n",
      "                                     langs: [['en', 'fr'], ['de', 'fr']]\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['en-fr', 'de-fr']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 1\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: True\n",
      "                                     meta_params: ...\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [[], []]\n",
      "                                     mono_dataset: {'de': {'train': 'data/enfrde/processed/train.de.pth', 'valid': 'data/enfrde/processed/valid.de.pth', 'test': 'data/enfrde/processed/test.de.pth'}, 'fr': {'train': 'data/enfrde/processed/train.fr.pth', 'valid': 'data/enfrde/processed/valid.fr.pth', 'test': 'data/enfrde/processed/test.fr.pth'}}\n",
      "                                     mt_steps: [[], []]\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': 8, 'valid': 2, 'test': 2}\n",
      "                                     n_task: 2\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001\n",
      "                                     para_dataset: {('de', 'fr'): {'valid': ('data/enfrde/processed/valid.de-fr.de.pth', 'data/enfrde/processed/valid.de-fr.fr.pth'), 'test': ('data/enfrde/processed/test.de-fr.de.pth', 'data/enfrde/processed/test.de-fr.fr.pth')}}\n",
      "                                     pc_steps: [[], []]\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: dumped/mlm_enfrde/maml/best-valid_mlm_ppl.pth,dumped/mlm_enfrde/maml/best-valid_mlm_ppl.pth\n",
      "                                     remove_long_sentences: {'train': True, 'valid': True, 'test': True}\n",
      "                                     remove_long_sentences_test: True\n",
      "                                     remove_long_sentences_train: True\n",
      "                                     remove_long_sentences_valid: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: valid_en-fr_mt_bleu,5\n",
      "                                     test_n_samples: 2\n",
      "                                     tokens_per_batch: 2000\n",
      "                                     train_n_samples: 8\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: 2\n",
      "                                     validation_metrics: valid_en-fr_mt_bleu\n",
      "                                     word_blank: 0.1\n",
      "                                     word_dropout: 0.1\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 3.0\n",
      "                                     world_size: 1\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - The experiment will be stored in ./dumped/meta_MT_enfrde/maml\n",
      "                                     \n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Running command: python train.py --exp_name meta_MT_enfrde --exp_id maml --dump_path './dumped/' --reload_model 'dumped/mlm_enfrde/maml/best-valid_mlm_ppl.pth,dumped/mlm_enfrde/maml/best-valid_mlm_ppl.pth' --data_path 'data/enfrde/processed' --lgs 'en-fr|de-fr' --ae_steps 'en,fr|de,fr' --bt_steps 'en-fr-en,fr-en-fr|de-fr-de,fr-de-fr' --word_shuffle 3 --word_dropout '0.1' --word_blank '0.1' --lambda_ae '0:1,100000:0.1,300000:0' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --tokens_per_batch 2000 --batch_size 1 --bptt 256 --optimizer 'adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001' --epoch_size 20 --max_epoch 1 --eval_bleu true --stopping_criterion 'valid_en-fr_mt_bleu,5' --validation_metrics 'valid_en-fr_mt_bleu' --train_n_samples 8 --valid_n_samples 2 --test_n_samples 2 --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True\n",
      "\n",
      "WARNING - 05/14/20 15:16:33 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - ============ langs: en, fr\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - ============ Monolingual data (en)\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Loading data from data/enfrde/processed/train.en.pth ...\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - 2000879 words (102 unique) in 14003 sentences. 1008 unknown words (44 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Selecting batches from 0 to 4 ...\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Removed 8204 too long sentences.\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Selecting sentences from 0 to 8 ...\n",
      "\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Loading data from data/enfrde/processed/valid.en.pth ...\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - 1522514 words (102 unique) in 10502 sentences. 740 unknown words (44 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Selecting batches from 0 to 1 ...\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Removed 6201 too long sentences.\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Selecting sentences from 0 to 2 ...\n",
      "\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Loading data from data/enfrde/processed/test.en.pth ...\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - 1523789 words (102 unique) in 10502 sentences. 742 unknown words (44 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Selecting batches from 0 to 1 ...\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Removed 6224 too long sentences.\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Selecting sentences from 0 to 2 ...\n",
      "\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - ============ Monolingual data (fr)\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Loading data from data/enfrde/processed/train.fr.pth ...\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - 2182826 words (102 unique) in 14003 sentences. 1025 unknown words (36 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Selecting batches from 0 to 4 ...\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Removed 8663 too long sentences.\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Selecting sentences from 0 to 8 ...\n",
      "\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Loading data from data/enfrde/processed/valid.fr.pth ...\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - 1629719 words (102 unique) in 10502 sentences. 754 unknown words (36 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Selecting batches from 0 to 1 ...\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Removed 6494 too long sentences.\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Selecting sentences from 0 to 2 ...\n",
      "\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Loading data from data/enfrde/processed/test.fr.pth ...\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - 1630697 words (102 unique) in 10502 sentences. 755 unknown words (36 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Selecting batches from 0 to 1 ...\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Removed 6508 too long sentences.\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Selecting sentences from 0 to 2 ...\n",
      "\n",
      "\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - ============ Parallel data (en-fr)\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Loading data from data/enfrde/processed/valid.en-fr.en.pth ...\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - 66317 words (102 unique) in 500 sentences. 27 unknown words (12 unique) covering 0.04% of the data.\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Loading data from data/enfrde/processed/valid.en-fr.fr.pth ...\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - 74507 words (102 unique) in 500 sentences. 30 unknown words (11 unique) covering 0.04% of the data.\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Removed 310 too long sentences.\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Selecting sentences from 0 to 2 ...\n",
      "\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Loading data from data/enfrde/processed/test.en-fr.en.pth ...\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - 67592 words (102 unique) in 500 sentences. 29 unknown words (7 unique) covering 0.04% of the data.\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Loading data from data/enfrde/processed/test.en-fr.fr.pth ...\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - 75485 words (102 unique) in 500 sentences. 31 unknown words (10 unique) covering 0.04% of the data.\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Removed 325 too long sentences.\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Selecting sentences from 0 to 2 ...\n",
      "\n",
      "\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Monolingual data   - train -           en:       102\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Monolingual data   - valid -           en:       169\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Monolingual data   -  test -           en:       174\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Monolingual data   - train -           fr:       169\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Monolingual data   - valid -           fr:       228\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Monolingual data   -  test -           fr:        19\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Parallel data      - valid -        en-fr:         2\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Parallel data      -  test -        en-fr:         2\n",
      "\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - ============ langs: de, fr\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - ============ Monolingual data (de)\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Loading data from data/enfrde/processed/train.de.pth ...\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - 2182826 words (102 unique) in 14003 sentences. 1025 unknown words (36 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Selecting batches from 0 to 4 ...\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Removed 8663 too long sentences.\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Selecting sentences from 0 to 8 ...\n",
      "\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Loading data from data/enfrde/processed/valid.de.pth ...\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - 1629719 words (102 unique) in 10502 sentences. 754 unknown words (36 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Selecting batches from 0 to 1 ...\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Removed 6494 too long sentences.\n",
      "INFO - 05/14/20 15:16:33 - 0:00:00 - Selecting sentences from 0 to 2 ...\n",
      "\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - Loading data from data/enfrde/processed/test.de.pth ...\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - 1630697 words (102 unique) in 10502 sentences. 755 unknown words (36 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - Selecting batches from 0 to 1 ...\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - Removed 6508 too long sentences.\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - Selecting sentences from 0 to 2 ...\n",
      "\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - ============ Monolingual data (fr)\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - Loading data from data/enfrde/processed/train.fr.pth ...\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - 2182826 words (102 unique) in 14003 sentences. 1025 unknown words (36 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - Selecting batches from 0 to 4 ...\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - Removed 8663 too long sentences.\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - Selecting sentences from 0 to 8 ...\n",
      "\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - Loading data from data/enfrde/processed/valid.fr.pth ...\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - 1629719 words (102 unique) in 10502 sentences. 754 unknown words (36 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - Selecting batches from 0 to 1 ...\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - Removed 6494 too long sentences.\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - Selecting sentences from 0 to 2 ...\n",
      "\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - Loading data from data/enfrde/processed/test.fr.pth ...\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - 1630697 words (102 unique) in 10502 sentences. 755 unknown words (36 unique) covering 0.05% of the data.\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - Selecting batches from 0 to 1 ...\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - Removed 6508 too long sentences.\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - Selecting sentences from 0 to 2 ...\n",
      "\n",
      "\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - ============ Parallel data (de-fr)\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - Loading data from data/enfrde/processed/valid.de-fr.de.pth ...\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - 74507 words (102 unique) in 500 sentences. 30 unknown words (11 unique) covering 0.04% of the data.\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - Loading data from data/enfrde/processed/valid.de-fr.fr.pth ...\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - 74507 words (102 unique) in 500 sentences. 30 unknown words (11 unique) covering 0.04% of the data.\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - Removed 304 too long sentences.\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - Selecting sentences from 0 to 2 ...\n",
      "\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - Loading data from data/enfrde/processed/test.de-fr.de.pth ...\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - 75485 words (102 unique) in 500 sentences. 31 unknown words (10 unique) covering 0.04% of the data.\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - Loading data from data/enfrde/processed/test.de-fr.fr.pth ...\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - 75485 words (102 unique) in 500 sentences. 31 unknown words (10 unique) covering 0.04% of the data.\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - Removed 0 empty sentences.\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - Removed 318 too long sentences.\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - Selecting sentences from 0 to 2 ...\n",
      "\n",
      "\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - ============ Data summary\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - Monolingual data   - train -           de:       169\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - Monolingual data   - valid -           de:       228\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - Monolingual data   -  test -           de:        19\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - Monolingual data   - train -           fr:       169\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - Monolingual data   - valid -           fr:       228\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - Monolingual data   -  test -           fr:        19\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - Parallel data      - valid -        de-fr:         2\n",
      "INFO - 05/14/20 15:16:34 - 0:00:00 - Parallel data      -  test -        de-fr:         2\n",
      "\n",
      "INFO - 05/14/20 15:16:35 - 0:00:01 - Reloading encoder from dumped/mlm_enfrde/maml/best-valid_mlm_ppl.pth ...\n",
      "INFO - 05/14/20 15:16:38 - 0:00:05 - Reloading decoder from dumped/mlm_enfrde/maml/best-valid_mlm_ppl.pth ...\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter layer_norm15.0.weight not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter layer_norm15.0.bias not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.0.q_lin.weight not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.0.q_lin.bias not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.0.k_lin.weight not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.0.k_lin.bias not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.0.v_lin.weight not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.0.v_lin.bias not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.0.out_lin.weight not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.0.out_lin.bias not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter layer_norm15.1.weight not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter layer_norm15.1.bias not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.1.q_lin.weight not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.1.q_lin.bias not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.1.k_lin.weight not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.1.k_lin.bias not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.1.v_lin.weight not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.1.v_lin.bias not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.1.out_lin.weight not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.1.out_lin.bias not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter layer_norm15.2.weight not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter layer_norm15.2.bias not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.2.q_lin.weight not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.2.q_lin.bias not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.2.k_lin.weight not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.2.k_lin.bias not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.2.v_lin.weight not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.2.v_lin.bias not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.2.out_lin.weight not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.2.out_lin.bias not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter layer_norm15.3.weight not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter layer_norm15.3.bias not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.3.q_lin.weight not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.3.q_lin.bias not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.3.k_lin.weight not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.3.k_lin.bias not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.3.v_lin.weight not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.3.v_lin.bias not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.3.out_lin.weight not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.3.out_lin.bias not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter layer_norm15.4.weight not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter layer_norm15.4.bias not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.4.q_lin.weight not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.4.q_lin.bias not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.4.k_lin.weight not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.4.k_lin.bias not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.4.v_lin.weight not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.4.v_lin.bias not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.4.out_lin.weight not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.4.out_lin.bias not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter layer_norm15.5.weight not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter layer_norm15.5.bias not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.5.q_lin.weight not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.5.q_lin.bias not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.5.k_lin.weight not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.5.k_lin.bias not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.5.v_lin.weight not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.5.v_lin.bias not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.5.out_lin.weight not found.\n",
      "WARNING - 05/14/20 15:16:38 - 0:00:05 - Parameter encoder_attn.5.out_lin.bias not found.\n",
      "INFO - 05/14/20 15:16:38 - 0:00:05 - Number of parameters (encoder): 76210278\n",
      "INFO - 05/14/20 15:16:38 - 0:00:05 - Number of parameters (decoder): 101412966\n",
      "INFO - 05/14/20 15:16:38 - 0:00:05 - Found 0 memories.\n",
      "INFO - 05/14/20 15:16:38 - 0:00:05 - Found 12 FFN.\n",
      "INFO - 05/14/20 15:16:38 - 0:00:05 - Found 264 parameters in model.\n",
      "INFO - 05/14/20 15:16:38 - 0:00:05 - Optimizers: model\n",
      "INFO - 05/14/20 15:16:39 - 0:00:05 - ============ Starting epoch 0 ... ============\n",
      "INFO - 05/14/20 15:16:39 - 0:00:05 - Creating new training data iterator (ae,en) ...\n",
      "INFO - 05/14/20 15:16:39 - 0:00:06 - Creating new training data iterator (ae,fr) ...\n",
      "INFO - 05/14/20 15:16:40 - 0:00:06 - Creating new training data iterator (ae,de) ...\n",
      "INFO - 05/14/20 15:16:40 - 0:00:07 - Creating new training data iterator (ae,fr) ...\n",
      "INFO - 05/14/20 15:16:41 - 0:00:08 - Creating new training data iterator (ae,en) ...\n",
      "INFO - 05/14/20 15:16:41 - 0:00:08 - Creating new training data iterator (ae,fr) ...\n",
      "INFO - 05/14/20 15:16:42 - 0:00:08 - Creating new training data iterator (ae,de) ...\n",
      "INFO - 05/14/20 15:16:42 - 0:00:09 - Creating new training data iterator (ae,fr) ...\n",
      "INFO - 05/14/20 15:16:43 - 0:00:10 - Creating new training data iterator (ae,en) ...\n",
      "INFO - 05/14/20 15:16:43 - 0:00:10 - Creating new training data iterator (ae,fr) ...\n",
      "INFO - 05/14/20 15:16:44 - 0:00:10 - Creating new training data iterator (ae,de) ...\n",
      "INFO - 05/14/20 15:16:44 - 0:00:10 - Creating new training data iterator (ae,fr) ...\n",
      "INFO - 05/14/20 15:16:45 - 0:00:11 - Creating new training data iterator (ae,en) ...\n",
      "INFO - 05/14/20 15:16:45 - 0:00:12 - Creating new training data iterator (ae,fr) ...\n",
      "INFO - 05/14/20 15:16:45 - 0:00:12 - Creating new training data iterator (ae,de) ...\n",
      "INFO - 05/14/20 15:16:46 - 0:00:12 - Creating new training data iterator (ae,fr) ...\n",
      "INFO - 05/14/20 15:16:47 - 0:00:13 - Creating new training data iterator (ae,en) ...\n",
      "INFO - 05/14/20 15:16:47 - 0:00:14 - Creating new training data iterator (ae,fr) ...\n",
      "INFO - 05/14/20 15:16:47 - 0:00:14 - Creating new training data iterator (ae,de) ...\n",
      "INFO - 05/14/20 15:16:48 - 0:00:14 - Creating new training data iterator (ae,fr) ...\n",
      "INFO - 05/14/20 15:16:49 - 0:00:15 - task : en-fr ||      5 -    7.93 sent/s -   429.51 words/s - AE-en:  3.9584 || AE-fr:  3.5905 -  - model LR: 7.2438e-07\n",
      "INFO - 05/14/20 15:16:49 - 0:00:15 - task : de-fr ||      5 - 84307.62 sent/s - 3835996.62 words/s - AE-de:  3.5949 || AE-fr:  3.5953 -  - model LR: 7.2438e-07\n",
      "INFO - 05/14/20 15:16:49 - 0:00:15 - Creating new training data iterator (ae,en) ...\n",
      "INFO - 05/14/20 15:16:49 - 0:00:16 - Creating new training data iterator (ae,fr) ...\n",
      "INFO - 05/14/20 15:16:49 - 0:00:16 - Creating new training data iterator (ae,de) ...\n",
      "INFO - 05/14/20 15:16:50 - 0:00:16 - Creating new training data iterator (ae,fr) ...\n",
      "INFO - 05/14/20 15:16:50 - 0:00:17 - Creating new training data iterator (ae,en) ...\n",
      "INFO - 05/14/20 15:16:51 - 0:00:17 - Creating new training data iterator (ae,fr) ...\n",
      "INFO - 05/14/20 15:16:51 - 0:00:18 - Creating new training data iterator (ae,de) ...\n",
      "INFO - 05/14/20 15:16:51 - 0:00:18 - Creating new training data iterator (ae,fr) ...\n",
      "INFO - 05/14/20 15:16:52 - 0:00:19 - Creating new training data iterator (ae,en) ...\n",
      "INFO - 05/14/20 15:16:53 - 0:00:19 - Creating new training data iterator (ae,fr) ...\n",
      "INFO - 05/14/20 15:16:53 - 0:00:20 - Creating new training data iterator (ae,de) ...\n",
      "INFO - 05/14/20 15:16:53 - 0:00:20 - Creating new training data iterator (ae,fr) ...\n",
      "INFO - 05/14/20 15:16:54 - 0:00:21 - Creating new training data iterator (ae,en) ...\n",
      "INFO - 05/14/20 15:16:55 - 0:00:21 - Creating new training data iterator (ae,fr) ...\n",
      "INFO - 05/14/20 15:16:55 - 0:00:22 - Creating new training data iterator (ae,de) ...\n",
      "INFO - 05/14/20 15:16:55 - 0:00:22 - Creating new training data iterator (ae,fr) ...\n",
      "INFO - 05/14/20 15:16:56 - 0:00:23 - Creating new training data iterator (ae,en) ...\n",
      "INFO - 05/14/20 15:16:56 - 0:00:23 - Creating new training data iterator (ae,fr) ...\n",
      "INFO - 05/14/20 15:16:57 - 0:00:23 - Creating new training data iterator (ae,de) ...\n",
      "INFO - 05/14/20 15:16:57 - 0:00:24 - Creating new training data iterator (ae,fr) ...\n",
      "INFO - 05/14/20 15:16:58 - 0:00:25 - task : en-fr ||     10 -    8.44 sent/s -   915.05 words/s - AE-en:  3.8902 || AE-fr:  3.5324 -  - model LR: 1.3488e-06\n",
      "INFO - 05/14/20 15:16:58 - 0:00:25 - task : de-fr ||     10 - 77350.00 sent/s - 7038850.42 words/s - AE-de:  3.5254 || AE-fr:  3.5124 -  - model LR: 1.3488e-06\n",
      "INFO - 05/14/20 15:16:58 - 0:00:25 - ============ End of epoch 0 ============\n",
      "INFO - 05/14/20 15:16:58 - 0:00:25 - epoch -> 0.000000\n",
      "INFO - 05/14/20 15:16:58 - 0:00:25 - __log__:{\"epoch\": 0}\n",
      "WARNING - 05/14/20 15:16:58 - 0:00:25 - Metric \"valid_en-fr_mt_bleu\" not found in scores!\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 550, in <module>\n",
      "    main(params)\n",
      "  File \"train.py\", line 440, in main\n",
      "    trainer.end_epoch(scores)\n",
      "  File \"/home/jupyter/meta_XLM/XLM/src/trainer.py\", line 715, in end_epoch\n",
      "    assert metric in scores, metric\n",
      "AssertionError: valid_en-fr_mt_bleu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%env train_n_samples=8\n",
    "%env valid_n_samples=2\n",
    "%env test_n_samples=2\n",
    "\n",
    "%env max_epoch=1   \n",
    "%env batch_size=1\n",
    "%env epoch_size=20\n",
    "\n",
    "# unsupervised MT\n",
    "! python train.py --exp_name meta_MT_enfrde --exp_id $exp_id --dump_path ./dumped/ --reload_model $reload_model --data_path $OUTPATH --lgs $lgs --ae_steps $ae_steps --bt_steps $bt_steps --word_shuffle 3 --word_dropout 0.1 --word_blank 0.1 --lambda_ae '0:1,100000:0.1,300000:0' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout 0.1 --attention_dropout 0.1 --gelu_activation true --tokens_per_batch 2000 --batch_size $batch_size --bptt 256 --optimizer adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001 --epoch_size $epoch_size --max_epoch $max_epoch --eval_bleu $eval_bleu --stopping_criterion $stopping_criterion --validation_metrics $validation_metrics --train_n_samples $train_n_samples --valid_n_samples $valid_n_samples --test_n_samples $test_n_samples --remove_long_sentences_train $remove_long_sentences_train --remove_long_sentences_valid $remove_long_sentences_valid --remove_long_sentences_test $remove_long_sentences_test    \n",
    "\n",
    "# supervised MT\n",
    "#%env mt_steps=en-fr,fr-en|de-fr,fr-de           \n",
    "#! python train.py --exp_name meta_MT_enfrde --exp_id $exp_id  --dump_path ./dumped/ --reload_model $reload_model --data_path $OUTPATH --lgs $lgs --ae_steps $ae_steps --mt_steps $mt_steps --bt_steps $bt_steps --word_shuffle 3 --word_dropout 0.1 --word_blank 0.1 --lambda_ae '0:1,100000:0.1,300000:0' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout 0.1 --attention_dropout 0.1 --gelu_activation true --tokens_per_batch 2000 --batch_size $batch_size --bptt 256 --optimizer adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001 --epoch_size $epoch_size --max_epoch $max_epoch --eval_bleu $eval_bleu --stopping_criterion $stopping_criterion --validation_metrics $validation_metrics --train_n_samples $train_n_samples --valid_n_samples $valid_n_samples --test_n_samples $test_n_samples --remove_long_sentences_train $remove_long_sentences_train --remove_long_sentences_valid $remove_long_sentences_valid --remove_long_sentences_test $remove_long_sentences_test    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune the meta-model on a specific (sub) nmt (meta) task : en-fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%env batch_size=...\n",
    "#%env epoch_size=...\n",
    "\n",
    "%env lgs=en-fr\n",
    "\n",
    "%env eval_bleu=true\n",
    "! chmod +x src/evaluation/multi-bleu.perl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env stopping_criterion=valid_en-fr_mt_bleu,5\n",
    "%env validation_metrics=valid_en-fr_mt_bleu\n",
    "%env reload_model=/dumped/meta_MT_enfrde/maml/todo.pth,/dumped/meta_MT_enfrde/maml/todo.pth\n",
    "%env ae_steps=en,fr\n",
    "%env bt_steps=en-fr-en,fr-en-fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env train_n_samples=8\n",
    "%env valid_n_samples=2\n",
    "%env test_n_samples=2\n",
    "\n",
    "%env max_epoch=1   \n",
    "%env batch_size=1\n",
    "%env epoch_size=20\n",
    "\n",
    "# unsupervised MT\n",
    "! python train.py --exp_name meta_MT_enfr --exp_id $exp_id --dump_path ./dumped/ --reload_model $reload_model --data_path $OUTPATH/fine-tune --lgs $lgs --ae_steps $ae_steps --bt_steps $bt_steps --word_shuffle 3 --word_dropout 0.1 --word_blank 0.1 --lambda_ae '0:1,100000:0.1,300000:0' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout 0.1 --attention_dropout 0.1 --gelu_activation true --tokens_per_batch 2000 --batch_size $batch_size --bptt 256 --optimizer adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001 --epoch_size $epoch_size --max_epoch $max_epoch --eval_bleu $eval_bleu --stopping_criterion $stopping_criterion --validation_metrics $validation_metrics --train_n_samples $train_n_samples --valid_n_samples $valid_n_samples --test_n_samples $test_n_samples #--remove_long_sentences_train $remove_long_sentences_train --remove_long_sentences_valid $remove_long_sentences_valid --remove_long_sentences_test $remove_long_sentences_test  \n",
    "\n",
    "# supervised MT\n",
    "#%env mt_steps=en-fr,fr-en           \n",
    "#! python train.py --exp_name meta_MT_enfr --exp_id $exp_id --dump_path ./dumped/ --reload_model $reload_model --data_path $OUTPATH/fine_tune --lgs $lgs --ae_steps $ae_steps --mt_steps $mt_steps --bt_steps $bt_steps --word_shuffle 3 --word_dropout 0.1 --word_blank 0.1 --lambda_ae '0:1,100000:0.1,300000:0' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout 0.1 --attention_dropout 0.1 --gelu_activation true --tokens_per_batch 2000 --batch_size $batch_size --bptt 256 --optimizer adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001 --epoch_size $epoch_size --max_epoch $max_epoch --eval_bleu $eval_bleu --stopping_criterion $stopping_criterion --validation_metrics $validation_metrics --train_n_samples $train_n_samples --valid_n_samples $valid_n_samples --test_n_samples $test_n_samples #--remove_long_sentences_train $remove_long_sentences_train --remove_long_sentences_valid $remove_long_sentences_valid --remove_long_sentences_test $remove_long_sentences_test      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
