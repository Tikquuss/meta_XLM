{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fUL1x0CGt6IE"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UwIvdQlpADmj"
   },
   "source": [
    "**Référence :** https://github.com/facebookresearch/XLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zacxcSS9HbDd"
   },
   "source": [
    "# **Applications: Supervised / Unsupervised MT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 992,
     "status": "ok",
     "timestamp": 1588653837138,
     "user": {
      "displayName": "Pascal Notsawo",
      "photoUrl": "",
      "userId": "05128058352342027621"
     },
     "user_tz": -120
    },
    "id": "5HR8cardIHFJ",
    "outputId": "93cfd5bc-d63e-4ac1-8f05-2bc292f4853b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(bookmark:HOME) -> /home/jupyter/meta_XLM/XLM\n",
      "/home/jupyter/meta_XLM/XLM\n",
      "env: OUTPATH=/home/XLM/data/30000/es-fr\n"
     ]
    }
   ],
   "source": [
    "%bookmark HOME \"/home/jupyter/meta_XLM/XLM\" \n",
    "%cd -b HOME\n",
    "%env OUTPATH /home/XLM/data/30000/es-fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1237,
     "status": "ok",
     "timestamp": 1588653868380,
     "user": {
      "displayName": "Pascal Notsawo",
      "photoUrl": "",
      "userId": "05128058352342027621"
     },
     "user_tz": -120
    },
    "id": "8mrQhqzNLWQ9",
    "outputId": "ea069a08-d334-4828-a960-e8bb6ca61dc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: exp_id=hypothesis\n",
      "env: batch_size=16\n",
      "env: lgs=es-fr\n",
      "env: max_len=100\n"
     ]
    }
   ],
   "source": [
    "%env exp_id=hypothesis\n",
    "#%env batch_size=32\n",
    "%env batch_size=16\n",
    "%env lgs=es-fr\n",
    "# Maximum length of sentences (after BPE)\n",
    "%env max_len=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 672,
     "status": "error",
     "timestamp": 1588653874834,
     "user": {
      "displayName": "Pascal Notsawo",
      "photoUrl": "",
      "userId": "05128058352342027621"
     },
     "user_tz": -120
    },
    "id": "5gOEASh1dsYz",
    "outputId": "3256bee3-4620-4275-877f-dce28d96d7fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_len = 38067593\n"
     ]
    }
   ],
   "source": [
    "# le plus grand fichier à tensor_len phrases, on cherche le multiple de \"batch_size\" le plus proche de ce \n",
    "# nombre par valeur supérieur : epoch_size doit etre un multiple non nul de ce nombre (pour ne pas gaspiller) \n",
    "\n",
    "main_path = \"/home/XLM/data/30000/\"\n",
    "\n",
    "import io\n",
    "\n",
    "def n_lines(file_path):\n",
    "    i = 0\n",
    "    with open(file_path, mode = \"r\", encoding='UTF-8') as myfile :\n",
    "        for _ in myfile.readlines() :\n",
    "            i = i + 1\n",
    "    return i\n",
    "\n",
    "tensor_len = n_lines(file_path = main_path + \"es-fr/es-fr.es.train\")\n",
    "print(\"tensor_len = \" + str(tensor_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_size = 2379225\n"
     ]
    }
   ],
   "source": [
    "tensor_len = 38067593\n",
    "batch_size = 16\n",
    "\n",
    "def getEpochSize(tensor_len, batch_size):\n",
    "    i = tensor_len\n",
    "    while True :\n",
    "        if i%batch_size == 0 :\n",
    "            return i//batch_size\n",
    "        i = i + 1\n",
    "\n",
    "    \n",
    "epoch_size = getEpochSize(tensor_len = tensor_len, batch_size = batch_size)\n",
    "print(\"epoch_size = \" + str(epoch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VqJWeWSRdvDm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: epoch_size=2379225\n"
     ]
    }
   ],
   "source": [
    "# Et deduire epoch_size (evaluation frequency, -1 for parallel data size)\n",
    "%env epoch_size=2379225\n",
    "#%env epoch_size=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: remove_long_sentences_train=True\n",
      "env: remove_long_sentences_valid=True\n",
      "env: remove_long_sentences_test=True\n"
     ]
    }
   ],
   "source": [
    "%env remove_long_sentences_train=True\n",
    "%env remove_long_sentences_valid=True\n",
    "%env remove_long_sentences_test=True\n",
    "#--remove_long_sentences_train $remove_long_sentences_train --remove_long_sentences_valid $remove_long_sentences_valid --remove_long_sentences_test $remove_long_sentences_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: train_n_samples=-1\n",
      "env: valid_n_samples=-1\n",
      "env: test_n_samples=-1\n"
     ]
    }
   ],
   "source": [
    "# limit the number of examples (-1 by default for non limitation)\n",
    "%env train_n_samples=-1\n",
    "%env valid_n_samples=-1\n",
    "%env test_n_samples=-1\n",
    "#--train_n_samples $train_n_samples --valid_n_samples $valid_n_samples --test_n_samples $test_n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7CmgkltWJgS7"
   },
   "source": [
    "**Pretrain a language model (MLM+TLM)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OXmsZLmdeyL0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_epoch = 27255\n",
      "nb_hour = 1362\n"
     ]
    }
   ],
   "source": [
    "def get_max_epoch(tensor_len):\n",
    "    return int(100*(tensor_len/139670)) # par expérience\n",
    "\n",
    "def get_nb_hour(max_epoch, nb_gpu = 1):\n",
    "    return int(5*(max_epoch/100)) * nb_gpu\n",
    "\n",
    "max_epoch = get_max_epoch(tensor_len)\n",
    "print(\"max_epoch = \"+ str(max_epoch))\n",
    "print(\"nb_hour = \" + str(get_nb_hour(max_epoch, nb_gpu = 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hOfP9eVEd5rY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: max_epoch=27255\n"
     ]
    }
   ],
   "source": [
    "%env max_epoch=27255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "01dQhpsfeH6y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: stopping_criterion=_valid_mlm_ppl,10\n",
      "env: eval_bleu=false\n",
      "env: lgs=es-fr\n",
      "env: mlm_steps=es,fr,es-fr\n"
     ]
    }
   ],
   "source": [
    "# stopping criterion (if criterion does not improve 10 times)\n",
    "%env stopping_criterion=_valid_mlm_ppl,10\n",
    "%env eval_bleu false\n",
    "%env lgs=es-fr\n",
    "%env mlm_steps=es,fr,es-fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eoACYa8IeZIY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: max_epoch=1\n",
      "env: use_memory=True\n",
      "env: mem_enc_positions=0\n",
      "env: mem_dec_positions=0\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm\n",
      "INFO - 05/10/20 21:04:16 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/10/20 21:04:16 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 16\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     command: python train.py --exp_name esfr_mlm_tlm --dump_path './dumped/' --data_path '/home/XLM/data/30000/es-fr' --lgs 'es-fr' --clm_steps '' --mlm_steps 'es,fr,es-fr' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 16 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 2379225 --max_epoch 1 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu false --exp_id hypothesis --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --use_memory True --mem_enc_positions 0 --mem_dec_positions 0 --exp_id \"hypothesis\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/XLM/data/30000/es-fr\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: ./dumped/esfr_mlm_tlm/hypothesis\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 2379225\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: False\n",
      "                                     exp_id: hypothesis\n",
      "                                     exp_name: esfr_mlm_tlm\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'es', 1: 'fr'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'es': 0, 'fr': 1}\n",
      "                                     langs: ['es', 'fr']\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: es-fr\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 1\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     mem_dec_positions: [(0, 'in')]\n",
      "                                     mem_enc_positions: [(0, 'in')]\n",
      "                                     mem_grouped_conv: False\n",
      "                                     mem_heads: 4\n",
      "                                     mem_implementation: pq_fast\n",
      "                                     mem_input2d: False\n",
      "                                     mem_input_dropout: 0\n",
      "                                     mem_k_dim: 256\n",
      "                                     mem_keys_learn: True\n",
      "                                     mem_keys_normalized_init: False\n",
      "                                     mem_keys_type: uniform\n",
      "                                     mem_knn: 32\n",
      "                                     mem_modulo_size: -1\n",
      "                                     mem_multi_query_net: False\n",
      "                                     mem_n_keys: 512\n",
      "                                     mem_normalize_query: False\n",
      "                                     mem_product_quantization: True\n",
      "                                     mem_query_batchnorm: False\n",
      "                                     mem_query_bias: True\n",
      "                                     mem_query_detach_input: False\n",
      "                                     mem_query_dropout: 0\n",
      "                                     mem_query_kernel_sizes: \n",
      "                                     mem_query_layer_sizes: [0, 0]\n",
      "                                     mem_query_net_learn: True\n",
      "                                     mem_query_residual: False\n",
      "                                     mem_score_normalize: False\n",
      "                                     mem_score_softmax: True\n",
      "                                     mem_score_subtract: \n",
      "                                     mem_share_values: False\n",
      "                                     mem_shuffle_indices: False\n",
      "                                     mem_shuffle_query: False\n",
      "                                     mem_size: 262144\n",
      "                                     mem_sparse: False\n",
      "                                     mem_temperature: 1\n",
      "                                     mem_use_different_keys: True\n",
      "                                     mem_v_dim: -1\n",
      "                                     mem_value_dropout: 0\n",
      "                                     mem_value_zero_init: False\n",
      "                                     mem_values_optimizer: adam,lr=0.001\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('es', None), ('fr', None), ('es', 'fr')]\n",
      "                                     mono_dataset: {'es': {'train': '/home/XLM/data/30000/es-fr/train.es.pth', 'valid': '/home/XLM/data/30000/es-fr/valid.es.pth', 'test': '/home/XLM/data/30000/es-fr/test.es.pth'}, 'fr': {'train': '/home/XLM/data/30000/es-fr/train.fr.pth', 'valid': '/home/XLM/data/30000/es-fr/valid.fr.pth', 'test': '/home/XLM/data/30000/es-fr/test.fr.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_indices: 262144\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': -1, 'valid': -1, 'test': -1}\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {('es', 'fr'): {'train': ('/home/XLM/data/30000/es-fr/train.es-fr.es.pth', '/home/XLM/data/30000/es-fr/train.es-fr.fr.pth'), 'valid': ('/home/XLM/data/30000/es-fr/valid.es-fr.es.pth', '/home/XLM/data/30000/es-fr/valid.es-fr.fr.pth'), 'test': ('/home/XLM/data/30000/es-fr/test.es-fr.es.pth', '/home/XLM/data/30000/es-fr/test.es-fr.fr.pth')}}\n",
      "                                     pc_steps: []\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': True, 'valid': True, 'test': True}\n",
      "                                     remove_long_sentences_test: True\n",
      "                                     remove_long_sentences_train: True\n",
      "                                     remove_long_sentences_valid: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: -1\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: -1\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: True\n",
      "                                     valid_n_samples: -1\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/10/20 21:04:16 - 0:00:00 - The experiment will be stored in ./dumped/esfr_mlm_tlm/hypothesis\n",
      "                                     \n",
      "INFO - 05/10/20 21:04:16 - 0:00:00 - Running command: python train.py --exp_name esfr_mlm_tlm --dump_path './dumped/' --data_path '/home/XLM/data/30000/es-fr' --lgs 'es-fr' --clm_steps '' --mlm_steps 'es,fr,es-fr' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 16 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 2379225 --max_epoch 1 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu false --exp_id hypothesis --remove_long_sentences_train True --remove_long_sentences_valid True --remove_long_sentences_test True --train_n_samples '-1' --valid_n_samples '-1' --test_n_samples '-1' --use_memory True --mem_enc_positions 0 --mem_dec_positions 0\n",
      "\n",
      "WARNING - 05/10/20 21:04:16 - 0:00:00 - Signal handler installed.\n",
      "INFO - 05/10/20 21:04:16 - 0:00:00 - ============ Monolingual data (es)\n",
      "INFO - 05/10/20 21:04:16 - 0:00:00 - Loading data from /home/XLM/data/30000/es-fr/train.es.pth ...\n",
      "INFO - 05/10/20 21:04:30 - 0:00:14 - 569013718 words (30229 unique) in 38067593 sentences. 4192 unknown words (815 unique) covering 0.00% of the data.\n",
      "\n",
      "INFO - 05/10/20 21:04:34 - 0:00:18 - Loading data from /home/XLM/data/30000/es-fr/valid.es.pth ...\n",
      "INFO - 05/10/20 21:04:37 - 0:00:21 - 70895753 words (30229 unique) in 4758449 sentences. 461 unknown words (172 unique) covering 0.00% of the data.\n",
      "\n",
      "INFO - 05/10/20 21:04:38 - 0:00:22 - Loading data from /home/XLM/data/30000/es-fr/test.es.pth ...\n",
      "INFO - 05/10/20 21:04:41 - 0:00:25 - 70966427 words (30229 unique) in 4758449 sentences. 589 unknown words (269 unique) covering 0.00% of the data.\n",
      "\n",
      "INFO - 05/10/20 21:04:41 - 0:00:25 - ============ Monolingual data (fr)\n",
      "INFO - 05/10/20 21:04:41 - 0:00:25 - Loading data from /home/XLM/data/30000/es-fr/train.fr.pth ...\n",
      "INFO - 05/10/20 21:05:01 - 0:00:46 - 577398557 words (30229 unique) in 38067593 sentences. 4353 unknown words (762 unique) covering 0.00% of the data.\n",
      "\n",
      "INFO - 05/10/20 21:05:06 - 0:00:50 - Loading data from /home/XLM/data/30000/es-fr/valid.fr.pth ...\n",
      "INFO - 05/10/20 21:05:10 - 0:00:54 - 72143220 words (30229 unique) in 4758449 sentences. 571 unknown words (214 unique) covering 0.00% of the data.\n",
      "\n",
      "INFO - 05/10/20 21:05:10 - 0:00:54 - Loading data from /home/XLM/data/30000/es-fr/test.fr.pth ...\n",
      "INFO - 05/10/20 21:05:13 - 0:00:57 - 72213233 words (30229 unique) in 4758449 sentences. 484 unknown words (194 unique) covering 0.00% of the data.\n",
      "\n",
      "\n",
      "INFO - 05/10/20 21:05:14 - 0:00:58 - ============ Parallel data (es-fr)\n",
      "INFO - 05/10/20 21:05:14 - 0:00:58 - Loading data from /home/XLM/data/30000/es-fr/train.es-fr.es.pth ...\n",
      "INFO - 05/10/20 21:05:34 - 0:01:18 - 569013718 words (30229 unique) in 38067593 sentences. 4192 unknown words (815 unique) covering 0.00% of the data.\n",
      "INFO - 05/10/20 21:05:34 - 0:01:18 - Loading data from /home/XLM/data/30000/es-fr/train.es-fr.fr.pth ...\n",
      "INFO - 05/10/20 21:05:58 - 0:01:42 - 577398557 words (30229 unique) in 38067593 sentences. 4353 unknown words (762 unique) covering 0.00% of the data.\n",
      "INFO - 05/10/20 21:06:05 - 0:01:49 - Removed 54 empty sentences.\n",
      "INFO - 05/10/20 21:06:12 - 0:01:56 - Removed 0 empty sentences.\n"
     ]
    }
   ],
   "source": [
    "%env max_epoch=1\n",
    "\n",
    "%env use_memory=True\n",
    "%env mem_enc_positions=0\n",
    "%env mem_dec_positions=0\n",
    "#--use_memory $use_memory --mem_enc_positions $mem_enc_positions --mem_dec_positions $mem_dec_positions\n",
    "\n",
    "! python train.py --exp_name esfr_mlm_tlm --dump_path ./dumped/ --data_path $OUTPATH --lgs $lgs --clm_steps '' --mlm_steps $mlm_steps --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout 0.1 --attention_dropout 0.1 --gelu_activation true --batch_size $batch_size --bptt 256 --optimizer adam,lr=0.0001 --epoch_size $epoch_size --max_epoch $max_epoch --validation_metrics _valid_mlm_ppl --stopping_criterion $stopping_criterion --eval_bleu $eval_bleu --exp_id $exp_id --remove_long_sentences_train $remove_long_sentences_train --remove_long_sentences_valid $remove_long_sentences_valid --remove_long_sentences_test $remove_long_sentences_test --train_n_samples $train_n_samples --valid_n_samples $valid_n_samples --test_n_samples $test_n_samples --use_memory $use_memory --mem_enc_positions $mem_enc_positions --mem_dec_positions $mem_dec_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NWr_m3Y2et7A"
   },
   "outputs": [],
   "source": [
    "# Si l'entrainement echoue\n",
    "#! rm -R dumped/esfr_mlm_tlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_5ozfc5qvYh5"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UNkD_5htPpAQ"
   },
   "source": [
    "**Train on unsupervised MT from a pretrained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AEVNOQW0neJE"
   },
   "outputs": [],
   "source": [
    "#%env batch_size=...\n",
    "#%env epoch_size=...\n",
    "\n",
    "%env eval_bleu=true\n",
    "# comme eval_bleu=true\n",
    "%cd /home/jupyter/meta_XLM/XLM\n",
    "! chmod +x src/evaluation/multi-bleu.perl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hiMo7NqTfi8O"
   },
   "outputs": [],
   "source": [
    "#%env stopping_criterion=valid_es-fr_mt_bleu,10\n",
    "%env stopping_criterion=valid_es-fr_mt_bleu,2\n",
    "%env validation_metrics=valid_es-fr_mt_bleu\n",
    "# reload encoder and decoder from dumped/esfr_mlm_tlm/hypothesis/best-valid_mlm_ppl.pth\n",
    "%env reload_model=dumped/esfr_mlm_tlm/hypothesis/best-valid_mlm_ppl.pth,dumped/esfr_mlm_tlm/hypothesis/best-valid_mlm_ppl.pth\n",
    "%env ae_steps=es,fr\n",
    "%env bt_steps=es-fr-es,fr-es-fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oRw4RN5Ufo6s"
   },
   "outputs": [],
   "source": [
    "#! python train.py --exp_name unsupMT_esfr --dump_path ./dumped/ --reload_model $reload_model --data_path $OUTPATH --lgs $lgs --ae_steps $ae_steps --bt_steps $bt_steps --word_shuffle 3 --word_dropout 0.1 --word_blank 0.1 --lambda_ae '0:1,100000:0.1,300000:0' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout 0.1 --attention_dropout 0.1 --gelu_activation true --tokens_per_batch 2000 --batch_size $batch_size --bptt 256 --optimizer adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001 --epoch_size $epoch_size --max_epoch $max_epoch --eval_bleu $eval_bleu --stopping_criterion $stopping_criterion --validation_metrics $validation_metrics --exp_id $exp_id  \n",
    "%env mt_steps=es-fr,fr-es\n",
    "! python train.py --exp_name unsupMT_esfr --dump_path ./dumped/ --reload_model $reload_model --data_path $OUTPATH --lgs $lgs --ae_steps $ae_steps --mt_steps $mt_steps --bt_steps $bt_steps --word_shuffle 3 --word_dropout 0.1 --word_blank 0.1 --lambda_ae '0:1,100000:0.1,300000:0' --encoder_only false --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout 0.1 --attention_dropout 0.1 --gelu_activation true --tokens_per_batch 2000 --batch_size $batch_size --bptt 256 --optimizer adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001 --epoch_size $epoch_size --max_epoch $max_epoch --eval_bleu $eval_bleu --stopping_criterion $stopping_criterion --validation_metrics $validation_metrics --exp_id $exp_id  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kCJFNZw1UY7N"
   },
   "outputs": [],
   "source": [
    "# Si l'entrainement echoue\n",
    "#! rm -R dumped/unsupMT_esfr"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "xgG9b83apvC3",
    "kNkA4ggJ0mkX",
    "A9wxgjwDADDz"
   ],
   "name": "train_espagnol_francais.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
