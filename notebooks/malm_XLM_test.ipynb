{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(bookmark:HOME) -> /home/jupyter/meta_XLM/XLM_our\n",
      "/home/jupyter/meta_XLM/XLM_our\n"
     ]
    }
   ],
   "source": [
    "%bookmark HOME \"/home/jupyter/meta_XLM/XLM_our\" \n",
    "%cd -b HOME "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Je fusionne deux dossiers es-it et de-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending incremental file list\n",
      "codes\n",
      "        487,116 100%  433.30MB/s    0:00:00 (xfr#1, to-chk=20/21)\n",
      "es-it.es.test.pth\n",
      "     39,869,081 100%  279.57MB/s    0:00:00 (xfr#2, to-chk=19/21)\n",
      "es-it.es.train.pth\n",
      "    313,208,065 100%  255.52MB/s    0:00:01 (xfr#3, to-chk=18/21)\n",
      "es-it.es.valid.pth\n",
      "     39,943,773 100%  124.90MB/s    0:00:00 (xfr#4, to-chk=17/21)\n",
      "es-it.it.test.pth\n",
      "     38,691,996 100%   83.67MB/s    0:00:00 (xfr#5, to-chk=16/21)\n",
      "es-it.it.train.pth\n",
      "    303,647,395 100%  199.99MB/s    0:00:01 (xfr#6, to-chk=15/21)\n",
      "es-it.it.valid.pth\n",
      "     38,741,018 100%   63.81MB/s    0:00:00 (xfr#7, to-chk=14/21)\n",
      "test.es-it.es.pth\n",
      "     39,869,081 100%   53.25MB/s    0:00:00 (xfr#8, to-chk=13/21)\n",
      "test.es-it.it.pth\n",
      "     38,691,996 100%   43.82MB/s    0:00:00 (xfr#9, to-chk=12/21)\n",
      "test.es.pth\n",
      "     39,869,081 100%   39.00MB/s    0:00:00 (xfr#10, to-chk=11/21)\n",
      "test.it.pth\n",
      "     38,691,996 100%   33.36MB/s    0:00:01 (xfr#11, to-chk=10/21)\n",
      "train\n",
      "    348,824,186 100%  247.33MB/s    0:00:01 (xfr#12, to-chk=9/21)\n",
      "train.es-it.es.pth\n",
      "    313,208,065 100%  200.74MB/s    0:00:01 (xfr#13, to-chk=8/21)\n",
      "train.es-it.it.pth\n",
      "    303,647,395 100%   89.43MB/s    0:00:03 (xfr#14, to-chk=7/21)\n",
      "train.es.pth\n",
      "    313,208,065 100%  246.04MB/s    0:00:01 (xfr#15, to-chk=6/21)\n",
      "train.it.pth\n",
      "    303,647,395 100%   82.86MB/s    0:00:03 (xfr#16, to-chk=5/21)\n",
      "valid.es-it.es.pth\n",
      "     39,943,773 100%   59.15MB/s    0:00:00 (xfr#17, to-chk=4/21)\n",
      "valid.es-it.it.pth\n",
      "     38,741,018 100%   47.19MB/s    0:00:00 (xfr#18, to-chk=3/21)\n",
      "valid.es.pth\n",
      "     39,943,773 100%   41.05MB/s    0:00:00 (xfr#19, to-chk=2/21)\n",
      "valid.it.pth\n",
      "     38,741,018 100%   34.50MB/s    0:00:01 (xfr#20, to-chk=1/21)\n",
      "vocab\n",
      "        381,041 100%    4.91MB/s    0:00:00 (xfr#21, to-chk=0/21)\n",
      "\n",
      "sent 2,672,650,033 bytes  received 415 bytes  197,974,107.26 bytes/sec\n",
      "total size is 2,671,996,327  speedup is 1.00\n"
     ]
    }
   ],
   "source": [
    "#!  sudo rsync -av --progress /home/hkseventh/data/30000/es-it/* /home/hkseventh/data/30000/meta_learning --exclude *.train --exclude *.test --exclude *.valid --exclude *.es --exclude *.it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending incremental file list\n",
      "codes\n",
      "        438,572 100%  129.00MB/s    0:00:00 (xfr#1, to-chk=20/21)\n",
      "de-en.de.test.pth\n",
      "     80,093,732 100%  271.83MB/s    0:00:00 (xfr#2, to-chk=19/21)\n",
      "de-en.de.train.pth\n",
      "    634,705,069 100%  245.96MB/s    0:00:02 (xfr#3, to-chk=18/21)\n",
      "de-en.de.valid.pth\n",
      "     80,091,671 100%  102.25MB/s    0:00:00 (xfr#4, to-chk=17/21)\n",
      "de-en.en.test.pth\n",
      "     82,283,219 100%   75.38MB/s    0:00:01 (xfr#5, to-chk=16/21)\n",
      "de-en.en.train.pth\n",
      "    649,432,702 100%  133.65MB/s    0:00:04 (xfr#6, to-chk=15/21)\n",
      "de-en.en.valid.pth\n",
      "     82,227,820 100%   96.69MB/s    0:00:00 (xfr#7, to-chk=14/21)\n",
      "test.de-en.de.pth\n",
      "     80,093,732 100%   68.88MB/s    0:00:01 (xfr#8, to-chk=13/21)\n",
      "test.de-en.en.pth\n",
      "     82,283,219 100%  188.63MB/s    0:00:00 (xfr#9, to-chk=12/21)\n",
      "test.de.pth\n",
      "     80,093,732 100%  106.68MB/s    0:00:00 (xfr#10, to-chk=11/21)\n",
      "test.en.pth\n",
      "     82,283,219 100%   76.71MB/s    0:00:01 (xfr#11, to-chk=10/21)\n",
      "train\n",
      "     78,917,864 100%  237.42MB/s    0:00:00 (xfr#12, to-chk=9/21)\n",
      "train.de-en.de.pth\n",
      "    634,705,069 100%   35.59MB/s    0:00:17 (xfr#13, to-chk=8/21)\n",
      "train.de-en.en.pth\n",
      "    649,432,702 100%   47.79MB/s    0:00:12 (xfr#14, to-chk=7/21)\n",
      "train.de.pth\n",
      "    634,705,069 100%   38.35MB/s    0:00:15 (xfr#15, to-chk=6/21)\n",
      "train.en.pth\n",
      "    649,432,702 100%   86.43MB/s    0:00:07 (xfr#16, to-chk=5/21)\n",
      "valid.de-en.de.pth\n",
      "     80,091,671 100%   66.77MB/s    0:00:01 (xfr#17, to-chk=4/21)\n",
      "valid.de-en.en.pth\n",
      "     82,227,820 100%   37.63MB/s    0:00:02 (xfr#18, to-chk=3/21)\n",
      "valid.de.pth\n",
      "     80,091,671 100%   58.31MB/s    0:00:01 (xfr#19, to-chk=2/21)\n",
      "valid.en.pth\n",
      "     82,227,820 100%   38.42MB/s    0:00:02 (xfr#20, to-chk=1/21)\n",
      "vocab\n",
      "        330,270 100%  472.22kB/s    0:00:00 (xfr#21, to-chk=0/21)\n",
      "\n",
      "sent 4,907,388,563 bytes  received 415 bytes  73,795,322.98 bytes/sec\n",
      "total size is 4,906,189,345  speedup is 1.00\n"
     ]
    }
   ],
   "source": [
    "# On conserve le code & vocab issu de de-en \n",
    "#!  sudo rsync -av --progress /home/hkseventh/data/30000/de-en/* /home/hkseventh/data/30000/meta_learning --exclude *.train --exclude *.test --exclude *.valid --exclude *.de --exclude *.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codes\t\t    es-it.it.test.pth\ttest.it.pth\t    valid.de-en.de.pth\n",
      "de-en.de.test.pth   es-it.it.train.pth\ttrain\t\t    valid.de-en.en.pth\n",
      "de-en.de.train.pth  es-it.it.valid.pth\ttrain.de-en.de.pth  valid.de.pth\n",
      "de-en.de.valid.pth  test.de-en.de.pth\ttrain.de-en.en.pth  valid.en.pth\n",
      "de-en.en.test.pth   test.de-en.en.pth\ttrain.de.pth\t    valid.es-it.es.pth\n",
      "de-en.en.train.pth  test.de.pth\t\ttrain.en.pth\t    valid.es-it.it.pth\n",
      "de-en.en.valid.pth  test.en.pth\t\ttrain.es-it.es.pth  valid.es.pth\n",
      "es-it.es.test.pth   test.es-it.es.pth\ttrain.es-it.it.pth  valid.it.pth\n",
      "es-it.es.train.pth  test.es-it.it.pth\ttrain.es.pth\t    vocab\n",
      "es-it.es.valid.pth  test.es.pth\t\ttrain.it.pth\n"
     ]
    }
   ],
   "source": [
    "# ! rm -r  /home/hkseventh/data/30000/meta_learning \n",
    "! ls /home/hkseventh/data/30000/meta_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OUTPATH=/home/hkseventh/data/30000/meta_learning\n"
     ]
    }
   ],
   "source": [
    "%env OUTPATH=/home/hkseventh/data/30000/meta_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: batch_size=2\n",
      "env: stopping_criterion=_valid_mlm_ppl,10\n",
      "env: eval_bleu=false\n",
      "env: exp_id=test\n"
     ]
    }
   ],
   "source": [
    "%env batch_size=2\n",
    "# stopping criterion (if criterion does not improve 10 times)\n",
    "%env stopping_criterion=_valid_mlm_ppl,10\n",
    "%env eval_bleu false\n",
    "%env exp_id=test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: max_epoch=1\n",
      "env: epoch_size=20\n",
      "env: train_n_samples=30\n",
      "env: valid_n_samples=10\n",
      "env: test_n_samples=10\n"
     ]
    }
   ],
   "source": [
    "# MLM only\n",
    "%env max_epoch=1\n",
    "%env epoch_size=20\n",
    "\n",
    "# limit the number of examples (-1 by default for non limitation)\n",
    "%env train_n_samples=30\n",
    "%env valid_n_samples=10\n",
    "%env test_n_samples=10\n",
    "#--train_n_samples $train_n_samples --valid_n_samples $valid_n_samples --test_n_samples $test_n_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: lgs=es-it\n",
      "env: mlm_steps=es,it\n",
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n",
      "1 es,it\n",
      "SLURM job: False\n",
      "0 - Number of nodes: 1\n",
      "0 - Node ID        : 0\n",
      "0 - Local rank     : 0\n",
      "0 - Global rank    : 0\n",
      "0 - World size     : 1\n",
      "0 - GPUs per node  : 1\n",
      "0 - Master         : True\n",
      "0 - Multi-node     : False\n",
      "0 - Multi-GPU      : False\n",
      "0 - Hostname       : african-translator-vm\n",
      "INFO - 05/12/20 09:22:40 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 05/12/20 09:22:40 - 0:00:00 - accumulate_gradients: 1\n",
      "                                     ae_steps: []\n",
      "                                     aes: [[]]\n",
      "                                     amp: -1\n",
      "                                     asm: False\n",
      "                                     attention_dropout: 0.1\n",
      "                                     batch_size: 2\n",
      "                                     beam_size: 1\n",
      "                                     bptt: 256\n",
      "                                     bt_src_langs: []\n",
      "                                     bt_steps: []\n",
      "                                     bts: [[]]\n",
      "                                     clip_grad_norm: 5\n",
      "                                     clm_steps: []\n",
      "                                     clms: [[]]\n",
      "                                     command: python train.py --exp_name test_meta_mlm --dump_path './dumped/' --data_path '/home/hkseventh/data/30000/meta_learning' --lgs 'es-it' --clm_steps '' --mlm_steps 'es,it' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 2 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 20 --max_epoch 1 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu false --exp_id test --train_n_samples 30 --valid_n_samples 10 --test_n_samples 10 --exp_id \"test\"\n",
      "                                     context_size: 0\n",
      "                                     data_path: /home/hkseventh/data/30000/meta_learning\n",
      "                                     debug: False\n",
      "                                     debug_slurm: False\n",
      "                                     debug_train: False\n",
      "                                     dropout: 0.1\n",
      "                                     dump_path: ./dumped/test_meta_mlm/test\n",
      "                                     early_stopping: False\n",
      "                                     emb_dim: 1024\n",
      "                                     encoder_only: True\n",
      "                                     epoch_size: 20\n",
      "                                     eval_bleu: False\n",
      "                                     eval_only: False\n",
      "                                     exp_id: test\n",
      "                                     exp_name: test_meta_mlm\n",
      "                                     fp16: False\n",
      "                                     gelu_activation: True\n",
      "                                     global_rank: 0\n",
      "                                     group_by_size: True\n",
      "                                     id2lang: {0: 'es', 1: 'it'}\n",
      "                                     is_master: True\n",
      "                                     is_slurm_job: False\n",
      "                                     lambda_ae: 1\n",
      "                                     lambda_bt: 1\n",
      "                                     lambda_clm: 1\n",
      "                                     lambda_mlm: 1\n",
      "                                     lambda_mt: 1\n",
      "                                     lambda_pc: 1\n",
      "                                     lang2id: {'es': 0, 'it': 1}\n",
      "                                     langs: [['es', 'it']]\n",
      "                                     length_penalty: 1\n",
      "                                     lg_sampling_factor: -1\n",
      "                                     lgs: ['es-it']\n",
      "                                     local_rank: 0\n",
      "                                     master_port: -1\n",
      "                                     max_batch_size: 0\n",
      "                                     max_epoch: 1\n",
      "                                     max_len: 100\n",
      "                                     max_vocab: -1\n",
      "                                     meta_learning: False\n",
      "                                     meta_params: {'es-it': Namespace(accumulate_gradients=1, ae_steps=[], amp=-1, asm=False, attention_dropout=0.1, batch_size=2, beam_size=1, bptt=256, bt_src_langs=[], bt_steps=[], clip_grad_norm=5, clm_steps=[], context_size=0, data_path='/home/hkseventh/data/30000/meta_learning', debug=False, debug_slurm=False, debug_train=False, dropout=0.1, dump_path='./dumped/', early_stopping=False, emb_dim=1024, encoder_only=True, epoch_size=20, eval_bleu=False, eval_only=False, exp_id='test', exp_name='test_meta_mlm', fp16=False, gelu_activation=True, group_by_size=True, id2lang={0: 'es', 1: 'it'}, lambda_ae='1', lambda_bt='1', lambda_clm='1', lambda_mlm='1', lambda_mt='1', lambda_pc='1', lang2id={'es': 0, 'it': 1}, langs=['es', 'it'], length_penalty=1, lg_sampling_factor=-1, lgs='es-it', local_rank=-1, master_port=-1, max_batch_size=0, max_epoch=1, max_len=100, max_vocab=-1, meta_learning=False, meta_params={}, min_count=0, mlm_steps=[('es', None), ('it', None)], mono_dataset={'es': {'train': '/home/hkseventh/data/30000/meta_learning/train.es.pth', 'valid': '/home/hkseventh/data/30000/meta_learning/valid.es.pth', 'test': '/home/hkseventh/data/30000/meta_learning/test.es.pth'}, 'it': {'train': '/home/hkseventh/data/30000/meta_learning/train.it.pth', 'valid': '/home/hkseventh/data/30000/meta_learning/valid.it.pth', 'test': '/home/hkseventh/data/30000/meta_learning/test.it.pth'}}, mt_steps=[], n_heads=8, n_langs=2, n_layers=6, n_samples={'train': 30, 'valid': 10, 'test': 10}, optimizer='adam,lr=0.0001', para_dataset={}, pc_steps=[], reload_checkpoint='', reload_emb='', reload_model='', remove_long_sentences={'train': True, 'valid': True, 'test': True}, remove_long_sentences_test=True, remove_long_sentences_train=True, remove_long_sentences_valid=True, sample_alpha=0, save_periodic=0, share_inout_emb=True, sinusoidal_embeddings=False, split_data=False, stopping_criterion='_valid_mlm_ppl,10', test_n_samples=10, tokens_per_batch=-1, train_n_samples=30, use_lang_emb=True, use_memory=False, valid_n_samples=10, validation_metrics='_valid_mlm_ppl', word_blank=0, word_dropout=0, word_keep=0.1, word_mask=0.8, word_mask_keep_rand='0.8,0.1,0.1', word_pred=0.15, word_rand=0.1, word_shuffle=0)}\n",
      "                                     min_count: 0\n",
      "                                     mlm_steps: [('es', None), ('it', None)]\n",
      "                                     mlms: [[('es', None), ('it', None)]]\n",
      "                                     mono_dataset: {'es': {'train': '/home/hkseventh/data/30000/meta_learning/train.es.pth', 'valid': '/home/hkseventh/data/30000/meta_learning/valid.es.pth', 'test': '/home/hkseventh/data/30000/meta_learning/test.es.pth'}, 'it': {'train': '/home/hkseventh/data/30000/meta_learning/train.it.pth', 'valid': '/home/hkseventh/data/30000/meta_learning/valid.it.pth', 'test': '/home/hkseventh/data/30000/meta_learning/test.it.pth'}}\n",
      "                                     mt_steps: []\n",
      "                                     mts: [[]]\n",
      "                                     multi_gpu: False\n",
      "                                     multi_node: False\n",
      "                                     n_gpu_per_node: 1\n",
      "                                     n_heads: 8\n",
      "                                     n_langs: 2\n",
      "                                     n_layers: 6\n",
      "                                     n_nodes: 1\n",
      "                                     n_samples: {'train': 30, 'valid': 10, 'test': 10}\n",
      "                                     node_id: 0\n",
      "                                     optimizer: adam,lr=0.0001\n",
      "                                     para_dataset: {}\n",
      "                                     pc_steps: []\n",
      "                                     pcs: [[]]\n",
      "                                     reload_checkpoint: \n",
      "                                     reload_emb: \n",
      "                                     reload_model: \n",
      "                                     remove_long_sentences: {'train': True, 'valid': True, 'test': True}\n",
      "                                     remove_long_sentences_test: True\n",
      "                                     remove_long_sentences_train: True\n",
      "                                     remove_long_sentences_valid: True\n",
      "                                     sample_alpha: 0\n",
      "                                     save_periodic: 0\n",
      "                                     share_inout_emb: True\n",
      "                                     sinusoidal_embeddings: False\n",
      "                                     split_data: False\n",
      "                                     stopping_criterion: _valid_mlm_ppl,10\n",
      "                                     test_n_samples: 10\n",
      "                                     tokens_per_batch: -1\n",
      "                                     train_n_samples: 30\n",
      "                                     use_lang_emb: True\n",
      "                                     use_memory: False\n",
      "                                     valid_n_samples: 10\n",
      "                                     validation_metrics: _valid_mlm_ppl\n",
      "                                     word_blank: 0\n",
      "                                     word_dropout: 0\n",
      "                                     word_keep: 0.1\n",
      "                                     word_mask: 0.8\n",
      "                                     word_mask_keep_rand: 0.8,0.1,0.1\n",
      "                                     word_pred: 0.15\n",
      "                                     word_rand: 0.1\n",
      "                                     word_shuffle: 0\n",
      "                                     world_size: 1\n",
      "INFO - 05/12/20 09:22:40 - 0:00:00 - The experiment will be stored in ./dumped/test_meta_mlm/test\n",
      "                                     \n",
      "INFO - 05/12/20 09:22:40 - 0:00:00 - Running command: python train.py --exp_name test_meta_mlm --dump_path './dumped/' --data_path '/home/hkseventh/data/30000/meta_learning' --lgs 'es-it' --clm_steps '' --mlm_steps 'es,it' --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout '0.1' --attention_dropout '0.1' --gelu_activation true --batch_size 2 --bptt 256 --optimizer 'adam,lr=0.0001' --epoch_size 20 --max_epoch 1 --validation_metrics _valid_mlm_ppl --stopping_criterion '_valid_mlm_ppl,10' --eval_bleu false --exp_id test --train_n_samples 30 --valid_n_samples 10 --test_n_samples 10\n",
      "\n",
      "WARNING - 05/12/20 09:22:40 - 0:00:00 - Signal handler installed.\n",
      "valeur.langs ['es', 'it']\n",
      "INFO - 05/12/20 09:22:40 - 0:00:00 - ============ Monolingual data (es)\n",
      "INFO - 05/12/20 09:22:40 - 0:00:00 - Loading data from /home/hkseventh/data/30000/meta_learning/train.es.pth ...\n",
      "INFO - 05/12/20 09:22:40 - 0:00:00 - 121534108 words (30760 unique) in 3847126 sentences. 668 unknown words (515 unique) covering 0.00% of the data.\n",
      "phrase_par_batch = 15\n",
      "INFO - 05/12/20 09:22:41 - 0:00:02 - Selecting batches from 0 to 1 ...\n",
      "\n",
      "INFO - 05/12/20 09:22:41 - 0:00:02 - Loading data from /home/hkseventh/data/30000/meta_learning/valid.es.pth ...\n",
      "INFO - 05/12/20 09:22:41 - 0:00:02 - 15199981 words (30760 unique) in 480890 sentences. 129 unknown words (111 unique) covering 0.00% of the data.\n",
      "phrase_par_batch = 7\n",
      "INFO - 05/12/20 09:22:42 - 0:00:02 - Selecting batches from 0 to 1 ...\n",
      "\n",
      "INFO - 05/12/20 09:22:42 - 0:00:02 - Loading data from /home/hkseventh/data/30000/meta_learning/test.es.pth ...\n",
      "INFO - 05/12/20 09:22:42 - 0:00:02 - 15162799 words (30760 unique) in 480890 sentences. 81 unknown words (73 unique) covering 0.00% of the data.\n",
      "phrase_par_batch = 7\n",
      "INFO - 05/12/20 09:22:42 - 0:00:02 - Selecting batches from 0 to 1 ...\n",
      "\n",
      "INFO - 05/12/20 09:22:42 - 0:00:02 - ============ Monolingual data (it)\n",
      "INFO - 05/12/20 09:22:42 - 0:00:02 - Loading data from /home/hkseventh/data/30000/meta_learning/train.it.pth ...\n",
      "INFO - 05/12/20 09:22:42 - 0:00:02 - 116753884 words (30760 unique) in 3847126 sentences. 633 unknown words (490 unique) covering 0.00% of the data.\n",
      "phrase_par_batch = 16\n",
      "INFO - 05/12/20 09:22:43 - 0:00:04 - Selecting batches from 0 to 1 ...\n",
      "\n",
      "INFO - 05/12/20 09:22:43 - 0:00:04 - Loading data from /home/hkseventh/data/30000/meta_learning/valid.it.pth ...\n",
      "INFO - 05/12/20 09:22:43 - 0:00:04 - 14598718 words (30760 unique) in 480890 sentences. 104 unknown words (84 unique) covering 0.00% of the data.\n",
      "phrase_par_batch = 8\n",
      "INFO - 05/12/20 09:22:43 - 0:00:04 - Selecting batches from 0 to 1 ...\n",
      "\n",
      "INFO - 05/12/20 09:22:43 - 0:00:04 - Loading data from /home/hkseventh/data/30000/meta_learning/test.it.pth ...\n",
      "INFO - 05/12/20 09:22:43 - 0:00:04 - 14574268 words (30760 unique) in 480890 sentences. 77 unknown words (72 unique) covering 0.00% of the data.\n",
      "phrase_par_batch = 8\n",
      "INFO - 05/12/20 09:22:44 - 0:00:04 - Selecting batches from 0 to 1 ...\n",
      "\n",
      "\n",
      "\n",
      "INFO - 05/12/20 09:22:44 - 0:00:04 - ============ Data summary\n",
      "INFO - 05/12/20 09:22:44 - 0:00:04 - Monolingual data   - train -           es:       267\n",
      "INFO - 05/12/20 09:22:44 - 0:00:04 - Monolingual data   - valid -           es:       158\n",
      "INFO - 05/12/20 09:22:44 - 0:00:04 - Monolingual data   -  test -           es:       218\n",
      "INFO - 05/12/20 09:22:44 - 0:00:04 - Monolingual data   - train -           it:        97\n",
      "INFO - 05/12/20 09:22:44 - 0:00:04 - Monolingual data   - valid -           it:        79\n",
      "INFO - 05/12/20 09:22:44 - 0:00:04 - Monolingual data   -  test -           it:       205\n",
      "\n",
      "INFO - 05/12/20 09:22:45 - 0:00:05 - Model: TransformerModel(\n",
      "                                       (position_embeddings): Embedding(512, 1024)\n",
      "                                       (lang_embeddings): Embedding(2, 1024)\n",
      "                                       (embeddings): Embedding(30760, 1024, padding_idx=2)\n",
      "                                       (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       (attentions): ModuleList(\n",
      "                                         (0): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): MultiHeadAttention(\n",
      "                                           (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                           (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm1): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (ffns): ModuleList(\n",
      "                                         (0): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (1): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (2): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (3): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (4): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                         (5): TransformerFFN(\n",
      "                                           (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                                           (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (layer_norm2): ModuleList(\n",
      "                                         (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                         (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                                       )\n",
      "                                       (memories): ModuleDict()\n",
      "                                       (pred_layer): PredLayer(\n",
      "                                         (proj): Linear(in_features=1024, out_features=30760, bias=True)\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 05/12/20 09:22:45 - 0:00:05 - Number of parameters (model): 107634728\n",
      "INFO - 05/12/20 09:22:48 - 0:00:08 - Found 0 memories.\n",
      "INFO - 05/12/20 09:22:48 - 0:00:08 - Found 6 FFN.\n",
      "INFO - 05/12/20 09:22:48 - 0:00:08 - Found 102 parameters in model.\n",
      "INFO - 05/12/20 09:22:48 - 0:00:08 - Optimizers: model\n",
      "WARNING - 05/12/20 09:22:48 - 0:00:08 - Reloading checkpoint from ./dumped/test_meta_mlm/test/checkpoint.pth ...\n",
      "WARNING - 05/12/20 09:22:49 - 0:00:09 - Not reloading checkpoint optimizer model.\n",
      "WARNING - 05/12/20 09:22:49 - 0:00:09 - No 'num_updates' for optimizer model.\n",
      "WARNING - 05/12/20 09:22:49 - 0:00:09 - Checkpoint reloaded. Resuming at epoch 1 / iteration 3 ...\n",
      "INFO - 05/12/20 09:22:49 - 0:00:09 - ============ Starting epoch 1 ... ============\n",
      "[] [('es', None), ('it', None)]\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 484, in <module>\n",
      "    main(params)\n",
      "  File \"train.py\", line 290, in main\n",
      "    trainer.mlm_step(lang1, lang2, params.lambda_mlm)\n",
      "  File \"/home/jupyter/meta_XLM/XLM_our/src/trainer.py\", line 782, in mlm_step\n",
      "    x, lengths, positions, langs, _ = self.generate_batch(lang1, lang2, 'pred')\n",
      "  File \"/home/jupyter/meta_XLM/XLM_our/src/trainer.py\", line 508, in generate_batch\n",
      "    x, lengths = self.get_batch(name, lang1, stream=True, data_key = data_key)\n",
      "  File \"/home/jupyter/meta_XLM/XLM_our/src/trainer.py\", line 335, in get_batch\n",
      "    assert lang1 in params.langs\n",
      "AssertionError\n"
     ]
    }
   ],
   "source": [
    "%env lgs=es-it\n",
    "%env mlm_steps=es,it\n",
    "! python train.py --exp_name test_meta_mlm --dump_path ./dumped/ --data_path $OUTPATH --lgs $lgs --clm_steps '' --mlm_steps $mlm_steps --emb_dim 1024 --n_layers 6 --n_heads 8 --dropout 0.1 --attention_dropout 0.1 --gelu_activation true --batch_size $batch_size --bptt 256 --optimizer adam,lr=0.0001 --epoch_size $epoch_size --max_epoch $max_epoch --validation_metrics _valid_mlm_ppl --stopping_criterion $stopping_criterion --eval_bleu $eval_bleu --exp_id $exp_id --train_n_samples $train_n_samples --valid_n_samples $valid_n_samples --test_n_samples $test_n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
